<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Pragmatic optimization in modern programming - Ordering optimization approaches</title>
    <meta name="description" content="Pragmatic optimization in modern programming - Introduction">
    <meta name="author" content="Marina Kolpakova (cuda.geek)">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="plugin/reveal/css/reveal.css">
    <link rel="stylesheet" href="plugin/reveal/css/theme/geek.css" id="theme">
    <link rel="stylesheet" href="css/colors-orange.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="plugin/reveal/lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'plugin/reveal/css/print/pdf.css' : 'plugin/reveal/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="plugin/reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Pragmatic optimization</h1>
          <h2>in modern programming</h2>
          <h3>Ordering optimization approaches</h3>
          <br>
          <br>
          <br>
          <small>Created by
            <a href="http://github.com/cuda-geek">Marina Kolpakova</a>
            for
            <a href="http://www.unn.ru/eng/">UNN</a> / 2015
          </small>
        </section>

        <section>
          <h2>Course Topics</h2>
          <ul>
            <li><strong>Pragmatics</strong>
              <ul>
                <li><b>Ordering optimization approaches</b></li>
                <li>Demystifying a compiler</li>
                <li>Mastering compiler optimizations</li>
              </ul>
            </li>
            <br/>
            <li><strong>Computer Architectures</strong>
              <ul>
                <li>Architecture of modern computers</li>
                <li>SIMD extensions</li>
                <li>Specific co-processors</li>
              </ul>
            </li>
          </ul>
        </section>

        <section>
          <h2>Outline</h2>
          <ul>
            <li>What is optimization?</li>
            <li>How to learn optimization?</li>
            <li>Recommended literature</li>
            <li>Pragmatic approach</li>
            <li>Knowledge which is required</li>
            <li>Optimization cycle</li>
            <li>Where to get the performance?</li>
            <li>Top-Down (High-low) approach</li>
            <li>Optimization trade-offs</li>
            <li>Overview of optimization steps</li>
            <li>Summary</li>
          </ul>
        </section>

        <section>
          <h2>What is optimization?</h2>
          <blockquote>In computing, <b>optimization</b> is a process of modifying a system to make some aspect of it
            work more efficiently or use fewer resources.
            <br/> In particular, this is a process of transforming a piece of code to make it
            <b>more efficient without changing its output</b>.
          </blockquote>
        </section>
        <section>
          <h2>Commonly considered metrics</h2>
          <dl>
            <dt><b>Wall(-clock) time</b></dt>
            <dd>
              <strong>is a human perception of the span of time from the start to the completion of a task.</strong>
            </dd><br/>
            <dt><b>Power consumption</b></dt>
            <dd>is the electrical energy which is consumed to complete a task.</dd><br/>
            <dt><b>Processor time (or runtime)</b></dt>
            <dd>is the total execution time during which a processor was dedicated
            to a task (i.e. executes instructions of that task).</dd>
          </dl>
        </section>

        <section>
          <h2>How to learn optimization?</h2>
          <blockquote><strong>Optimization is a <b>craft</b> rather than a <b>science</b></strong>.</blockquote>
          <dl>
            <dt><b>Practice more</b></dt>
            <dd>Do not make practical knowledge too theoretical.</dd>
            <br/>
            <dt><b>Look, what other people do</b></dt>
            <dd>Do find real use cases of different optimization approaches and techniques.</dd>
            <br/>
            <dt><b>Dig into an architecture</b></dt>
            <dd>Hardware evolves rapidly hence today's devices obsolete in a wink. <strong>Comprehensive
            knowledge helps to see beforehand</strong>.</dd>
          </dd>
        </section>

        <section>
          <section>
            <h2>Recommended literature</h2>
            <img width="30%" src="images/popt/hp.jpg">
            <p>
              <a href="http://www.amazon.com/Computer-Architecture-Fifth-Quantitative-Approach/dp/012383872X/ref=asap_bc?ie=UTF8">
                Computer Architecture, Fifth Edition: A Quantitative Approach</a><br/>by
              <a href="http://www.computerhistory.org/fellowawards/hall/bios/John,Hennessy/">John L. Hennessy</a>
              and
              <a href="https://www.eecs.berkeley.edu/Faculty/Homepages/patterson.html">David A. Patterson.</a>
            </p>
          </section>

          <section>
            <h2>Recommended literature</h2>
            <img src="images/popt/moh.jpg">
            <p>
              <a href="http://carlos.bueno.org/optimization/mature-optimization.pdf">The Mature Optimization Handbook</a>
              by <a href="http://carlos.bueno.org/about.html">Carlos Bueno</a>
            </p>
          </section>

          <section>
            <h2>Recommended literature</h2>
            <img width="30%" src="images/popt/pm.jpg">
            <p><a href="https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook-1c.2015.01.31a.pdf">
              Is Parallel Programming Hard, And, If So, What Can You Do About It?</a><br/> by
              <a href="http://www.rdrop.com/~paulmck/">Paul E. McKenney</a>
            </p>
          </section>

          <section>
              <h2>Recommended literature</h2>
              <img width="30%" src="images/popt/ec.jpg">
              <p><a href="http://www.amazon.com/Engineering-Compiler-Second-Edition-Cooper/dp/012088478X">
              Engineering a Compiler</a><br/> by
              <a href="http://www.cs.rice.edu/~keith/">Keith Cooper</a> and <a href="http://www.cs.rice.edu/~linda/">Linda Torczon</a>
            </p>
          </section>
        </section>

        <section>
          <h2>Pragmatic approach</h2>
          <blockquote>
          &ldquo;Programmers waste enormous amounts of time thinking about, or worrying about, the speed of non-critical
          parts of their programs, and these attempts at efficiency actually have a strong negative impact when
          debugging and maintenance are considered. We should forget about small inefficiencies, say about 97% of the time;
          <b>premature optimization is the root of all evil</b>. Yet we should not pass up our opportunities in
          that critical 3%.&ldquo;
            <div style="text-align:right;">
              <small>-Donald Knuth, Structured Programming With go to Statements</small>
            </div>
          </blockquote>
          <ol>
            <li><strong>Find what to start from (3%)</strong></li>
            <li><strong>Know when to stop (97%)</strong></li>
          </ol>
        </section>

        <section>
          <h2>Knowledge which is required</h2>
          <ol>
            <li><b>The code</b>
              <ul>
                <li>The problem, it solves</li>
                <li>The algorithm, it implements</li>
                <li>The algorithmic complexity</li>
              </ul>
            </li>
            <li><b>The compiler</b>
              <ul>
                <li>Compilation trajectory</li>
                <li>Compiler's capabilities and obstacles</li>
              </ul>
            </li>
            <li><b>The platform</b>
              <ul>
                <li>Architecture capabilities
                  <ul>I<small style="vertical-align: bottom">nstruction</small>
                      S<small style="vertical-align: bottom">et</small>
                      A<small style="vertical-align: bottom">rchitecture</small></ul>
                </li>
                <li>Micro-architecture specifics</li>
              </ul>
            </li>
          </ol>
        </section>

        <section>
          <h2>Optimization cycle</h2>
          <img class="simple" src="images/popt/opt_cycle.svg"/>
        </section>

        <section>
          <section>
            <h2>Where to get the performance?</h2>
            <h3>Algorithm</h3>
            <p>Compilers are not aware of semantics of code, taking this into account <b>focus
            on an algorithmic aspect first.</b></p>

            <ul>
              <li>Decrease big-O complexity</li>
              <li>Use optimized libraries for subroutines</li>
              <li>Restructure the code to use fewer resources</li>
              <li>Split problem on subtasks, organize them wisely</li>
              <li>Parallelize</li>
            </ul>
          </section>
          <section>
            <h2>Where to get the performance?</h2>
            <h3>Memory access patterns &amp; operations</h3>
            <p>Compilers are quite good at local optimization, such as</p>
            <ul>
              <li>loop bodies transformations,</li>
              <li>local functions inlining,</li>
              <li>arithmetic expressions simplification</li>
            </ul>
            <p>so <b>help a compiler rather than try to outfox it.</b></p>
            <p>Work cohesively with it on</p>
            <ul>
              <li>enabling auto-vectorization,</li>
              <li>optimizing critical loops,</li>
              <li>vectorizing.</li>
            </ul>
          </section>
          <section>
            <h2>Where to get the performance?</h2>
            <h3>HW-specific optimizations</h3>
            <p>Modern hardware is quite advanced,</p>
            <ul>
              <li>deep pipelines,</li>
              <li>out-of-order execution,</li>
              <li>sophisticated branch prediction,</li>
              <li>multi-level memory hierarchies,</li>
              <li>processor specialization.</li>
            </ul><br>
            <p>so <b>utilize unique properties of the hardware</b>.</p>
            <p>Peephole optimization is not as important as used to be 10 years ago.</p>
          </section>
          <section>
            <h2>Whence get the performance?</h2>
            <br/><br/>
            <table style="font-size:2em;">
              <colgroup>
                <col></col>
                <col></col>
              </colgroup>
              <tbody>
                <tr>
                  <td style="padding:0.1em;">High-level</td>
                  <td style="padding:0.1em;">Programmer</td>
                </tr>
                <tr>
                  <td style="padding:0.1em;">Middle-level</td>
                  <td style="padding:0.1em;">Compiler</td>
                </tr>
                <tr>
                  <td style="padding:0.1em;">Low-level</td>
                  <td style="padding:0.1em;">Hardware</td>
                </tr>
              </tbody>
            </table>
          </section>
        </section>

        <section>
          <h2>Optimization trade-offs</h2>
          <ul>
            <li>Code portability decreases when we go deeper</li>
            <li>Performance portability decreases when we go deeper</li>
            <li>The cost of maintenance &amp; extendability increases when we go deeper</li>
            <li>Optimizations are often not reusable</li>
            <li>Optimizations become obsolete very quickly</li>
          </ul>
          <p><strong>...but still performance is a crucial requirement for most applications.</strong></p>
        </section>

        <section>
          <h2>Top-Down (High-low) approach</h2>
          <ol>
            <li>Understand the code</li>
            <li>Use appropriate algorithms</li>
            <br/>
            <li>Optimize memory access patterns</li>
            <li>Minimize number of operations</li>
            <br/>
            <li>Shrink the critical path</li>
            <li>Perform HW-specific optimizations</li>
            <br/>
            <li>Dive into assembly</li>
          </ol>
        </section>

        <section>
          <h2>step #1: Understand the code</h2>
          <ul>
            <li>Different people think differently
              <ul>
                <li>you'll need some time to get used to the code</li>
              </ul>
            </li><br/>
            <li><b>Understand dataflow</b>
              <ul>
                <li>input/output parameters</li>
                <li>data dependencies</li>
              </ul>
            </li><br/>
            <li><b>Identify performance limiters</b>
              <ul>
                <li>Time</li>
                <li>Profile</li>
                <li>Collect metrics (e.g. power consumption)</li>
              </ul>
            </li>
          </ul>
        </section>

        <section>
          <section>
            <h2>Step #2: use appropriate algorithm</h2>
            <ul>
              <li>Consider and lower big O complexity
            <img width="60%" src="images/popt/Big-O-Time-Complexity-Chart.png"/></li>
              <li>Choose data structures wisely</li>
              <li>Look for optimized libraries</li>
              <li>Find opportunities to scalarize &amp; parallelize</li>
            </ul>
          </section>

          <section>
            <h2>Step #2: use appropriate algorithm</h2>
            <p>You need to sort 100 Mb of numerical data...</p>
            <h3>What sorting algorithm would you choose?</h3>
          </section>
        </section>

        <section>
          <section>
            <h2>Step #3: optimize memory accesses</h2>
            <blockquote>You will be surprised to see<br/>how many algorithms are memory bound</blockquote>
            <p>Optimization for memory usually involves:</p>
            <dl>
              <dt><b>Data restructuring</b></dt>
              <ul>
                <li>to load only data that is really needed for computations.</li>
              </ul>
              <dt><b>Data packaging</b></dt>
                <ul>
                  <li>to shrink the data in size</li>
                </ul>
              </dd>
              <dt><b>Loop transformations</b></dt>
              <ul>
                <li>to walk through the data in a more efficient way,</li>
                <li>to increase temporal &amp; spacial locality,</li>
                <li>to perform cache-aware optimization</li>
              </ul>
            </dl>
          </section>

          <section>
            <h2>Step #3: optimize memory accesses</h2>
            <p><pre><code class="cpp">for (int j = 0; j &lt; height; j++)
  for (int i = 0; i &lt; width; i++)
    if (img[j * width + i] &gt; 0)
      count++;</code></pre></p>
            <p><pre><code class="cpp">for (int i = 0; i &lt; width; i++)
  for (int j = 0; j &lt; height; j++)
    if (img[j * width + i] &gt; 0)
      count++;</code></pre></p>
            <h3>Which is more optimal for conventional CPU processor?</h3>
          </section>
          <section>
            <h2>Step #3: optimize memory accesses</h2>
            <p><pre style="border: 3px double #FF8D14;"><code class="cpp">for (int j = 0; j &lt; height; j++)
  for (int i = 0; i &lt; width; i++)
    if (img[j * width + i] &gt; 0)
      count++;</code></pre></p>
             <p><pre><code class="cpp">for (int i = 0; i &lt; width; i++)
  for (int j = 0; j &lt; height; j++)
    if (img[j * width + i] &gt; 0)
      count++;</code></pre></p>
          </section>
        </section>

        <section>
          <section>
            <h2>Step #4: minimize number of operations</h2>
            <blockquote>Reducing a program in the number of operations
            <br/> does not necessarily decrease its running time,
            <br/>but it is a good heuristic, though.</blockquote>
            <p>The compiler usually helps a lot here:</p>
            <table class="simple">
              <colgroup>
                <col></col>
                <col></col>
              </colgroup>
              <tbody>
                <tr>
                  <td width="50%">
                    <b>Machine-independent optimizations</b>
                    <ul>
                      <li>Common sub-expression elimination</li>
                      <li>Constant propagation</li>
                      <li>Redundancy elimination</li>
                      <li>..</li>
                    </ul>
                  </td>
                  <td>
                    <b>Machine-dependent optimizations</b>
                    <ul>
                      <li>Register allocation</li>
                      <li>Instruction selectIon</li>
                      <li>Instruction scheduling</li>
                      <li>Peephole optimization</li>
                      <li>..</li>
                    </ul>
                  </td>
              </tbody>
            </table>
          </section>
          <section>
            <h2>Step #4: minimize number of operations</h2>
            <img src="images/popt/g2rgba.svg" class="simple" width="55%" />
            <pre><code class="cpp">uint32_t gray2rgba_v1(uint8_t c)
{
  return c + (c&lt;&lt;8) + (c&lt;&lt;16) + (c&lt;&lt;24);
}</code></pre>
            <p><pre><code class="cpp">uint32_t gray2rgba_v2(uint8_t c)
{
  return c * 0x01010101;
}</code></pre></p>
            <h3>Are they same?</h3>
          </section>
          <section>
            <h2>Step #4: minimize number of operations</h2>
            <pre><code class="asm">gray2rgba_v1:
  movzbl  %dil, %eax
  imull $16843009, %eax, %eax
  ret</code></pre>
            <p><pre><code class="asm">gray2rgba_v2:
  movzbl  %dil, %eax
  imull $16843009, %eax, %eax
  ret</code></pre></p>
            <h3>Are they same?</h3>
            <code>$ gcc -O2  -S 1.c -o 1.s ; cat 1.s</code>
          </section>
            <!-- <p>c + (c<<8) + (c<<16) + (c<<24) = c + c*(1<<8) + c*(1<<16) + c*(1<<24) = c*(1 + 1<<8 + 1<<16 + 1<<24)</p> -->

          <section>
            <h2>Step #4: minimize number of operations</h2>
            <p>Unfortunately, sometimes a compiler fails some optimization steps (e.g. register allocation, scalarization)
            and harms the performance by
            introducing redundant operations.</p>
            <p><blockquote><strong>Starting from this optimization step it is worth to <b>look at the assembly code</b> to check
             whether the compiler is actually automating a particular optimization.</strong></blockquote></p>
          </section>
        </section>

        <section>
          <section>
            <h2>Step #5: shrink the critical path</h2>
            <blockquote><strong>Critical path</strong> is <b>the longest</b> sequence of operations in a code block
              that must be completed <b>in order</b>, which is usually caused by dependencies between steps or operations.</dd>
            </blockquote>
            <ul>
              <li>The critical path of a code block is hardly deducible from high-level code and requires assembly code
              inspection.</li>
              <li>Knowledge about architecture capabilities is required to estimate critical path more precisely.</li>
              <li>Some profilers are able to do critical path analysis.</li>
              <li>The term could also refer to the longest sequence of dependent steps in the pipeline that limit
              its parallelization capabilities.</li>
              <li>Finding the critical path of a pipeline requires a construction of a control flow diagram.</li>
            </ul>
          </section>
          <section>
            <h2>Step #5: shrink the critical path</h2>
            <p>Let's look at the critical path of the following code block.</p>
            <p><pre><code class="cpp">const uint8_t* p0 = src.ptr(row0);
const uint8_t* p1 = src.ptr(row1);
uint8_t* dptr = dst.ptr(row);

for (int col = 0; col < cols; ++col)
{
  dptr[col] = (p0[col*2]+p0[col*2+1]
             + p1[col*2]+p1[col*2+1]+2)&gt;&gt;2;
}</code></pre></p>
            <h3>What is the critical path of this code line?</h3>
          </section>
          <section>
            <h2>Step #5: shrink the critical path</h2>
            <p>Let's create 3-positional representation of the code block</p>
            <pre><code class="cpp">r0 = col*2           //  1
r1 = r0+1            //  2
r2 = load(sptr0, r0) //  3
r3 = load(sptr0, r1) //  4
r4 = load(sptr1, r0) //  5
r5 = load(sptr1, r1) //  6
r6 = r2+r3           //  7
r7 = r6+r4           //  8
r8 = r7+r5           //  9
r9 = r8+2            // 10
r10 = shl(r9, 2)     // 11</code></pre>
            <h3>11 ?</h3>
          </section>
          <section>
            <h2>Step #5: shrink the critical path</h2>
            <p>Let's construct the dependency graph</p>
            <img width="70%" class="simple" src="images/popt/path.svg"/>
            <h3>8 ?</h3>
          </section>
          <section>
            <h2>Step #5: shrink the critical path</h2>
            <p>The compiler reorders instructions <br> since integer math is associative</p>
            <img width="70%" class="simple" src="images/popt/path2.svg"/>
            <h3>6 ?</h3>
          </section>
          <section>
            <h2>Step #5: shrink the critical path</h2>
            <p>Let's assume that hardware schedules 1 arithmetic and 1 memory operation per clock.</p>
            <img width="60%" class="simple" src="images/popt/path3.svg"/>
            <h3>and back to 8 again</h3>
          </section>
        </section>

        <section>
          <h2>Step #6: do HW-specific optimization</h2>
          <p>Requires comprehensive understanding of the target HW,<br/> which usually goes beyond compiler's abilities</p>
          <ul>
            <li>Using special hardware capabilities</li>
            <li>Overcoming micro-architecture weakness</li>
            <li>Using instructions, which are specific for concrete HW</li>
            <li>balancing usage of different instruction types</li>
          </ul>
          <br/>
          <p>A classical example here is a question of <strong>recomputing temporal v.s. getting it from the memory</strong>.</p>
        </section>

        <section>
          <h2>Step #7: dive into assembly</h2>
          <blockquote><b>Assembler is a must-have to check the compiler</b> but it is rarely used to write low-level code.</blockquote>
          <p><strong>Raw assembly make sense to:</strong></p>
          <ul>
            <li><b>Overcome compiler bugs & optimization limitations</b>
              <ul>
                <li>addition of redundant instructions</li>
                <li>suboptimal register allocation</li>
              </ul>
            </li>
            <li><b>Use specific hardware features</b>
              <ul>
                <li>which are not expressed in higher level ISA</li>
              </ul>
            </li>
          </ul>
          <p><strong>Keep in mind that:</strong></p>
          <ul>
            <li>Assembly writing is the least portable optimization</li>
            <li>In-line assembly limits compiler optimizations</li>
          </ul>
        </section>

        <section>
          <h2>Summary</h2>
          <ul>
            <li>Practice, look what others do, dig into an architecture.</li>
            <li>Wall time is the most important metric to optimize.</li>
            <li>The main task of an optimizer is finding the critical part.</li>
            <li>Optimizer's mastership is to know where to stop.</li>
            <li>Knowledge about the code, the compiler and the platform is a must-have.</li>
            <li>Optimization is a measure-analyze-optimize-check cycle.</li>
            <li>Stick to the high-to-low approach.</li>
            <li>Get the performance from algorithmic and data structure choices first, ensure memory access patterns next, then go deeper</li>
          </ul>
        </section>

        <section id="end1">
          <h1>THE END</h1>
          <img class="simple" src="images/popt/infinity.png">
          <h4><a href="https://github.com/cuda-geek">Marina Kolpakova</a> / 2015</h4s>
        </section>

      </div>

    </div>

    <script src="plugin/reveal/lib/js/head.min.js"></script>
    <script src="plugin/reveal/js/reveal.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        slideNumber: true,
        history: true,
        center: false,

        width: 960,
        height: 720,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'plugin/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>


