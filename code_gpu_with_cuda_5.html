<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Code GPU with CUDA</title>

    <meta name="description" content="CUDA cource: Code GPU with CUDA">
    <meta name="author" content="cuda.geek">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="3dparty/reveal/css/reveal.css">
    <link rel="stylesheet" href="stylesheets/cuda.geek.css" id="theme">
    <link rel="stylesheet" href="stylesheets/cuda.css">
    <link rel="stylesheet" href="3dparty/reveal/lib/css/zenburn.css">
    <script>
      document.write( '<link rel="stylesheet" href="3dparty/reveal/css/print/' + ( window.location
        .search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>
    <!--[if lt IE 9]>
      <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Code GPU with CUDA</h1>
          <h2>Applying optimization techniques</h2>
          <h3></h3>
          <br>
          <br>
          <br>
          <small>Created by Marina Kolpakova (
            <a href="http://github.com/cuda-geek">cuda.geek</a>
            ) for
            <a href="http://itseez.com">Itseez</a>
          </small>
          <h4><a href="code_gpu_with_cuda_6.html#/end1">previous</a></h4>
        </section>

        <section>
          <section>
            <h2>Outline</h2>
            <ul>
              <li><a href="#/sec_0">Streaming kernels</a>
                <ul>
                  <li>Threshold</li>
                  <li>Transpose</li>
                </ul>
              </li>
              <!-- <li>Stencil kernel</li> -->
              <li><a href="#/sec_1">Reduction</a></li>
              <!-- <li>Scan</li> -->
              <li>Optimizing control flow</li>
            </ul>
          </section>
        </section>

        <section id="sec_0">
          <h1>Streaming kernels</h1>
        </section>

        <section>
          <h2>Streaming kernel</h2>
          <section>
            <blockquote style="width:100%;" cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
              <math display="block">
              <mstyle>
                  <mi>y</mi>
                  <mo>=</mo>
                  <mi>f</mi>
                  <mrow>
                    <mo>(</mo>
                    <mi>x</mi>
                    <mo>)</mo>
                  </mrow>
                </mstyle>
              </math>
            </blockquote>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp">template &lt;typename Ptr2DIn, typename Ptr2DOut, typename Op&gt;
__global__ void streaming(const Ptr2DIn src, Ptr2DOut dst)
{
     const int x = <span class="keyword">blockDim.x</span> * <span class="keyword">blockIdx.x</span> + <span class="keyword">threadIdx.x</span>;
     const int y = <span class="keyword">blockDim.y</span> * <span class="keyword">blockIdx.y</span> + <span class="keyword">threadIdx.y</span>;

     if (x < dst.cols && y < dst.rows)
         dst(y, x) = saturate_cast&lt;Ptr2DOut::elem_type&gt;(Op::apply(src(x, y)));
}</code></pre>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp"><span class="keyword">dim3</span> block(block_x, block_y);
<span class="keyword">dim3</span> grid(roundUp(dst.cols, block_x), roundUp(dst.rows, block_y));
streaming<span class="keyword">&lt;&lt;&lt;</span>grid, block<span class="keyword">&gt;&gt;&gt;</span>(src, dst);</code></pre>
            <p>General arithmetic and conversions, repack by map, resize, etc.</p>
          </section>
        </section>

        <section>
          <h2>Threshold</h2>
          <section>
            <h3>Pixel per thread</h3>
            <blockquote style="width:100%;" cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
              <math display="block">
                <mstyle>
                  <mi>y</mi>
                  <mo>=</mo>
                  <mi>max</mi>
                  <mrow>
                    <mo>(</mo>
                    <mi>x</mi>
                    <mo>,</mo>
                    <mi>&#964;</mi>
                    <mo>)</mo>
                  </mrow>
                </mstyle>
              </math>
            </blockquote>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp"><span class="keyword">__global__</span> threshold_bw(const <span class="cls">DPtrb</span> src, <span class="cls">DPtrb</span> dst, <span class="cls">int32s</span> cols, <span class="cls">int32s</span> rows, <span class="cls">int8u</span> thr)
{
     const int x = <span class="keyword">blockDim.x</span> * <span class="keyword">blockIdx.x</span> + <span class="keyword">threadIdx.x</span>;
     const int y = <span class="keyword">blockDim.y</span> * <span class="keyword">blockIdx.y</span> + <span class="keyword">threadIdx.y</span>;

     if (x < cols && y < rows)
         dst.row(y)[x] = max(thr, src.row(y)[x]);
}</code></pre>
            <small><a href="https://github.com/cuda-geek/cumib/blob/master/threshold.cu#L24">The code is available in cumib microbenchmars library</a></small>
            <p>Adjusting launch parameters for specific hardware</p>
          </section>
          <section>
            <h3>Pixel per thread: results</h3>
            <table class="tbl1" style="width:100%;">
              <colgroup>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
              </colgroup>
              <thead>
                <tr>
                  <th>block size</th>
                  <th>GK107*, μs</th>
                  <th>X-factor</th>
                  <th>GM107**, μs</th>
                  <th>X-factor</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th>32x16</th><td>456.49</td><td>1.00</td><td>214.94</td><td>1.00</td>
                </tr>
                <tr>
                  <th>32x8</th><td>431.42</td><td>1.06</td><td>210.39</td><td>1.02</td>
                </tr>
                <tr>
                  <th>32x6</th><td>435.87</td><td>1.05</td><td>226.02</td><td>0.95</td>
                </tr>
                <tr>
                  <th>32x4</th><td>412.01</td><td>1.11</td><td>222.11</td><td>0.98</td>
                </tr>
                <tr>
                  <th>32x2</th><td>785.36</td><td>0.58</td><td>228.28</td><td>0.94</td>
                </tr>
                <tr>
                  <th>32x1</th><td>1516.19</td><td>0.30</td><td>419.94</td><td>0.51</td>
                </tr>
              </tbody>
            </table>
            <small>* Time has been measured on 1080p input for GK107 with 2 SMX (1.0 GHz), 128-bit GDDR5
            <br/>** Time has been measured on 1080p input for GM107 with 5 SMX (0.5 GHz), 128-bit GDDR5</small>
          </section>

          <section>
            <h3>Kernel unrolling: image row per warp</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp"><span class="keyword">__global__</span> threshold_bw(const <span class="cls">DPtrb</span> src, <span class="cls">DPtrb</span> dst, <span class="cls">int32s</span> cols, <span class="cls">int32s</span> rows, <span class="cls">int8u</span> thr)
{
    int block_id = (<span class="keyword">blockIdx.y</span> * <span class="keyword">gridDim.x</span> + <span class="keyword">blockIdx.x</span>) * (<span class="keyword">blockDim.x</span> / <span class="keyword">warpSize</span>);
    int y = (<span class="keyword">threadIdx.y</span> * <span class="keyword">blockDim.x</span> + <span class="keyword">threadIdx.x</span>) / <span class="keyword">warpSize</span> + block_id;

    if (y < rows)
        for (int x = lane(); x < cols; x += <span class="keyword">warpSize</span>)
            dst.row(y)[x] = max(thr, src.row(y)[x]);
}</code></pre>
            <small><a href="https://github.com/cuda-geek/cumib/blob/master/threshold.cu#L43">The code is available in cumib microbenchmars library</a></small>
            <table class="tbl1" style="width:100%;">
              <colgroup>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
              </colgroup>
              <thead>
                <tr>
                  <th>block size</th>
                  <th>GK107*, μs</th>
                  <th>X-factor</th>
                  <th>GM107**, μs</th>
                  <th>X-factor</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th>blockwise 32x4</th><td>412.01</td><td>1.0</td><td>226.02</td><td>1.00</td>
                </tr>
                <tr>
                  <th>warpwise 8</th><td>223.01</td><td>1.85</td><td>265.13</td><td>0.85</td>
                </tr>
                <tr>
                  <th>warpwise 4</th><td>222.53</td><td>1.85</td><td>248.04</td><td>0.91</td>
                </tr>
                <tr>
                  <th>warpwise 2</th><td>374.47</td><td>1.10</td><td>246.83</td><td>1.92</td>
                </tr>
              </tbody>
            </table>
            <small>* Time has been measured on 1080p input for GK107 with 2 SMX (1.0 GHz), 128-bit GDDR5
            <br/>** Time has been measured on 1080p input for GM107 with 5 SMX (0.5 GHz), 128-bit GDDR5</small>
          </section>

          <section>
            <h3>Kernel unrolling: more independent elements</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp">// same as previous
unsigned char tmp[2];
if (y * 2 < rows - 2)
    for (int x = lane(); x < cols; x += warpSize)
    {
        tmp[0] = max(threshold, src.row(y * 2)[x]);
        tmp[1] = max(threshold, src.row(y * 2+ 1)[x]);

        dst.row(y * 2)[x] = tmp[0];
        dst.row(y * 2 + 1)[x] = tmp[1];
    }
else {/*compute tail*/ }</code></pre>
            <small><a href="https://github.com/cuda-geek/cumib/blob/master/threshold.cu#L61">The code is available in cumib microbenchmars library</a></small>
            <table class="tbl1" style="width:100%;">
              <colgroup>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
              </colgroup>
              <thead>
                <tr>
                  <th>block size</th>
                  <th>GK107*, μs</th>
                  <th>X-factor</th>
                  <th>GM107**, μs</th>
                  <th>X-factor</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th>blockwise 32x4</th><td>412.01</td><td>1.0</td><td>226.02</td><td>1.10</td>
                </tr>
                <tr>
                  <th>warpwise 4</th><td>222.53</td><td>1.85</td><td>248.04</td><td>0.91</td>
                </tr>
                <tr>
                  <th>warpwise 4 U2</th><td>185.28</td><td>2.22</td><td>269.75</td><td>0.83</td>
                </tr>
            </tbody>
            </table>
            <small>* Time has been measured on 1080p input for GK107 with 2 SMX (1.0 GHz), 128-bit GDDR5
            <br/>** Time has been measured on 1080p input for GM107 with 5 SMX (0.5 GHz), 128-bit GDDR5</small>
          </section>

          <section>
            <h3>Kernel unrolling: use wider transactions</h3>
            <p>unsigned char -&gt; unsigned integer</p>
            <pre style="width:100%"><code class="cpp" style="padding: 10px;">template&lt;typename T&gt; <span class="keyword">__device__ __forceinline__</span> T vmin_u8(T,T);
template&lt;&gt; <span class="keyword">__device__ __forceinline__</span> unsigned int vmin_u8(unsigned v, unsigned m)
{
    unsigned int res = 0;
    asm("vmax4.u32.u32.u32 %0, %1, %2, %3;" : "=r"(res) : "r"(v), "r"(m), "r"(0));
    return res;
}</code></pre>
            <pre style="width:100%"><code class="cpp" style="padding: 10px;">// same as previous
if (y < rows)
    for (int x = lane(); x < cols / sizeof(int8u); x += warpSize)
    {
        int32u tmp = src.row&lt;unsigned int&gt;(y)[x];
        int32u res = vmin_u8(tmp, mask);
        dst.row&lt;unsigned int&gt;(y)[x] = res;
    }</code></pre>
          <small><a href="https://github.com/cuda-geek/cumib/blob/master/threshold.cu#L114">The code is available in cumib microbenchmars library</a></small>
          </section>

          <section>
            <h3>Wider transactions : results</h3>
            <table class="tbl1" style="width:100%;">
              <colgroup>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
              </colgroup>
              <thead>
                <tr>
                  <th>block size</th>
                  <th>GK107*, μs</th>
                  <th>X-factor</th>
                  <th>GM107**, μs</th>
                  <th>X-factor</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th>blockwise 32x4</th><td>412.01</td><td>1.0</td><td>226.02</td><td>1.10</td>
                </tr>
                <tr>
                  <th>warpwise 4</th><td>222.53</td><td>1.85</td><td>248.04</td><td>0.91</td>
                </tr>
                <tr>
                  <th>warpwise 4 U2</th><td>185.28</td><td>2.22</td><td>269.75</td><td>0.83</td>
                </tr>
                <tr>
                  <th>warpwise 2W </th><td>126.01</td><td>3.27</td><td>245.21</td><td>0.92</td>
                </tr>
                <tr>
                  <th>warpwise 2WU2</th><td>96.71</td><td>4.26</td><td>162.83</td><td>1.39</td>
                </tr>
                <tr>
                  <th>warpwise 4W </th><td>85.22</td><td>4.83</td><td>257.69</td><td>0.88</td>
                </tr>
                <tr>
                  <th>warpwise 4WU2</th><td>83.42</td><td>4.93</td><td>161.18</td><td>1.40</td>
                </tr>
              </tbody>
            </table>
            <small>* Time has been measured on 1080p input for GK107 with 2 SMX (1.0 GHz), 128-bit GDDR5
            <br/>** Time has been measured on 1080p input for GM107 with 5 SMX (0.5 GHz), 128-bit GDDR5</small>
          </section>
        </section>

        <section>
          <h2>Transpose</h2>
          <section>
            <blockquote style="width:100%;" cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
              <math display="block">
                <mstyle>
                  <msub>
                    <mo>D</mo>
                    <mrow>
                      <mi>i,j</mi>
                    </mrow>
                  </msub>
                  <mo>=</mo>
                  <msub>
                    <mo>S</mo>
                    <mrow>
                      <mi>j,i</mi>
                    </mrow>
                  </msub>
                </mstyle>
              </math>
            </blockquote>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp">template &lt;typename T&gt;<span class="keyword">__global__</span>
void transposeNaive(const <span class="cls">DPtr</span>&lt;T&gt; idata, <span class="cls">DPtr</span>&lt;T&gt; odata, int cols, int rows)
{
    int xIndex = <span class="keyword">blockIdx.x</span> * <span class="keyword">blockDim.x</span> + <span class="keyword">threadIdx.x</span>;
    int yIndex = <span class="keyword">blockIdx.y</span> * <span class="keyword">blockDim.y</span> + <span class="keyword">threadIdx.y</span>;

    odata.row(xIndex)[yIndex] = idata.row(yIndex)[xIndex];
}</code></pre>
            <small><a href="https://github.com/cuda-geek/cumib/blob/master/transpose.cu#L124">The code is available in cumib microbenchmars library</a></small>
            <img src="images/c3/tr_mat.svg" width="60%" class="simple">
          </section>

          <section>
            <h3>Coalesce memory access: smem usage</h3>
            <ul>
              <li>Split the input matrix into tiles, assigning <b>one thread block for one tile</b>. Tile size (in elements) and block size (in threads) are not necessarily the same.</li>
              <li>Load tile in coalesced fashion to smem -> read from smem by column -> write to destination in coalesced fashion.</li>
            </ul>
            <img src="images/c3/transpose.svg" class="simple">
          </section>

          <section>
            <h3>Coalesce memory access: smem usage code</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp">template &lt;typename T&gt;<span class="keyword">__global__</span>
void transposeCoalesced(const DPtr&lt;T&gt; idata, DPtr&lt;T&gt; odata, int cols, int rows)
{
    <span class="keyword">__shared__</span> float tile[TRANSPOSE_TILE_DIM][TRANSPOSE_TILE_DIM];

    int xIndex = <span class="keyword">blockIdx.x</span> * TRANSPOSE_TILE_DIM + <span class="keyword">threadIdx.x</span>;
    int yIndex = <span class="keyword">blockIdx.y</span> * TRANSPOSE_TILE_DIM + <span class="keyword">threadIdx.y</span>;
    for (int i = 0; i < TRANSPOSE_TILE_DIM; i += TRANSPOSE_BLOCK_ROWS)
        tile[<span class="keyword">threadIdx.y</span> + i][<span class="keyword">threadIdx.x</span>] = idata.row(yIndex + i)[xIndex];

    __syncthreads();

    xIndex = <span class="keyword">blockIdx.y</span> * TRANSPOSE_TILE_DIM + <span class="keyword">threadIdx.x</span>;
    yIndex = <span class="keyword">blockIdx.x</span> * TRANSPOSE_TILE_DIM + <span class="keyword">threadIdx.y</span>;
    for (int i = 0; i < TRANSPOSE_TILE_DIM; i += TRANSPOSE_BLOCK_ROWS)
        odata.row(yIndex + i)[xIndex] = tile[<span class="keyword">threadIdx.x</span>][<span class="keyword">threadIdx.y</span> + i];
}</code></pre>
          <small><a href="https://github.com/cuda-geek/cumib/blob/master/transpose.cu#L167">The code is available in cumib microbenchmars library</a></small>
          </section>

          <section>
            <h3>Smem accesses: avoid bank conflicts</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp">template &lt;typename T&gt;<span class="keyword">__global__</span>
void transposeCoalescedPlus1(const DPtr&lt;T&gt; idata, DPtr&lt;T&gt; odata, int cols, int rows)
{
    <span class="keyword">__shared__</span> float tile[TRANSPOSE_TILE_DIM][TRANSPOSE_TILE_DIM <b>+ 1</b>];

    int xIndex = <span class="keyword">blockIdx.x</span> * TRANSPOSE_TILE_DIM + <span class="keyword">threadIdx.x</span>;
    int yIndex = <span class="keyword">blockIdx.y</span> * TRANSPOSE_TILE_DIM + <span class="keyword">threadIdx.y</span>;
    for (int i = 0; i < TRANSPOSE_TILE_DIM; i += TRANSPOSE_BLOCK_ROWS)
        tile[<span class="keyword">threadIdx.y</span> + i][<span class="keyword">threadIdx.x</span>] = idata.row(yIndex + i)[xIndex];

    __syncthreads();

    xIndex = <span class="keyword">blockIdx.y</span> * TRANSPOSE_TILE_DIM + <span class="keyword">threadIdx.x</span>;
    yIndex = <span class="keyword">blockIdx.x</span> * TRANSPOSE_TILE_DIM + <span class="keyword">threadIdx.y</span>;
    for (int i = 0; i < TRANSPOSE_TILE_DIM; i += TRANSPOSE_BLOCK_ROWS)
        odata.row(yIndex + i)[xIndex] = tile[<span class="keyword">threadIdx.x</span>][<span class="keyword">threadIdx.y</span> + i];
}</code></pre>
            <small><a href="https://github.com/cuda-geek/cumib/blob/master/transpose.cu#L202">The code is available in cumib microbenchmars library</a></small>
          </section>

          <section>
            <h3>Warp shuffle</h3>
            <img src="images/c2/shuffle.png" class="simple">
          </section>

          <section>
            <h3>Transpose shuffle</h3>
            <img src="images/c3/traspose_shuffle.svg" class="simple">
          </section>

          <section>
            <h3>Transpose shuffle code</h3>
            <pre style="width:100%;"><code class="cpp"><span class="keyword">__global__</span> void transposeShuffle(
    const <span style="color:#FF9C33;">DPtr32</span> idata, <span style="color:#FF9C33;">DPtr32</span> odata, int cols, int rows)
{
    int xIndex = <span class="keyword">blockIdx.x</span> * <span class="keyword">blockDim.x</span> + <span class="keyword">threadIdx.x</span>;
    int yIndex = <span class="keyword">blockIdx.y</span> * <span class="keyword">blockDim.y</span> + <span class="keyword">threadIdx.y</span>;
    int yIndex1 = yIndex * SHUFFLE_ELEMENTS_VECTORS;

    yIndex *= SHUFFLE_ELEMENTS_PERF_WARP;

    int4 reg0, reg1;

    reg0.x = idata.row(yIndex + 0)[xIndex]; reg0.y = idata.row(yIndex + 1)[xIndex];
    reg0.z = idata.row(yIndex + 2)[xIndex]; reg0.w = idata.row(yIndex + 3)[xIndex];

    reg1.x = idata.row(yIndex + 4)[xIndex]; reg1.y = idata.row(yIndex + 5)[xIndex];
    reg1.z = idata.row(yIndex + 6)[xIndex]; reg1.w = idata.row(yIndex + 7)[xIndex];
</code></pre>
          <p>continued on the next slide...</p>
          </section>
          <section>
            <h3>Transpose shuffle code (cont.)</h3>
            <pre style="width:100%;"><code class="cpp">
    unsigned int isEven = laneIsEven<unsigned int>();
    int4 target = isEven ? reg1 : reg0;

    target.x = <span style="color:#66FF33;">__shfl_xor</span>(target.x, 1);
    target.y = <span style="color:#66FF33;">__shfl_xor</span>(target.y, 1);
    target.z = <span style="color:#66FF33;">__shfl_xor</span>(target.z, 1);
    target.w = <span style="color:#66FF33;">__shfl_xor</span>(target.w, 1);

    const int oIndexY = <span class="keyword">blockIdx.x</span> * <span class="keyword">blockDim.x</span> + (<span class="keyword">threadIdx.x</span> >> 1) * 2;
    const int oIndexX = yIndex1 + (isEven == 0);

    if (isEven) reg1 = target; else reg0 = target;

    odata(oIndexY + 0, oIndexX, reg0);
    odata(oIndexY + 1, oIndexX, reg1);
}</code></pre>
            <small><a href="https://github.com/cuda-geek/cumib/blob/master/transpose.cu#L245">The code is available in cumib microbenchmars library</a></small>
          </section>

          <section>
            <h3>Results</h3>
              <table class="tbl1" style="width:100%;">
                <colgroup>
                  <col></col>
                  <col></col>
                  <col></col>
                  <col></col>
                </colgroup>
                <thead>
                  <tr>
                    <th>approach / time, ms</th>
                    <th>GK107*</th>
                    <th>GK20A**</th>
                    <th>GM107***</th>
                  </tr>
                </thead>
                <tbody>
                  <tr><th>Copy</th>                    <td>0.486</td><td>2.182</td><td>0.658</td></tr>
                  <tr><th>CopySharedMem</th>           <td>0.494</td><td>2.198</td><td>0.623</td></tr>
                  <tr><th>CopySharedMemPlus1</th>      <td>0.500</td><td>2.188</td><td>0.691</td></tr>
                  <tr><th>TransposeCoalescedPlus1</th> <td>0.569</td><td>2.345</td><td>0.631</td></tr>
                  <tr><th>TransposeCoalesced</th>      <td>0.808</td><td>3.274</td><td>0.771</td></tr>
                  <tr><th>TransposeShuffle</th>        <td>1.253</td><td>2.352</td><td>0.689</td></tr>
                  <tr><th>TransposeNaive</th>          <td>1.470</td><td>5.338</td><td>1.614</td></tr>
                  <tr><th>TransposeNaiveBlock</th>     <td>1.735</td><td>5.477</td><td>1.451</td></tr>
                </tbody>
              </table>
            <small>* Time has been measured on 1080p input for GK107 with 2 SMX (1 GHz), 128-bit GDDR5<br>
            ** Time has been measured on 1080p input for GK20A with 1 SMX (0.6 GHz)<br>
            *** Time has been measured on 1080p input for GM107 with 5 SMX (0.5 GHz), 128-bit GDDR5
            </small>
          </section>
        </section>

<!--      <section>
            <h2>Resize area</h2>
          </section> -->

          <section>
            <h2>when use kernel fusion?</h2>
            <ul>
              <li>Batch of small kernels
                <ul>
                  <li>competitive solution for kernel unrolling since it improves <b>instruction per byte</b> ratio</li>
                </ul>
              </li>
              <li>Append one or more small kernels to register-heavy kernel
                <ul>
                  <li>might affect kernel unrolling factors or launch parameters</li>
                </ul>
              </li>
            </ul>
          </section>

 <!--                <section>
                    <h2>Stencil kernel</h2>
                    <blockquote style="width:100%;" cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
                        <math xmlns="http://www.w3.org/1998/Math/MathML">
                                <mrow>
                                    <munderover>
                                        <mo>&Sum;</mo>
                                        <mrow>
                                            <msub>
                                                <mi>y</mi>
                                                <mrow>
                                                    <mi>1</mi>
                                                </mrow>
                                            </msub>
                                        </mrow>
                                        <mrow>
                                            <msub>
                                                <mi>y</mi>
                                                <mrow>
                                                    <mi>2</mi>
                                                </mrow>
                                            </msub>
                                        </mrow>
                                    </munderover>
                                    <munderover>
                                        <mo>&Sum;</mo>
                                        <mrow>
                                            <msub>
                                                <mi>x</mi>
                                                <mrow>
                                                    <mi>1</mi>
                                                </mrow>
                                            </msub>
                                        </mrow>
                                        <mrow>
                                            <msub>
                                                <mi>x</mi>
                                                <mrow>
                                                    <mi>2</mi>
                                                </mrow>
                                            </msub>
                                        </mrow>
                                    </munderover>
                                    <mrow>
                                        <mi>f</mi>
                                        <mrow>
                                            <mo>(</mo>
                                            <mi>I</mi>
                                            <mo>)</mo>
                                        </mrow>
                                    </mrow>
                                </mrow>
                            </math>
                    </blockquote>
                        <pre style="width:100%;"><code style="padding: 10px;" class="cpp">
</code></pre>
                </section>

                <section>
                    <h2>Filters</h2>
                </section>
 -->
        <section id="sec_1">
          <h1>Reduction</h1>
        </section>

        <section>
          <h2>Reduction</h2>
          <blockquote style="width:100%" cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mo>S</mo>
                <mi>=</mi>
                <munderover>
                  <mo>&Sum;</mo>
                  <mrow>
                    <mi>i</mi>
                    <mi>=</mi>
                    <mi>1</mi>
                  </mrow>
                  <mrow>
                    <mi>n</mi>
                  </mrow>
                </munderover>
                <mrow>
                  <mi>f</mi>
                  <mrow>
                    <mo>(</mo>
                    <msub>
                      <mi>I</mi>
                      <mrow>
                        <mi>i</mi>
                      </mrow>
                    </msub>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </mrow>
            </math>
          </blockquote>
          <pre style="width:100%;"><code style="padding: 10px;" class="cpp"><span class="keyword">__global__</span> void reduceNaive(const int *idata, int *odata, <span class="keyword">size_t</span> N )
{
    int partial = 0;
    <span class="keyword">size_t</span> i = <span class="keyword">blockIdx.x</span> * <span class="keyword">blockDim.x</span> + <span class="keyword">threadIdx.x</span>;
    for ( ; i < N; i += <span class="keyword">blockDim.x</span> * <span class="keyword">gridDim.x</span> )
        partial += idata[i];

    <span style="color:#66FF33;">atomicAdd</span>( odata, partial );
}
</code></pre>
          <p>Naive implementation with CUDA atomics is limited by write queue atomic throughput,<br/>therefore <b>hierarchical approaches</b> are used:</p>
          <ul>
            <li>Grid level: meta reduction approach</li>
            <li>Block level: block reduction approach</li>
          </ul>
        </section>

        <section>
          <h2>Meta reduction approaches</h2>
          <ul>
            <li><b>Tree</b>
              <ul>
                <li>divide array of <b>N</b> elements by factor <b>b</b> (block size). Problem size grows with <b>N</b>. Number of blocks used:
                  <b>
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
                      <mrow>
                        <mfrac>
                          <mrow>
                            <mi>(N - 1)</mi>
                          </mrow>
                          <mrow>
                            <mn>(b - 1)</mn>
                          </mrow>
                        </mfrac>
                      </mrow>
                    </math>
                  </b>
                </li>
              </ul>
            </li>
            <li><b>2-level</b>
              <ul>
                <li>Use constant number of blocks <b>C</b>. Each block processes <b>N</b>/<b>C</b> elements.
                Problem size independent form <b>N</b>, <b>C</b> is hardware dependent heuristic == block size <b>b</b>.
                Number of blocks used:
                  <b>
                    <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
                      <mrow>
                        <mi>(C + 1)</mi>
                      </mrow>
                    </math>
                  </b>
                </li>
              </ul>
            </li>
            <li><b>Constant &amp; atomic</b>
              <ul>
                <li>one level of reduction, fixed number of blocks <b>C</b>. Each block performs block-wide reduction and stores results to gmem using atomic write.</li>
              </ul>
            </li>
          </ul>
        </section>
        <section>
          <h2>Block reduction approaches</h2>
          <section>
            <h3>Ranking</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp"><span class="keyword">__global__</span> void reduceRanking(const int *idata, int *odata, <span class="keyword">size_t</span> N )
{
    extern <span class="keyword">__shared__</span> int* partials;
    int thread_partial = thread_reduce(idata, N);
    partials[<span class="keyword">threadIdx.x</span>] = sum;
    __syncthreads();
    if (<span class="keyword">threadIdx.x</span> &lt; 512) partials[<span class="keyword">threadIdx.x</span>] += partials[<span class="keyword">threadIdx.x</span> + 512];
    __syncthreads();
    if (<span class="keyword">threadIdx.x</span> &lt; 256) partials[<span class="keyword">threadIdx.x</span>] += partials[<span class="keyword">threadIdx.x</span> + 256];
    __syncthreads();
    if (<span class="keyword">threadIdx.x</span> &lt; 128) partials[<span class="keyword">threadIdx.x</span>] += partials[<span class="keyword">threadIdx.x</span> + 128];
     __syncthreads();
    if (<span class="keyword">threadIdx.x</span> &lt; 64)  partials[<span class="keyword">threadIdx.x</span>] += partials[<span class="keyword">threadIdx.x</span> + 64];
    __syncthreads();
    if (<span class="keyword">threadIdx.x</span> &lt; 32) warp_reduce(partials);
    if (!<span class="keyword">threadIdx.x</span>) odata[blockIdx.x] = partials[0];
}
</code></pre>
          </section>
          <section>
            <h3>Warp reduce</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp">template&lt;int SIZE&gt;<span class="keyword">
__device__ __forceinline__</span> void warp_reduce(int val, volatile int* smem)
{
    #pragma unroll
    for (int offset = SIZE &gt;&gt; 1; offset &gt;= 1 ; offset &gt;&gt;= 1 )
        val += __shfl_xor( val, offset );

    int warpId = warp::id();
    int laneId = warp::lane();
    if (!laneId) smem[warpId] = val;
}</code></pre>
          </section>
          <section>
            <h3>warp-centric reduction</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp"><span class="keyword">__global__</span> void reduceWarpCentric(const int *idata, int *odata, <span class="keyword">size_t</span> N )
{
    <span class="keyword">__shared__</span> int partials[WARP_SIZE];
    int thread_partial = thread_reduce(idata, N);
    warp_reduce&lt;WARP_SIZE&gt;(thread_partial, partials);
    __syncthreads();

    if (!warp::id())
      warp_reduce&lt;NUM_WARPS_IN_BLOCK&gt;(partials[<span class="keyword">threadIdx.x</span>], partials);
    if (!<span class="keyword">threadIdx.x</span>) odata[blockIdx.x] = partials[0];
}</code></pre>
          </section>
          <section>
            <h3>Results</h3>
            <table class="tbl1" style="width:100%;">
              <colgroup>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
                <col></col>
              </colgroup>
              <thead>
                <tr>
                  <th>approach</th>
                  <th>Block</th>
                  <th>transaction, bit</th>
                  <th>bandwidth*, GB/s</th>
                </tr>
              </thead>
              <tbody>
                <tr><th>warp-centric</th><td>32</td><td>32</td><td>32.58</td></tr>
                <tr><th>warp-centric</th><td>128</td><td>32</td><td>56.07</td></tr>
                <tr><th>warp-centric</th><td>256</td><td>32</td><td>56.32</td></tr>
                <tr><th>warp-centric</th><td>32</td><td>128</td><td>44.22</td></tr>
                <tr><th>warp-centric</th><td>128</td><td>128</td><td>56.10</td></tr>
                <tr><th>warp-centric</th><td>256</td><td>128</td><td>56.74</td></tr>

                <tr><th>ranking</th><td>32</td><td>32</td><td>32.32</td></tr>
                <tr><th>ranking</th><td>128</td><td>32</td><td>55.16</td></tr>
                <tr><th>ranking</th><td>256</td><td>32</td><td>55.36</td></tr>
                <tr><th>ranking</th><td>32</td><td>128</td><td>44.00</td></tr>
                <tr><th>ranking</th><td>128</td><td>128</td><td>56.56</td></tr>
                <tr><th>ranking</th><td>256</td><td>128</td><td>57.12</td></tr>
<!--                 <tr><th>warp-centric</th><td>32</td><td>1</td><td>8</td><td>24.02</td></tr>
                <tr><th>ranking</th><td>64</td><td>2</td><td>12</td><td>63.13</td></tr>
                <tr><th>ranking</th><td>128</td><td>1</td><td>24</td><td>66.83</td></tr>
                <tr><th>warp-centric</th><td>160</td><td>4</td><td>20</td><td>67.09</td></tr> -->
              </tbody>
            </table>
            <small>* Bandwidth has been measured on 1080p input for GK208 with 2 SMX (1 GHz), 64-bit GDDR5</small>
          </section>
        </section>

<!--                 <section>
                    <h2>Scan</h2>
                    <section>
                        <blockquote style="width:100%;" cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations">
                            <math xmlns="http://www.w3.org/1998/Math/MathML">
                                <mrow>
                                    <mo>[</mo>
                                    <msub><mi>x</mi><mrow><mi>0</mi></mrow></msub>
                                    <mo>,</mo>
                                    <msub><mi>x</mi><mrow><mi>1</mi></mrow></msub>
                                    <mo>,</mo>
                                    <mo>...</mo>
                                    <mo>,</mo>
                                    <msub><mi>x</mi><mrow><mi>n</mi><mo>-</mo><mi>1</mi></mrow></msub>
                                    <mo>]</mo>
                                    <mi>&#8594;</mi>
                                    <mo>[</mo>
                                    <mi>0</mi>
                                    <mo>,</mo>
                                    <msub><mi>x</mi><mrow><mi>0</mi></mrow></msub>
                                    <mo>,</mo>
                                    <mrow>
                                        <mo>[</mo>
                                        <msub><mi>x</mi><mrow><mi>0</mi></mrow></msub>
                                        <mo>&#x2297;</mo>
                                        <msub><mi>x</mi><mrow><mi>1</mi></mrow></msub>
                                        <mo>]</mo>
                                    </mrow>
                                    <mo>,</mo>
                                    <mo>...</mo>
                                    <mo>,</mo>
                                    <mrow>
                                        <mo>[</mo>
                                        <msub><mi>x</mi><mrow><mi>0</mi></mrow></msub>
                                        <mo>&#x2297;</mo>
                                        <msub><mi>x</mi><mrow><mi>1</mi></mrow></msub>
                                        <mo>&#x2297;</mo>
                                        <mo>...</mo>
                                        <mo>&#x2297;</mo>
                                        <msub><mi>x</mi><mrow><mi>n-1</mi></mrow></msub>
                                        <mo>]</mo>
                                    </mrow>
                                    <mo>]</mo>
                                </mrow>
                            </math>
                        </blockquote>
                        <p>2-step Blelloch scan approach</p>
                        <img src="img/blelloch.png" style="padding:10px;">
                    </section>
                    <section>
                        <h3>Classic meta-scan</h3>
                        <img src="img/merill_scan.png" class="simple">
                    </section>
                    <section>
                        <h3>Reduce-then-scan</h3>
                        <img src="img/merill_reduce_scan.png" class="simple">
                    </section>
                    <section>
                        <h3>Merill's approach: 2-levels reduce-then-scan</h3>
                        <img src="img/merill_reduce_scan_2.png" class="simple">
                    </section> -->
<!--                     <section>
                        <h3>Block-wide scan</h3>
                    </section> -->
<!--                     <section>
                        <h3>Integral image</h3>
                    </section> -->
                <!-- </section> -->
        <section>
          <h1>Optimizing<br>Control flow</h1>
        </section>

        <section>
          <h2>Sliding window detector</h2>
          <section>
            <img src="images/c3/slw.png" class="simple" width="50%" style="display:inline;">
            <pre style="width:40%;display:inline;float:right;"><code>

function isPedestrain(x)
d ← 0
for t = 1...T do
    d ← d + C(x)
    if d < rt then
        return false
    end if
end for
return true
end function

</code></pre>
          </section>
          <section>
            <h3>Thread per window</h3>
            <img class="simple" src="images/c3/tpp.svg">
          </section>
          <section>
            <h3>Thread per window: analysis</h3>
            <ul>
                <li>Coalesced access to gmem in the beginning</li>
                <li>Sparse access to the latest stages</li>
                <li>Unbalanced workload. Time of block residence on SM is
                <br><b>T(b) = max{T(w_0) , ..., T(w_blok_size) }</b></li>
                <li>Warp processes 32 sequential window positions, so it likely diverge</li>
            </ul>
          </section>
          <section>
            <h3>Warp per window</h3>
            <img class="simple" src="images/c3/wpp.svg">
          </section>
          <section>
            <h3>Warp per window: analysis</h3>
            <ul>
              <li>All lanes in a warp load different features. Access pattern is random</li>
              <li>Textures are used to amortize random pattern</li>
              <li>Work balanced for a warp. Warps in a block compute neighboring windows . Likelihood that block needs same number of features is hight</li>
              <li>Use warp-wide prefix sum for decision making.</li>
            </ul>
            <pre style="width:100%"><code class="cpp">for ( int offset = ExecutionPolicy::warp_pixels; offset < 32; offset *= 2)
{
    asm volatile  ( " { "
    " .reg .f32 r0 ; "
    " .reg .pred p ; "
    " shfl.up.b32 r0|p , %0 , %1 , 0x0 ; "
    " @p add.f32 r0 , r0 , %0; "
    " mov . f32 %0 , r0 ; "
    " } " : " +f " ( impact ) : "r" ( offset ) ) ;
}
</code></pre>
                </section>
                <section>
                    <h3>Warp per N windows</h3>
                    <img class="simple" src="images/c3/wp4p.svg">
                </section>
                <section>
                    <h3>Warp per N windows: analysis, N = 4</h3>
                    <pre style="width:100%"><code class="cpp"><span class="keyword">__device__ __forceinline__</span> static int pixel() { return <span class="keyword">threadIdx.x</span> &amp; 3; }
<span class="keyword">__device__ __forceinline__</span> static int stage() { return <span class="keyword">threadIdx.x</span> &gt;&gt; 2; }</code></pre>
                    <ul>
                        <li>Each warp loads 8 features for 4 windows. Each feature consists of 4 pixels. Total warp transactions:
                        8x4 of 16 bytes (instead of 32x4 of 4 byte).</li>
                        <li>Warp is active while at least one of window positions is active.</li>
                    </ul>
                    <pre style="width:100%"><code class="cpp">uint desision = (confidence + impact > trace[t]);
uint mask = __ballot(desision);
uint pattern   = 0x11111111 << pixel;

if ( active && (__popc(mask & pattern) != 8)) active = 0;
if (__all(!active)) break;</code></pre>
                </section>
                <section>
                    <h3>Results</h3>
                    <table class="tbl1" style="width:100%;">
                        <colgroup>
                            <col></col>
                            <col></col>
                            <col></col>
                            <col></col>
                            <col></col>
                            <col></col>
                        </colgroup>
                        <thead>
                            <tr>
                                <th>video sequence</th>
                                <th>thread /window, ms</th>
                                <th>warp /window, ms</th>
                                <th>warp /4 windows, ms</th>
                                <th>speedup w/w, X</th>
                                <th>speedup w/4w, X</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <th>seq06</th><td>169.13</td><td>98.29</td><td>27.13</td><td>1.72</td><td>6.23</td>
                            </tr>
                            <tr>
                                <th>seq07</th><td>166.92</td><td>100.12</td><td>36.52</td><td>1.66</td><td>4.57</td>
                            </tr>
                            <tr>
                                <th>seq08</th><td>172.89</td><td>98.12</td><td>38.87</td><td>1.76</td><td>4.44</td>
                            </tr>
                            <tr>
                                <th>seq09</th><td>175.82</td><td>102.54</td><td>34.18</td><td>1.76</td><td>4.45</td>
                            </tr>
                            <tr>
                                <th>seq10</th><td>144.13</td><td>96.87</td><td>32.40</td><td>1.71</td><td>5.14</td>
                            </tr>
                        </tbody>
                    </table>

                </section>
              </section>

        <section>
          <h2>Final words</h2>
          <ul>
            <li>Use warp-wise approaches for memory bound kernels</li>
            <li>Minimize transactions with global memory</li>
            <li>Load only bytes needed</li>
            <li>Use hierarchical approaches for non parallelizeble codes</li>
            <li>Consider data dependency while optimizing complicated algorithms</li>
          </ul>
        </section>

        <section id="end1">
          <h1>THE END</h1>
          <h6><a href="code_gpu_with_cuda_6.html">next</a></h6>
          <br>
          <br>
          <br>
          <h3>BY <a href="https://github.com/cuda-geek">cuda.geek</a> / 2013&ndash;2015</h3>
        </section>

      </div>
    </div>
    <script src="3dparty/reveal/lib/js/head.min.js"></script>
    <script src="3dparty/reveal/js/reveal.min.js"></script>
    <script>
            Reveal.initialize({
                controls: true,
                progress: true,
                history: true,
                center: false,
                rollingLinks: false,

                theme: Reveal.getQueryHash().theme,
                transition: Reveal.getQueryHash().transition || 'concave', // default/cube/page/concave/zoom/linear/fade/none
                dependencies: [
                    { src: '3dparty/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: '3dparty/reveal/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: '3dparty/reveal/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: '3dparty/reveal/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: '3dparty/reveal/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: '3dparty/reveal/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
                    // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
                    // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
                ]
            });
    </script>
  </body>
</html>
