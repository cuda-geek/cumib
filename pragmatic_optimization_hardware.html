<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Pragmatic optimization in modern programming</title>
    <meta name="description" content="Pragmatic optimization in modern programming - Introduction">
    <meta name="author" content="Marina Kolpakova (cuda.geek)">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="plugin/reveal/css/reveal.css">
    <link rel="stylesheet" href="plugin/reveal/css/theme/geek.css" id="theme">
    <link rel="stylesheet" href="css/colors-orange.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="plugin/reveal/lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'plugin/reveal/css/print/pdf.css' : 'plugin/reveal/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="plugin/reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Pragmatic optimization</h1>
          <h2>in modern programming</h2>
          <h3>Modern computer architecture concepts</h3>
          <!-- Optimizations in hardware -->
          <!-- Hardware optimization capabilities -->
          <br>
          <br>
          <br>
          <small>Created by
            <a href="http://github.com/cuda-geek">Marina Kolpakova</a>
            for
            <a href="http://www.unn.ru/eng/">UNN</a>
          </small>
        </section>

        <section>
          <h2>Themes &amp; Contents</h2>
          <ul>
            <li><strong>Pragmatics</strong>
              <ul>
                <li>Ordering optimization approaches</li>
                <li>Demystifying a compiler</li>
                <li>Mastering compiler optimizations</li>
              </ul>
            </li>
            <li><strong>Computer Architectures</strong>
              <ul>
                <li><b>Modern computer architectures</b></li>
                <li>SIMD extensions</li>
                <li>Specific co-processors</li>
              </ul>
            </li>
          </ul>
        </section>

        <section>
          <h2>Outline</h2>
          <ul>
            <li>Modern architecture taxonomy
              <ul>
                <li>RISC</li>
                <li>CISC</li>
                <li>VLIW</li>
                <li>Vector</li>
              </ul>
            </li>
            <li>Hardware optimizations
              <ul style="width:100%; columns: 2; -webkit-columns: 2; -moz-columns: 2;">
                <li>Cache hierarchies</li>
                <li>Hardware prefetching</li>
                <li>Write stream detection</li>

                <li>Pipelined execution</li>
                <li>Superscalar execution</li>
                <li>Speculative execution</li>
                <li>Out-of-order execution</li>
              </ul>
            </li>
            <li>summary</li>
          </ul>
        </section>

        <section>
          <h2>Defining Computer Architecture</h2>
          <dl style="text-align: justify;">
            <dt><b>Instruction Set Architecture (ISA)</b></dt>
            <dd>is a contract between the Hardware and Software, which specifies right, possibilities and limitations.<br/>
              <ul>
                <li>Class of ISA (load-store, register-memory)</li>
                <li>Memory addressing (alignment rules)</li>
                <li>Addressing modes (base-immediate, base-register)</li>
                <li>Types and sizes of operands (size of byte, short)</li>
                <li>Operations (general arithmetic, control, logical)</li>
                <li>Control flow instructions (branches, jumps, calls, and returns)</li>
                <li>Encoding an ISA (fixed or variable length)</li>
              </ul>
            </dd>

            <dt><b>Microarchitecture (organization)</b></dt>
            <dd>is a concrete implementation of the ISA, the high-level aspects of a processors design (memory system,
            memory interconnect, design of the internal processor).</dd>

            <dt><b>Hardware (design)</b></dt>
            <dd>is the specifics of a computer, including the logic design and packaging, a concrete implementation of
            the microarchitecture.</dd>
          </dl>
        </section>

        <section>
          <section>
            <h2>Key architecture families</h2>
            <ul>
              <li><strong>RISC</strong> - Reduced Instruction Set Computer</li><br/>
              <li><strong>CISC</strong> - Complex Instruction Set Computer</li><br/>
              <li><strong>VLIW</strong> - VEry Long Instruction Word</li><br/>
              <li><strong>Vector</strong> architectures</li>
            </ul>
          </section>

          <section>
            <h2>CISC</h2>
            <b>Complex Instruction Set Computer</b>
            <dl style="text-align: justify;">
              <dt><strong>Designed in the 1970s</strong></dt>
              <dd>which was a time there transistors were expensive and compilers were weak.</dd>
              <dt><strong>The goal</strong></dt>
              <dd>is to define an instruction set that allows high level language constructs be translated into as few
              assembly language instructions as possible.</dd>
              <dt><strong>Features</strong></dt>
              <dd>As the result <i>many instructions access memory</i>, <i>plenty of addressing modes</i>,
              many instruction families, <i>very reach ISA</i>, and consequently, <i>complicated instruction decoding</i> logic.
              Only a <i>few registers are available</i> for programmers.</dd>
              <dt><strong>Examples</strong></dt>
              <dd>VAX, x86, AMD64.</dd>
              <dt><strong>Nowadays</strong></dt>
              <dd>CISC instructions are typically broken down into lower level instructions called microcode which are
              much easy to pipeline, large reorder buffers to eliminate stalls.</dd>
            </dl>
          </section>

          <section>
            <h2>RISC</h2>
            <b>Reduced Instruction Set Computer</b>
            <dl style="text-align: justify;">
              <dt><strong>Designed in the 1980s</strong></dt>
              <dd>which was a time there IPL was the most concern.</dd>
              <dt><strong>The goal</strong></dt>
              <dd>decrease the number of clocks per instruction (CPI), pipeline instructions as much as possible.</dd>
              <dt><strong>Features</strong></dt>
              <dd>Relatively few instructions all the same length. Only load and store instructions access memory.
              Has more registers than CISC processors have. <strike>No microcode</strike></dd>
              <dt><strong>Examples</strong></dt>
              <dd>MIPS, ARM, PowerPC.</dd>
              <dt><strong>Nowadays</strong></dt>
              <dd>Most architectures that comes from RISC are called Load-Store architectures. They use most modern
              hardware enhancements with deeper pipelines, multi-cycle instructions, OoO execution. Modern RISC may have
              micro ops. Whereas they still employ special instructions for memory accesses.</dd>
            </dl>
          </section>

          <section>
            <h2>VLIW</h2>
            Very Long Instruction Word
            <dl style="text-align: justify;">
              <dt><strong>Designed in the 1980s</strong></dt>
              <dd>which was a time there IPL was the most concern.</dd>
              <dt><strong>The goal</strong></dt>
              <dd>is maximize instruction level parallelism (ILP) with pipelining done by software.</dd>
              <dt><strong>Features</strong></dt>
              <dd>Software determines which instructions can be performed in parallel, bundles this information
              and the instructions, and passes the bundle to the hardware.</dd>
              <dt><strong>Examples</strong></dt>
              <dd>IntelÂ­HP Itanium.</dd>
              <dt><strong>Nowadays</strong></dt>
              <dd>Aren't used in generic processors design, but widely for programmable co-processors where shrink in
              the power consumption is crustal.</dd>
            </dl>
          </section>

          <section>
            <h2>Vector Processors</h2>
            <dl style="text-align: justify;">
              <dt><strong>First introduced in 1976.</strong></dt>
              <dd>dominated for HPC in the 1980s because of high instruction throughput.</dd>
              <dt><strong>The goal</strong></dt>
              <dd>perform operations on vectors of data to increase instruction throughput. Vector pipelining which is
              also called chaining.</dd>
              <dt><strong>Examples</strong></dt>
              <dd>Cray</dd>
              <dt><strong>Nowadays</strong></dt>
              <dd>Aren't used in generic processors design, but used as a co-processors for a specific workloads.</dd>
            </dl>
          </section>

          <section>
            <h2>Why RISC?</h2>
            <ol>
              <li>Keep the number of instructions small.</li>
              <li>Keep the functionality of the instructions orthogonal.</li>
              <li>Keep the instructions isolated to one piece of hardware on chip.</li>
              <li>Decoding logic is power efficient</li>
              <li>Easy for compiler optimizations</li>
              <li>Design, place and route more elegantly</li>
              <li>Drive a higher clock rate</li>
              <li>Have a deeper pipeline</li>
              <li>Expose opportunities for instruction parallelism to the compiler</li>
            </ol>
          </section>
        </section>

        <section>
          <h2>Hardware optimizations</h2>
          <ul>
            <li>Cache hierarchies</li><br/>

            <li>Pipelined execution</li>
            <li>Superscalar execution</li>
            <li>Speculative execution</li>
            <li>Out-of-order execution</li>
            <li>Forwarding</li><br>

            <li>Hardware prefetching</li>
            <li>Write stream detection</li>
          </ul>
        </section>

        <section>
          <section>
            <h2>The memory-processor gap</h2>
            <div><img src="images/popt/hp-mem.jpg"></div>
            <small>image source Hennessey &amp; Paterson</small>
          </section>

          <section>
            <h2>Cache hierarchies</h2>
            <p><strong>The goal for caches is to mitigate a performance gap between<br/>latencies of memory access
            and processor operations</strong></p>
            <dl style="text-align: justify;">
              <dt><b>Temporal locality</b></dt>
              <dd>relies on the fact that <strong>recently accessed items</strong> are <strong>likely</strong>
              to be accessed in the <strong>near future</strong>. If address <strong><code>M(n)</code></strong>
              is accessed at time <strong><code>t</code></strong>, it is likely that <strong><code>M(n)</code></strong>
              will be used again at time <strong><code>t+</code>&epsilon;</strong> , where <strong>&epsilon;</strong>
              is small.</dd><br/>

              <dt><b>Spatial locality</b></dt>
              <dd>relies on the fact that items whose addresses are <strong>near each other likely to accessed close
              together in time</strong>. If we access <strong><code>M(n)</code></strong> it is likely that
              <strong><code>M(n + &epsilon;)</code></strong> will be used, where <strong><code>&epsilon;</code></strong>
              is small.</dd>
            </dl>
          </section>
        </section>

        <section>
          <section>
            <h2>Cache terminology</h2>
            <dl style="text-align: justify;">
              <dt><b>Access time</b></dt>
              <dd>is the time from when a read or write is requested until it arrives at its destination.</dd><br/>
              <dt><b>Cycle time</b></dt>
              <dd>is the minimum time between requests to memory.</dd><br/>
              <dt><b>Cache hit</b></dt>
              <dd>is the situation when the requested data is in the cache.</dd><br/>
              <dt><b>Cache miss</b></dt>
              <dd>is the situation when the requested data isn't in the cache. Upon a cache miss, a cache line is retrieved
               from main memory (or a higher level of cache) and placed in the cache.</dd>
            </dl>
          </section>

          <section>
            <h2>Cache terminology</h2>
            <dl style="text-align: justify;">
              <dt><b>Cache miss rate</b></dt>
              <dd>is the fraction of cache accesses that result in a miss.</dd>
              <dt><b>Local miss rate</b></dt>
              <dd>is the number of misses in a cache divided by the total number of memory access to this cache</dd>
              <dt><b>Global miss rate</b></dt>
              <dd>is the number of misses in a cache divided by the total number of memory accesses generated</dd>
              <br/>
              <dt><b>Cache miss penalty</b></dt>
              <dd>is the time required for serving a cache miss<br/>(depends on the latency and bandwidth of the memory).</dd><br/>
              <dt><b>Memory stall cycles</b></dt>
              <dd>is the cycles during which a CPU is stalled waiting for a memory access.</dd>
            </dl>
          </section>
        </section>

<!--         Using multiple levels of cache allows a small fast cache to keep pace with the
CPU, while slower larger caches can be used to reduce the miss penalty since
the next level cache can be used to capture many accesses that would go to
main memory.
The local miss rate is large for higher level caches because the first level
cache benefits the most from data locality.  Thus a global miss rate that
indicates what fraction of the memory access that leave the CPU go all the way
to memory is a more useful measure. -->

        <section>
          <h2>Cache properties </h2>
          <ul>
            <li>Modern processors have from 1 to 4 levels of caches
            <ul>
              <li>Inclusive caches duplicates the data from higher level cache in lower levels</li>
              <li>Exclusive caches contains data only in one level of cache</li>
              <li>Most caches are <b>inclusive</b>.</li>
            </ul>
            </li>
            <li>Data transfer unit between registers and L1 cache is usually in between 1-16 bytes</li>
            <li><b>Cache line</b> is the smallest unit of memory that can be transferred to and form main memory
            (typically 32, 64 bytes, sometimes 128 bytes and more)</li>
            <li>Line is also unit of storage allocation in cache</li>
          </ul>
        </section>

        <section>
          <h2>Associativity</h2>
          <ul>
            <li><b>Cache associativity</b> is the number of unique places in a cache where any given memory item can reside.</li>
            <li>In an n way associative cache, any cache line from memory can map to any of the n locations in a set.</li>
            <li>1 way set associative cache is called direct mapped.</li>
            <li>A fully associative cache is one in which a cache line can be placed anywhere in cache.</li>
          </ul>
          <p>Direct mapped means only one location. But very, very fast.
          Higher associativity is better, but costly in terms of gates and complexity (power and performance).</p>
        </section>

        <section>
          <h2>Cache misses taxonomy</h2>
          <dl style="text-align: justify;">
            <dt><b>Compulsory</b></dt>
            <dd>refers to misses for the very first access to a cache line.</dd><br/>
            <dt><b>Capacity</b></dt>
            <dd>is the situation than the cache cannot contain all the cache lines needed during execution of a program.</dd><br/>
            <dt><b>Conflict</b></dt>
            <dd>occurs when a block must be discarded and later retrieved because too many cache lines mapped to the
            same set. It is only applicable to a (less than fully) set associative or direct mapped cache.</dd><br/>
            <dt><b>Coherency</b></dt>
            <dd></dd>
          </dl>
        </section>

        <section>
          <h2>Replacement policy &amp; Write strategy</h2>
          <p>Replacement policy</p>
          <ul>
            <li>Random</li>
            <li>Round robin</li>
            <li>Least Recently Used (LRU): performs better than random or round robin but is more difficult to implement</li>
            <li>Pseudo-LRU</li>
          </ul>
          <p>Write Strategy is strategy to maintain coherence between cache and memory</p>
          <ul>
            <li>Write through: The data are written to both the cache line in the cache and to main memory.</li>
            <li>Write back: The data are written only to the cache line in the cache.  The modified cache line
            is written to memory only when necessary (e.g., when it is replaced by another cache line.</li>
          </ul>
        </section>

        <section>
          <h2>Nonblocking Caches</h2>
          Pipelined computers that allow out-of-order execution can continue fetching instructions from the instruction
          cache while waiting on a data cache miss. A nonblocking cache design allows the data cache to continue to
          supply cache hits during a miss, called <b>hit under miss</b>, or <b>hit under multiple miss</b> if multiple
          misses can be overlapped.  Hit under miss significantly increases the complexity of the cache controller,
          with the complexity increasing as the number of outstanding misses allowed increases. Out-of-order processors
          with hit under miss are generally capable of hiding the  miss penalty of an L1 data cache miss that hits
          in the L2 cache, but are not capable of hiding a significant portion of the L2 miss penalty.
        </section>

        <section>
          <h2>Coherency</h2>
          Cache Contention on SMPs
          âº When two or more CPUs alternately and repeatedly update the same cache line
          âº memory contention
          â¢  when two or more CPUs update the same variable
          â¢ correcting it involves an algorithm change
          âº false sharing
          â¢ when CPUs update distinct variables that occupy the same cache line
          â¢ correcting it involves modification of data structure layout
        </section>

<!--         <section>
At least 3 major approaches:
Make cores as fast/slow as main memory (SiCortex, Tensilica)
Add faster/closer memory pipes (Opteron, Nehalem)
Streaming compute engines (NVIDIA,AMD), vectorized memory pipelines (Convey).

With multiple cores, more than one level can keep MESI states.

Hardware Prefetching
When you see linear scaling graphs, be (somewhat) suspicious.
Linear scalability is easy(ier) when per-core performance is low!
The faster a single core computes, the more vulnerable it is to other bottlenecks.
So producing a linear graph, does not make your program efficient.

Cache Blocking for Multicore
Generally, block for the largest non-shared cache.
L2 on Nehalem.
Depending on the speed difference and amount of work per iteration, L1 may be better.
Never block for the shared cache size.
        </section> -->

<!--         <section>

When a memory location is accessed, the whole cache line
containing the address is copied from memory to the cache
a cache replacement policy defines how old data in the
cache is replaced with new data
tries to keep frequently used data in the cache â
Least Recently Used algorithm

A cache mapping defines where memory locations will be
placed in cache
in which cache line a memory addresses will be placed
we can think of the memory as being divided into blocks of
the size of a cache line
the cache mapping is a simple hash function from
addresses to cache sets
Cache is much smaller than main memory
more than one of the memory blocks can be mapped to the
same cache line
Each cache line is identified by a tag
determines which memory addresses the cache line holds
based on the tag and the valid bit, we can find out if a
particular address is in the cache (hit) or not (miss)

DIRECT MAPPED CACHES
A memory location can be placed in exactly one cache line
Easy to find out if a memory address is in cache or not
there is only one possible location in the cache where a
memory address can be
need only to check the tag and the valid bit in that line
if the tag in the cache matches the address, we have a hit
if the tag does not match the address, we have a miss

FULLY ASSOCIATIVE CACHE
A fully associative cache consists of a single set
a memory block can be placed in any cache line
There is no information about where a memory block will be
placed in the cache
have to search through the whole cache to find the
location containing the tag we are looking for
Implemented by associative memory
can search through all cache lines simultaneously for a
matching tag
Fully associative caches are small and expensive
mainly used for translation lookaside buffers (TLBâs) and
similar

SET ASSOCIATIVE CACHE
A set contains more than one line
A memory block can be placed in any of the lines in the set
Similar to a number of small fully associative caches
Which set a memory block is located in is computed from the
address
2-, 4-, 8- and 16-way set associative caches are often used in
modern processors
        </section> -->

        <section>
          <h2>Cache optimizations</h2>
          <ol>
            <li>Larger block size to reduce miss rate</li>
            <li>Bigger caches to reduce miss rate</li>
            <li>Higher associativity to reduce miss rate</li>
            <li>Multilevel caches to reduce miss penalty</li>
            <li>Giving priority to read misses over writes to reduce miss penalty</li>
            <li>Avoiding address translation during indexing of the cache to reduce hit time</li>
          </ol>
        </section>

        <!--  -->

        <section>
          <section>
            <h2>Pipelined execution - Classic 5-stage pipeline</h2>
            <p>The idea is to break down an instruction into multiple stages so that instructions
            on different stages can be processed at the same clock cycle. Introduced in 1960s.</p>
            <dl style="text-align: justify;">
              <dt><b>Instruction fetch cycle (IF)</b></dt>
              <dd>An instructions is fetched from IC and may be placed either into an instruction register
              or a queue of pending instructions.</dd>
              <dt><b>Instruction decode/register fetch cycle (ID)</b></dt>
              <dd>An instruction is decoded, register operands are read from the register file.</dd>
              <dt><b>Execution/effective address cycle (EX)</b></dt>
              <dd>Either an ALU operation is performed or an effective address for memory access is computed,
              which depends on an instruction type</dd>
              <dt><b>Memory access/branch completion cycle (MEM)</b></dt>
              <dd>The memory is accessed, if appropriate.</dd>
              <dt><b>WriteÂ­back cycle (WB)</b></dt>
              <dd>The result is written into the register file, whether it comes from the memory system (for a load)
              or from the ALU (for an ALU instruction)</li>
            </dl>
          </section>

          <section>
            <h2>Without pipelined execution</h2>
            <img class="simple" width="80%" src="images/popt/nopipeline.svg">
          </section>

          <section>
            <h2>Pipelined execution</h2>
            <img class="simple" width="80%" src="images/popt/pipeline.svg">
          </section>

          <section>
            <h2>Pipeline hazards</h2>
            <p><b>Pipeline hazard</b> is a situation that prevents the next instruction in the pipeline from executing
            during its designated clock cycle and thus cause pipeline stalls.</p>
            <dl style="text-align: justify;">
              <dt><b>Structural hazard</b></dt>
              <dd>arise from resource conflicts when the hardware cannot support all possible combinations of instructions
              simultaneously in overlapped execution.</dd>
              <dt><b>Data hazard</b></dt>
              <dd>arise when an instruction depends on the results of a previous instruction in a way that is exposed
              by the overlapping of instructions in the pipeline.</dd>
              <dt><b>Control hazard</b></dt>
              <dd>arise from the pipelining of branches and other instructions that change the PC.</dd>
            </dl>
            <br/><small>All the terms used are taken from Hennessey & Paterson</small>
          </section>

          <section>
            <h2>multiple-issue superscalars: Cortex-A53</h2>
            Instructions are dispatched 2 per cycle to the appropriate issue queue.<br/>
            Issue Width &mdash; 2 &mu;ops, Pipeline Length &mdash; 8 stages, Pipeline Length &mdash; 8 stages,
            Number of pipes &mdash; 5: I0, I1, FP, B, LD/DT.
            <div><img width="50%" src="images/popt/A53-Diagram.png"></div>
            <small>image source is <a href="http://images.anandtech.com/doci/8718/A53-Diagram.png">AnandTech.com</a></small>
          </section>
        </section>


        <section>
          <h2>Speculative execution</h2>
        </section>

        <section>
          <h2>Out-of-order execution</h2>
        </section>

        <section>
          <h2>Prefetch streams and write stream</h2>
        </section>


        <section>
          <h2>SIMD vs Vector co-processors</h2>
        </section>

        <section>
          <h2>Summary</h2>
        </section>

        <section id="end1">
          <h1>THE END</h1>
          <img class="simple" src="images/popt/infinity.png">
          <h4><a href="https://github.com/cuda-geek">Marina Kolpakova</a> / 2015</h4s>
        </section>

      </div>

    </div>

    <script src="plugin/reveal/lib/js/head.min.js"></script>
    <script src="plugin/reveal/js/reveal.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        slideNumber: true,
        history: true,
        center: false,

        width: 960,
        height: 720,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'plugin/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>
