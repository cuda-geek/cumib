<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Pragmatic optimization in modern programming</title>
    <meta name="description" content="Pragmatic optimization in modern programming - Modern computer architecture concepts">
    <meta name="author" content="Marina Kolpakova (cuda.geek)">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="plugin/reveal/css/reveal.css">
    <link rel="stylesheet" href="plugin/reveal/css/theme/geek.css" id="theme">
    <link rel="stylesheet" href="css/colors-orange.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="plugin/reveal/lib/css/zenburn.css">

    <style type="text/css">
    </style>

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'plugin/reveal/css/print/pdf.css' : 'plugin/reveal/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="plugin/reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Pragmatic optimization</h1>
          <h2>in modern programming</h2>
          <h3>Modern computer architecture concepts</h3>
          <br>
          <br>
          <br>
          <small>Created by
            <a href="http://github.com/cuda-geek">Marina Kolpakova</a>
            for
            <a href="http://www.unn.ru/eng/">UNN</a>
          </small>
        </section>

        <section>
          <h2>Themes &amp; Contents</h2>
          <ul>
            <li><strong>Pragmatics</strong>
              <ul>
                <li>Ordering optimization approaches</li>
                <li>Demystifying a compiler</li>
                <li>Mastering compiler optimizations</li>
              </ul>
            </li>
            <li><strong>Computer Architectures</strong>
              <ul>
                <li><b>Modern computer architecture concepts</b></li>
                <li>SIMD extensions</li>
                <li>Specific co-processors</li>
              </ul>
            </li>
          </ul>
        </section>

        <section>
          <h2>Outline</h2>
          <ul>
            <li>Aspects of computer architecture</li>
            <li>Modern architecture families
              <ul>
                <li><strong>RISC</strong>, CISC, VLIW, Vector</li>
              </ul>
            </li>
            <li>Cache optimizations</li>
            <li>Pipelining</li>
            <li>summary</li>
          </ul>
        </section>

        <section>
          <section>
            <h2>Three Aspects of Computer Architecture</h2>
            <dl style="text-align: justify;">
              <dt><b>Instruction Set Architecture or ISA (interface)</b></dt>
              <dd>is a <strong>contract</strong> between HW and SW, which specifies <strong>right</strong>,
              <strong>possibilities</strong> &amp; <strong>limitations</strong>.<br/>
                <ul>
                  <li>Class of ISA (e.g. load-store, register-memory)</li>
                  <li>Memory addressing modes &amp; rules (e.g. base-immediate, alignment requirements)</li>
                  <li>Types &amp; sizes of operands (e.g. size of byte, short)</li>
                  <li>Operations (e.g. general arithmetic, control, logical)</li>
                  <li>Control flow instructions (e.g. branches, jumps, calls, and returns)</li>
                  <li>Encoding an ISA (e.g. fixed or variable length)</li>
                </ul>
              </dd><br/>

              <dt><b>Microarchitecture (organization)</b></dt>
              <dd>is a concrete implementation of the ISA, the high-level aspects of a processor design (memory system,
              memory interconnect, design of the processor internals).</dd><br/>

              <dt><b>Hardware or chip (design)</b></dt>
              <dd>is the specifics of a computer, including the logic design and packaging. This is a concrete
              implementation of the microarchitecture.</dd>
            </dl>
          </section>

          <section>
            <h2>Architecture: 64-bit ARM</h2>
              <table>
                <colgroup>
                  <col></col>
                  <col></col>
                  <col></col>
                  <col></col>
                </colgroup>
                <thead>
                  <tr>
                    <th>ISA</th>
                    <th>&mu;arch</th>
                    <th>IP</th>
                    <th>Hardware</th>
                  </tr>
                </thead>
                <tbody style="">
                  <tr>
                    <td class="common" rowspan="11">
                      <div style="min-width:200%;white-space: nowrap;padding-bottom:1.5em;" class='rotate'>Armv8-a</div>
                    </td>
                    <td>Cortex-A53</td>
                    <td>ARM</td>
                    <td>Octa Exynos 7(7580) 1.6GHz 28nm <abbr title="High-k Metal Gate">HKMG</abbr></td>
                  </tr>
                  <tr>
                    <td width="17%">Cortex-A57</td>
                    <td>ARM</td>
                    <td>Octa Exynos 7(7420) <small style="vertical-align:bottom; ">big.LITTLE</small>
                    2.1/1.5 14<abbr title="Fin-Shaped Field Effect Transistor">FF</abbr> (
                    <abbr title="Low power Early">LPE</abbr>) (Samsung)</td>
                  </tr>
                  <tr>
                    <td>Cortex-A72</td>
                    <td>ARM</td>
                    <td>Deca MediaTek Helio X20 <small style="vertical-align:bottom; ">big.LITTLE</small> 2.5/2.0/1.4
                    20<abbr title="High-k Metal Gate">HKMG</abbr> (TSMC)  </td>
                  </tr>
                  <tr>
                    <td>Cortex-A35</td>
                    <td>ARM</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>Denver</td>
                    <td>NVIDIA</td>
                    <td>Dual Tegra K1 2.3GHz 28nm <abbr title="High Performance Mobile">HPM</abbr></td>
                  </tr>
                  <tr>
                    <td>Cyclone</td>
                    <td>Apple</td>
                    <td>Dual A7 (APL0698) 1.4GHz 28nm <abbr title="High-k Metal Gate">HKMG</abbr> (Samsung)</td>
                  </tr>
                  <tr>
                    <td>Typhoon</td>
                    <td>Apple</td>
                    <td>Dual A8 (APL1012) 1.3GHz 20<abbr title="High-k Metal Gate">HKMG</abbr> (TSMC)</td>
                  </tr>
                  <tr>
                    <td rowspan="2">Twister</td>
                    <td rowspan="2">Apple</td>
                    <td>Dual A9 (APL0898) 1.85GHz 16<abbr title="Fin-Shaped Field Effect Transistor">FF+</abbr> (TSMC)</td>
                  </tr>
                  <tr>
                    <td>Dual A9 (APL1022) 1.85GHz 14<abbr title="Fin-Shaped Field Effect Transistor">FF</abbr>(
                    <abbr title="Low power Plus">LPP)</abbr> (Samsung)</td>
                  </tr>
                  <tr>
                    <td>Kryo</td>
                    <td>Qualcomm</td>
                    <td>Tetra S820 <small style="vertical-align:bottom; ">big.LITTLE</small>
                    2.2/1.6GHz 14<abbr title="Fin-Shaped Field Effect Transistor">FF (LPP)</abbr> (Samsung)</td>
                  </tr>
                  <tr>
                    <td>Exynos M1</td>
                    <td>Samsung</td>
                    <td>Exynos 8890 (no details yet) </td>
                  </tr>
                  <tr>
                    <td></td>
                    <td></td>
                    <td></td>
                    <td></td>
                  </tr>
                </tbody>
              </table>
          </section>
        </section>

        <section>
          <section>
            <h2>Latency <small>vs</small> Throughput architectures</h2>
            <dl>
              <dt><b>Latency</b> oriented architecture</dt>
              <dd>
                <ul>
                  <li><b>addresses latency hiding issues;</b></li>
                  <li>features sophisticated pipelining, out-of-order;</li>
                  <li>employs advanced cache hierarchies;</li>
                  <li>widely uses speculation.</li>
                  <li><b>Compute cores occupy only a small part of a die.</b></li>
                </ul>
              </dd>
              <br>
              <dt><b>Throughput</b> oriented architecture</dt>
              <dd>
                <ul>
                  <li><b>performs a bunch of operations in fly;</b></li>
                  <li>features many simple compute units/cores;</li>
                  <li>employs simple pipelines to provide a low-cost thread scheduling;</li>
                  <li>uses wide basses, tiling, programmable local memory.</li>
                  <li><b>Compute cores occupy most part of a die.</b></li>
                </ul>
              </dd>
            </dl>
          </section>
          <section>
            <h2>What is on die?</h2>
            <img width="50%" class="simple" src="images/popt/pLDBDAIaeFFrBEdN.large.jpeg">
            <br>
            <small>Image source <a href="http://www.chipworks.com">ChipWorks</a></small>
          </section>
        </section>

        <section>
          <h1>Key architecture families</h1>
          <ul>
            <li><strong>RISC</strong> - Reduced Instruction Set Computer</li>
            <li><strong>CISC</strong> - Complex Instruction Set Computer</li>
            <li><strong>VLIW</strong> - Very Long Instruction Word</li>
            <li><strong>Vector</strong> architecture</li>
          </ul>
        </section>

        <section>
          <section>
            <h2>CISC</h2>
            <p><b>Complex Instruction Set Computer</b></p>
            <ul>
              <li><b>Designed in the 1970s</b> which was a time where transistors were expensive while compilers
              were naive. Additionally instruction packaging was the main concern due to shortage of memory.</li>
              <br>
              <li><b>The goal</b> was to define an instruction set that allows high level language constructs
              be translated into <strong>as few assembly language instructions as possible</strong>,
              improving performance as well as code density.</li>
              <br>
              <li><b>Examples</b> are VAX, x86, <b>AMD64</b>.</li>
              <br>
              <li><b>Latency-oriented architecture</b></li>
            </ul>
          </section>

          <section>
            <h2>CISC</h2>
            <ul>
              <li><b>Features</b>
                <ul>
                  <li><i>many instructions access memory</i>,</li>
                  <li><i>a plenty of addressing modes</i>,</li>
                  <li>many instruction families and a <i>very rich ISA</i>,</li>
                  <li>consequently, <i><strong>complicated instruction decoding</strong></i> logic.</li>
                  <li>Moreover, only a <i>few registers are available</i> for programmers.</li>
                </ul>
              </li>
              <br/>
              <li><b>Nowadays</b>
                <ol>
                  <li>CISC instructions are typically broken down into the lower level instructions (&mu;code) which
                  are much easy to pipeline and process in a power efficient fashion.</li>
                  <li>Transistors are spent to cache hierarchies, out-of-order execution, large reorder buffers and
                   speculation to eliminate pipeline stalls.</li>
                   <li>Symmetric multi-processing.</li>
                </ol>
              </li>
            </ul>
          </section>
        </section>

        <section>
          <section>
            <h2>RISC</h2>
            <p><b>Reduced Instruction Set Computer</b></p>
            <ul>
              <li><b>Designed in the 1980s</b> which was a time there <abbr >IPL</abbr> was the most concern.</li>
              <br/>

              <li><b>The goal</b> was to decrease the number of clocks per instruction (CPI) while pipeline instructions
              as much as possible employing hardware to help with it. Uniform ISA, pipelining and large register file
              is a must-have.</li>
              <br/>

              <li><b>Examples</b> are <b>MIPS</b>, <b>ARM</b>, <b>PowerPC</b>.</li>
              <br>

              <li><b>Latency-oriented architecture</b></li>
            </ul>
          </section>

          <section>
            <h2>RISC</h2>
            <ul>
              <li><b>Features</b>
                <ul>
                  <li>Relatively few instructions, all are the same length.</li>
                  <li>Only load and store instructions access memory.</li>
                  <li>Large resister file than typical CISC processors have.</li>
                  <li><strike>No &mu;code</strike> (it is not true for some modern &mu;architectures)</li>
                </ul>
              </li>
              <br>

              <li><b>Nowadays</b>
                 most architectures that comes from RISC are called <b>Load-Store architectures</b>. They combine
                 concepts of a classic RISC with usage of modern hardware enhancements:<br/>
                   <ol>
                    <li>deep pipelines, multi-cycle instructions,</li>
                    <li>out-of-order execution,</li>
                    <li>speculation.</li>
                  </ol>
                </li>
                The latest RISCs may employ micro-ops. Whereas, they still use special instructions to access memory.
              </ul>
          </section>
        </section>

        <section>
          <section>
            <h2>VLIW</h2>
            <p><b>Very Long Instruction Word</b></p>

            <ul>
              <li><b>Designed in the 1980s</b> which was a time there IPL was the most concern.</li>
              <br>

              <li><b>The goal</b> was to pipeline instructions as much as possible employing software to help with it
              reducing complexity of the hardware.
                <ul>
                  <li>Compiler
                    <ul>
                      <li>analyzes control flow, analyzes dependency</li>
                      <li>schedules instructions</li>
                      <li>maps variables to limited register set</li>
                    </ul>
                  </li>
                  <li>Hardware
                    <ul>
                      <li>analyzes control flow, analyzes dependency</li>
                      <li>schedules instructions</li>
                      <li>remaps ISA register to large internal register set</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <br>

              <li><b>Example</b> is Intel­HP Itanium.</li>
              <br>

              <li><b>Latency-oriented architecture</b></li>
            </ul>
          </section>

          <section>
            <h2>VLIW</h2>
            <ul>
              <li><b>Features</b>
                <ul>
                  <li>Software determines which instructions can be performed in parallel,</li>
                  <li>bundles this information and the instructions,</li>
                  <li>and passes the <b>bundle</b> to the hardware.</li>
                  <li>No data dependencies between instructions in a word (instruction bundle).</li>
                  <li>Each operation in a word assigned to specific issue slot (dedicated FU).</li>
                </ul>
              </li>
              <br>

              <li><b>Nowadays</b> hardly any generic processor implements VLIW due to:<br>
                <ul>
                  <li>brunchy nature of production codes (in contrast to HPC or scientific codes),</li>
                  <li>need to follow binary compatibility across the &mu;architecture families.</li>
                </ul>
              Whereas architecture is widely adopted for programmable co-processors where shrink in power consumption
              without lose of performance is crucial (DSP, GPU).<br>
              The newest trend is providing RISC-based ISA, but construct VLIW instructions on fry during
              instruction fetch, which was already used for Denver &mu;architecture.</li>
            </ul>
          </section>
        </section>

        <section>
          <section>
            <h2>Vector Processors</h2>
            <ul>
              <li><b>First introduced in 1976</b> and dominated for HPC in the 1980s because of high instruction
              throughput.</li>
              <br>
              <li><b>The goal</b> was to perform operations on vectors of data exposing DLP to increase instruction
              throughput. Vector pipelining is also called chaining.</li>
              <br>
              <li><b>Example</b> is Cray</li>
              <br>
              <li><b>Throughput oriented architectures.</b></li>
            </ul>
          </section>
          <section>
            <h2>Vector Processors</h2>
            <ul>
              <li><b>Features</b>
                <ul>
                  <li>Process the data in vectors, each element in a vector is independent on any other.</li>
                  <li>Deep pipelines, wide execution units, not necessary the same width as size if vector in elements</li>
                  <li>Most efficient for simple memory patterns, but getter/scatter is usually possible too</li>
                  <li>fide memory interfaces to saturate execution units</li>
                  <li>Large vector register file, cache is not a strict requirement and absent for classical vector
                  processors.</li>
                </ul>
              </li>
              <br>

              <li><b>Nowadays</b> they aren't used in generic processors design, but used as a co-processors for
              a specific workloads: HPC, multimedia, precursors of most designs of modern GPUs. Vector pipes with short
              vector length (8-16 bytes) called SIMD units are widely integrated in modern general purpose processors to
              accelerate most demanding loops.</li>
            </ul>
          </section>
        </section>

        <section>
          <section>
            <h2>Why everything is doing to be <strike>RISC</strike> Load/Store?</h2>
            <ol>
              <li><b>Simple instructions of the same width &amp; few addressing modes</b>
                <ul>
                  <li>Instructions can be loaded by cache line granularity, branch addresses are aligned.</li>
                  <li>Simple hardware logic &#8594; power efficient chips.</li>
                  <li>Drive a higher clock rate.</li>
                </ul>
              </li><br/>
              <li><b>Concise set of the instructions with orthogonal functionality</b>
                <ul>
                  <li>Since complex instructions are mostly ignored by compilers due to semantic gap &#8594;
                  simple instructions are easily compiler-optimizable. </li>
                  <li>Complex addressing modes lead either to variable length instructions or big instruction size
                   &#8594; inefficient decoding and scheduling as well as alignment issues.</li>
                </ul>
              </li><br/>
              <li><b>Large register set</b>
                <ul>
                  <li>Save cycles on memory requests.</li>
                  <li>Expose opportunities for instruction parallelism to the compiler.</li>
                </ul>
              </li>
            </ol>
          </section>
        </section>

        <!-- caches -->

        <section>
          <h2>The memory-processor gap</h2>
          <p>Processor advances much faster than memory.</p>
          <div><img src="images/popt/hp-mem.jpg"></div>
          <small>image source Hennessey &amp; Paterson</small>
          <p><strong>The goal for cache hierarchy is to mitigate a performance gap between<br/>latencies
          of memory access and processor operations</strong></p>
        </section>

        <section>
          <h2>Cache hierarchy</h2>
          <ul>
            <li><b>Cache</b> is a small fast memory usually integrated on the chip that holds recently used data or instructions
            for further use. It is slower than accessing registers but faster than main memory.</li>
            <li>Modern computers have memory of different types and properties organized in hierarchy, several levels
            of caches lay between processor and main memory. Hierarchy offers a reasonable tread off between speed, size
            and cost.</li>
            <li>Modern architectures belong to Harvard class so have data and instruction cache separated.</li>
            <li>Cache is usually marked as <b><code>Lx</code></b> for referring to any type of it. <b><code>Dx</code></b>,
             for <strong>data</strong> cache and <b><code>Ix</code></b> for <strong>instruction</strong> cache.</li>
            <li>L1 is used to call the closest to processor level of cache.</li>
            <li>Cache may offer some shrink in power consumption as well.</li>
          </ul>
        </section>


        <section>
          <h2>Locality principals</h2>
          <dl>
            <dt><b>Temporal locality</b></dt>
            <dd>relies on the fact that <strong>recently accessed items</strong> are <strong>likely</strong>
            to be accessed in the <strong>nearest future</strong>. If address <b><code>M(n)</code></b>
            is accessed at time <b><code>t</code></b>, it is likely that <b><code>M(n)</code></b>
            will be used again at time <b><code>t+</code>&epsilon;</b> , where <b>&epsilon;</b>
            is small.</dd><br/>

            <dt><b>Spatial locality</b></dt>
            <dd>relies on the fact that items whose addresses are <strong>near each other likely to accessed close
            together in time</strong>. If address <b><code>M(n)</code></b> is accessed it is likely that
            <b><code>M(n+&epsilon;)</code></b> will be used, where <b><code>&epsilon;</code></b> is small.</dd>
          </dl>
        </section>

        <section>
          <section>
            <h2>terminology</h2>
            <dl>
              <dt><b>Access time</b></dt>
              <dd>is the time from when a read or write is requested until it arrives at its destination.</dd>
              <dt><b>Cycle time</b></dt>
              <dd>is the minimum time between requests to memory.</dd><br/>
              <dt><b>Cache hit</b></dt>
              <dd>is the situation when the requested data is in the cache.</dd>
              <dt><b>Cache miss</b></dt>
              <dd>is the situation when the requested data isn't in the cache. Upon a cache miss, a cache line is retrieved
               from main memory (or a higher level of cache) and placed in the cache.</dd>
            </dl>
          </section>

          <section>
            <h2>terminology</h2>
            <dl>
              <dt><b>Cache miss rate</b></dt>
              <dd>is the fraction of cache accesses that result in a miss.</dd>
              <dt><b>Local miss rate</b></dt>
              <dd>is the number of misses in a cache divided by the total number of memory access to this cache</dd>
              <dt><b>Global miss rate</b></dt>
              <dd>is the number of misses in a cache divided by the total number of memory accesses generated</dd>
              <br/>
              <dt><b>Cache miss penalty</b></dt>
              <dd>is the time required for serving a cache miss<br/>(depends on the latency and bandwidth of the memory).</dd><br/>
              <dt><b>Memory stall cycles</b></dt>
              <dd>is the cycles during which a CPU is stalled waiting for a memory access.</dd>
            </dl>
          </section>
        </section>

<!--         Using multiple levels of cache allows a small fast cache to keep pace with the
CPU, while slower larger caches can be used to reduce the miss penalty since
the next level cache can be used to capture many accesses that would go to
main memory.
The local miss rate is large for higher level caches because the first level
cache benefits the most from data locality.  Thus a global miss rate that
indicates what fraction of the memory access that leave the CPU go all the way
to memory is a more useful measure. -->

        <section>
          <h2>Cache properties </h2>
          <ul>
            <li>Modern processors have from 1 to 4 levels of caches
            <ul>
              <li><b>Inclusive</b> caches duplicates the data from higher level cache in lower levels</li>
              <li><b>Exclusive</b> caches contains data only in one level of cache</li>
              <li>Most caches are <b>inclusive</b> which simplifies coherency handling.</li>
            </ul>
            </li><br>
            <li>Data transfer unit between registers and L1 cache is usually in between 1-16 bytes</li>
            <li><b>Cache line</b> is the smallest transfer unit between main memory(or higher level cache) and cache
            (typically 32, 64 bytes, sometimes 128 bytes and more). Cache line is also unit of storage
            allocation in cache</li><br>
            <li>Whole cache is split into <b>sets</b> equal in size.</li>
            <li>Each line has a <b>tag</b> that indicates the address in memory from which the line has been copied.</li>
          </ul>
        </section>

        <section>
          <h2>Associativity</h2>
          <ul>
            <li><b>Cache associativity</b> is the number of unique places in a cache where any given memory item can reside.</li>

            <li>Cache hit is detected through an <b>associative search</b> of all the tags.</li>
          </ul>
          <blockquote>Consequently, there is a <b>cache size/hit latency tread off</b>,<br/>large caches can keep mere data,
            but need more time for tag search.</blockquote>
          <ul>
            <li>In an <b>N-way set associative</b> cache, any cache line from memory can map to any of the
            N locations in a <b>set</b>. 2-/4-/8-/16-way associative caches are often used in modern chips.</li>
            <li>1 way set associative cache is called <b>direct mapped</b>. Direct mapped means only one location,
            and so high probability of conflict, but tag search is very fast.</li>
            <li>A fully associative cache is one in which a cache line can be placed anywhere in cache. Full association
            is feasible for very small size caches only (TLB's).</li>
          </ul>
          <blockquote>Higher associativity gives faster caches, but costly in terms of gates and complexity
          (power and performance).</blockquote>
        </section>

        <section>
          <section>
            <h2>Direct Mapped</h2>
            <img class="simple" width="50%" src="images/popt/dmc.svg"/>
          </section>
          <section>
            <h2>Set-associative</h2>
            <img class="simple" width="50%" src="images/popt/sac.svg"/>
          </section>
          <section>
            <h2>Fully-associative</h2>
            <img class="simple" width="50%" src="images/popt/fac.svg"/>
          </section>
        </section>

        <section>
          <h2>Cache misses taxonomy</h2>
          <ol>
            <li><b>Compulsory</b> miss refers to a miss for the <i>very first</i> access for a cache line,
            which is unavoidable for any program.</li><br>

            <li><b>Capacity</b> miss is the situation than the cache <i>cannot contain all the cache lines</i> needed
            during execution of a program. This type is inevitable for any production program.</li><br>

            <li><b>Conflict</b> miss occurs when a block is to be discarded and later retrieved because too many cache
            lines mapped to the <i>same set</i>. It is only applicable to a (less than fully) set associative caches.
            Conflict misses may in the worst case lead to a <b>cache trashing</b> where a cache line is repeatedly
            brought in, and then immediately thrown out because of a conflict miss</li><br>

            <!-- miss forced by hardware coherence protocol -->
            <li><b>Coherency</b> miss occurs when a block must be reloaded from the memory or lower level of cache to keep data
            coherent between multiple processors in <abbr title="Symmetric Multi Processor">SMP</abbr> system. </li>
          </ol>
        </section>

        <section>
          <h2>Replacement policy</h2>
          <blockquote style="width:100%;"><b>Replacement policy</b> defines which cache line is to be replaced under a miss.</blockquote>
          <ul>
            <li><b>Random</b> evicts cache lines without any preference or specific order</li><br>
            <li><b>Round robin (RR)</b> evicts cache lines in the FIFO order</li><br>
            <li><b>Least Recently Used (LRU)</b> evicts cache line which was not in use for the longest time.
            It performs better than random or RR but is more difficult to implement</li><br>
            <li><b>Pseudo-LRU</b> evicts one of the least recently used. close to LRU in term of cache efficiency,
            but much faster to operate.</li><br>
            <li><b>Belady</b> evicts block that will be used the furthest in future. Unreachable optimum.</li>
          </ul>
        </section>

        <section>
          <h2>Write strategy</h2>
          <blockquote><b>Write Strategy</b> is a strategy to maintain coherence between caches in a hierarchy
          and the main memory while write occur.</blockquote>
          <p>Data is in the cache (write hit)</p>
          <dl>
            <dt><b>Write through</b></dt>
            <dd>The data is written through all cache hierarchy up to main memory. Memory and cache are always in
            coherent state. Uniform latency on misses, but extra bus traffic.</dd>
            <dt><b>Write back</b></dt>
            <dd>The data is written only to the cache line in the cache. The modified cache line
            is written to memory only when necessary (it need to be replaced by another one). Memory and caches can be
            in not coherent state. Non-uniform miss latency
              <ul>
                <li><b>Clean miss</b> results in one transaction with lower level (fill)</li>
                <li><b>Dirty miss</b> results in two transactions with lower level (writeback & fill)</li>
              </ul>
            </dd>
          </dl>
        </section>

        <section>
          <h2>Allocation policy</h2>
          <blockquote><b>Allocation policy</b> is a policy of a new cache allocation<br> if the write miss occurs.
          </blockquote>
          <p>Data is not in the cache (write miss)</p>

          <dl>
            <dt><b>Write-allocate</b> (is to be used with write back)</dt>
            <dd>Allocate a line in a cache to store by retrieving the data from lower level of cache and update the value.
            It requires <strong>additional bandwidth</strong>, but may <strong>decrease miss rate</strong> for reads.</dd>

            <dt><b>Write-non-allocate</b> (is to be used with write through)</dt>
            <dd>By pass the cache and store the data directly to the memory or lower level of cache. No extra bandwidth
            is added, but might result in miss rate increase.</dd>
          </dl>
        </section>

        <section>
          <h2>Write buffer</h2>
          <blockquote><b>Write buffer</b> is a buffer between the current level of cache and the next level of
          cache or memory</blockquote>
          <ul>
            <li>Store latency is not uniform ( e.g. if a cache line must be read or allocated)</li>
            <li>Buffering writes is used to avoid stalling the CPU. Especially helpful for write-back</li>
            <li>Write buffer (store buffer/write-behind buffer) is a FIFO queue of incomplete stores</li>
            <li>intermediate values can be forwarded for use upon request</li>
          </ul>

<!-- Store A to service load of a value that is still in write buffer avoids unnecessary stalls of load operations

Store B Implies that memory contents are temporarily stale
on a multiprocessor, CPUs see different order of writes
“weak store order”, to be revisited in SMP context -->

        </section>

        <section>
          <section>
            <h2>Hardware cache optimizations</h2>
            <p>Basic:</p>
            <ol>
              <li>Larger block size to reduce miss rate</li>
              <li>Bigger caches to reduce miss rate</li>
              <li>Higher associativity to reduce miss rate</li>
              <li>Multilevel caches to reduce miss penalty</li>
              <li>Giving priority to read misses over writes to reduce miss penalty</li>
              <li>Avoiding address translation during indexing of the cache to reduce hit time</li>
              <li>Victim buffer to reduce conflict misses</li>
            </ol>
          </section>

          <section>
            <h2>Hardware cache optimizations</h2>
            <p>Advanced:</p>
            <ol>
              <li>Small and Simple First-Level Caches to Reduce Hit Time and Power</li>
              <li>Way Prediction to Reduce Hit Time</li>
              <li>Pipelined Cache Access to Increase Cache Bandwidth</li>
              <li><b>Nonblocking Caches to Increase Cache Bandwidth</b></li>
              <li>Multi-banked Caches to Increase Cache Bandwidth</li>
              <li>Critical Word First and Early Restart to Reduce Miss Penalty</li>
              <li>Merging Write Buffer to Reduce Miss Penalty</li>
              <li>Hardware Prefetching of Instructions and Data to Reduce Miss Penalty or Miss Rate</li>
            </ol>
          </section>
        </section>

        <section>
          <h2>Nonblocking Caches</h2>
          <blockquote><b>Nonblocking cache (or Lockup Free)</b> is a cache design for which cache continues to supply
          cache hits during a miss, called <b>hit under miss</b>, or <b>hit under multiple miss</b>.</blockquote>

          <ul>
            <li>Especially useful for out-or-order pipeline</li>
            <li>Out-or-oder with coup of nonblocking cache is generally capable of hiding the  miss penalty of an L1 data cache miss that hits
            in the L2 cache, but is not capable of hiding a significant portion of the L2 miss penalty.</li>
            <li>Hit under miss significantly increases the complexity of the cache controller with the complexity
            increasing as the number of outstanding misses allowed increases.</li>
          </ul>
        </section>

        <section>
          <h2>Victim buffer</h2>
          <blockquote><b>Victim buffer (or Victim cache)</b> is a very small fully-associative cache (&approx;4 entries) used to store
          evicted cache lines before complete kick out. If the cache line in a victim buffer is requested it is placed
          in cache again.</blockquote>
          <ul>
            <li>In fact it is 4 extra ways, shared among all sets</li>
            <li>VB is very useful to deal with conflict misses</li>
            <li>VB helps then associativity is not enough</li>
          </ul>
        </section>

        <section>
          <h2>Hardware prefetching</h2>
          <blockquote><b>Hardware prefetching</b> is the mechanism implemented in cache HW that puts memory blocks promising to be accessed
           in the nearest future speculatively in cache.</blockquote>
          <ul>
            <li>Hardware is able to recognize different access pattern</li>
            <li>Prefetcher keeps on track multiple read and write streams</li>
            <li>The simplest pattern is streaming kernel.</li>
            <li>The compromise should be found
              <ul>
                <li>Do not evict useful data</li>
                <li>Properly select prefetching distance</li>
              </ul>
            </li>
          </ul>
        </section>

<!--         <section>
          <h2>Coherency problem</h2>
          <p>Cache Contention on SMPs</p>
          <ul>
            <li>When two or more CPUs alternately and repeatedly update the same cache line</li>
            <li>memory contention
              <ul>
                <li>when two or more CPUs update the same variable</li>
                <li>correcting it involves an algorithm change</li>
              </ul>
            </li>
            <li>false sharing
              <ul>
                <li>when CPUs update distinct variables that occupy the same cache line</li>
                <li>correcting it involves modification of data structure layout</li>
              </ul>
            </li>
          </ul>
        </section> -->

        <section>
          <h2>Pragmatics: caches</h2>
          <ul>
            <li>Don't access for write into the same cache lines from different processes<br>
            &#8594; avoid coherency conflicts</li>
            <li>Ensure data locality, use proper data structures and algorithms<br>
            &#8594; avoid contention conflicts</li>
            <li>Fetch from many different streams carefully<br>
            &#8594; avoid associativity conflicts</li>
            <li>Tile for the largest non-shared cache. Never block for the shared cache size<br>
            &#8594; avoid contention conflicts<br>
            <small>Depending on the speed difference and amount of work per iteration, L1 may be better.</small></li>
            <li>Use software prefetching (<code>PLD</code>)<br>
            &#8594; reduce cold cash misses<br>
            <small>Prefetching suits contiguous memory accesses patterns, but not random</small></li>
            <!-- <li>Compiler Optimizations to Reduce Miss Rate</li> -->
            <!-- <li>Compiler-Controlled Prefetching to Reduce Miss Penalty or Miss Rate</li> -->
          </ul>
        </section>

        <section>
          <h2>Optimize for caches</h2>
          <ul>
            <li>Strip mining & tiling </li>
            <li>Stride minimization</li>
          </ul>
        </section>

        <section>
          <h1>Pipelined execution</h1>
        </section>

        <section>
          <section>
            <h2>Pipelined execution - Classic 5-stage pipeline</h2>
            <p>The idea is to break down an instruction into multiple stages so that instructions
            on different stages can be processed at the same clock cycle. Introduced in 1958.</p>
            <dl>
              <dt><b>Instruction fetch cycle (IF)</b></dt>
              <dd>An instructions is fetched from IC and may be placed either into an instruction register
              or a queue of pending instructions.</dd>
              <dt><b>Instruction decode/register fetch cycle (ID)</b></dt>
              <dd>An instruction is decoded, register operands are read from the register file.</dd>
              <dt><b>Execution/effective address cycle (EX)</b></dt>
              <dd>Either an ALU operation is performed or an effective address for memory access is computed,
              which depends on an instruction type</dd>
              <dt><b>Memory access/branch completion cycle (MEM)</b></dt>
              <dd>The memory is accessed, if appropriate.</dd>
              <dt><b>Write­back cycle (WB)</b></dt>
              <dd>The result is written into the register file, whether it comes from the memory system (for a load)
              or from the ALU (for an ALU instruction)</li>
            </dl>
          </section>

          <section>
            <h2>Without pipelined execution</h2>
            <img class="simple" width="80%" src="images/popt/nopipeline.svg">
          </section>

          <section>
            <h2>Pipelined execution</h2>
            <img class="simple" width="80%" src="images/popt/pipeline.svg">
          </section>

          <section>
            <h2>Pipeline hazards</h2>
            <p><b>Pipeline hazard</b> is a situation that prevents the next instruction in the pipeline from executing
            during its designated clock cycle and thus cause pipeline stalls.</p>
            <dl style="text-align: justify;">
              <dt><b>Structural hazard</b></dt>
              <dd>arise from resource conflicts when the hardware cannot support all possible combinations of instructions
              simultaneously in overlapped execution.</dd>
              <dt><b>Data hazard</b></dt>
              <dd>arise when an instruction depends on the results of a previous instruction in a way that is exposed
              by the overlapping of instructions in the pipeline.</dd>
              <dt><b>Control hazard</b></dt>
              <dd>arise from the pipelining of branches and other instructions that change the PC.</dd>
            </dl>
            <br/><small>All the terms used are taken from Hennessey & Paterson</small>
          </section>

          <section>
            <h2>Superscalar</h2>
            <img class="simple" width="60%" src="images/popt/super-scalar.png">
          </section>

          <section>
            <h2>multiple-issue superscalars: Cortex-A53</h2>
            Instructions are dispatched 2 per cycle to the appropriate issue queue.<br/>
            Issue Width &mdash; 2 &mu;ops, Pipeline Length &mdash; 8 stages, Pipeline Length &mdash; 8 stages,
            Number of pipes &mdash; 5: I0, I1, FP, B, LD/DT.
            <div><img width="50%" src="images/popt/A53-Diagram.png"></div>
            <small>image source is <a href="http://images.anandtech.com/doci/8718/A53-Diagram.png">AnandTech.com</a></small>
          </section>
        </section>

<!--         <section>
          <h2>Speculative execution</h2>
        </section>

        <section>
          <h2>Branch prediction</h2>
        </section>

        <section>
          <h2>Out-of-order execution</h2>
        </section>

        <section>
          <h2>Prefetch streams and write stream</h2>
        </section>

        <section>
          <h2>SMP vs AMP</h2>
          <p>difference, coherency problems, multi-core vs multi-cluster big.LITTLE</p>
        </section>

        <section>
          <h2>SIMD vs Vector co-processors</h2>
        </section>

        <section>
          <h2>Summary</h2>
          <ul>
            <li>Deep pipeline is to a silver-bullet (CPI, overhead)</li>
            <li>Programmer is almost unable to schedule registers on modern architectures: OoO, renaming, optimizing compilers
          </ul>
        </section> -->

        <section id="end1">
          <h1>THE END</h1>
          <img class="simple" src="images/popt/infinity.png">
          <h4><a href="https://github.com/cuda-geek">Marina Kolpakova</a> / 2015</h4s>
        </section>

<!--         <section>
At least 3 major approaches:
Make cores as fast/slow as main memory (SiCortex, Tensilica)
Add faster/closer memory pipes (Opteron, Nehalem)
Streaming compute engines (NVIDIA,AMD), vectorized memory pipelines (Convey).

With multiple cores, more than one level can keep MESI states.

Hardware Prefetching
When you see linear scaling graphs, be (somewhat) suspicious.
Linear scalability is easy(ier) when per-core performance is low!
The faster a single core computes, the more vulnerable it is to other bottlenecks.
So producing a linear graph, does not make your program efficient.
        </section> -->

      </div>

    </div>

    <script src="plugin/reveal/lib/js/head.min.js"></script>
    <script src="plugin/reveal/js/reveal.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        slideNumber: true,
        history: true,
        center: false,

        width: 960,
        height: 720,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'plugin/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>
