<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Pragmatic optimization in modern programming</title>
    <meta name="description" content="Pragmatic optimization in modern programming - Introduction">
    <meta name="author" content="Marina Kolpakova (cuda.geek)">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="plugin/reveal/css/reveal.css">
    <link rel="stylesheet" href="plugin/reveal/css/theme/geek.css" id="theme">
    <link rel="stylesheet" href="css/colors-orange.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="plugin/reveal/lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'plugin/reveal/css/print/pdf.css' : 'plugin/reveal/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="plugin/reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Pragmatic optimization</h1>
          <h2>in modern programming</h2>
          <h3>Modern computer architecture concepts</h3>
          <!-- Optimizations in hardware -->
          <!-- Hardware optimization capabilities -->
          <br>
          <br>
          <br>
          <small>Created by
            <a href="http://github.com/cuda-geek">Marina Kolpakova</a>
            for
            <a href="http://www.unn.ru/eng/">UNN</a>
          </small>
        </section>

        <section>
          <h2>Themes &amp; Contents</h2>
          <ul>
            <li><strong>Pragmatics</strong>
              <ul>
                <li>Ordering optimization approaches</li>
                <li>Demystifying a compiler</li>
                <li>Mastering compiler optimizations</li>
              </ul>
            </li>
            <li><strong>Computer Architectures</strong>
              <ul>
                <li><b>Modern computer architectures</b></li>
                <li>SIMD extensions</li>
                <li>Specific co-processors</li>
              </ul>
            </li>
          </ul>
        </section>

        <section>
          <h2>Outline</h2>
          <ul>
            <li>Modern architecture taxonomy
              <ul>
                <li>RISC</li>
                <li>CISC</li>
                <li>VLIW</li>
                <li>Vector</li>
              </ul>
            </li>
            <li>Hardware optimizations
              <ul>
                <li>Pipelined execution</li>
                <li>Speculative execution</li> <!--multi-piping-->
                <li>Out-of-order execution</li>
                <li>Cache hierarchies</li>
                <li>Branch prediction</li>
                <li>Hardware prefetching</li>
                <li>Write stream detection</li>
              </ul>
            </li>
            <li>summary</li>
          </ul>
        </section>

        <section>
          <h2>Key architecture families</h2>
          <ul>
            <li><strong>RISC</strong> - Reduced Instruction Set Computer</li><br/>
            <li><strong>CISC</strong> - Complex Instruction Set Computer</li><br/>
            <li><strong>VLIW</strong> - VEry Long Instruction Word</li><br/>
            <li><strong>Vector</strong> architectures</li>
          </ul>
        </section>

        <section>
          <h2>CISC</h2>
          <b>Complex Instruction Set Computer</b>
          <dl style="text-align: justify;">
            <dt><strong>Designed in the 1970s</strong></dt>
            <dd>which was a time there transistors were expensive and compilers were weak.</dd>
            <dt><strong>The goal</strong></dt>
            <dd>is to define an instruction set that allows high level language constructs be translated into as few
            assembly language instructions as possible.</dd>
            <dt><strong>Features</strong></dt>
            <dd>As the result <i>many instructions access memory</i>, <i>plenty of addressing modes</i>,
            many instruction families, <i>very reach ISA</i>, and consequently, <i>complicated instruction decoding</i> logic.
            Only a <i>few registers are available</i> for programmers.</dd>
            <dt><strong>Examples</strong></dt>
            <dd>VAX, x86, AMD64.</dd>
            <dt><strong>Nowadays</strong></dt>
            <dd>CISC instructions are typically broken down into lower level instructions called microcode which are
            much easy to pipeline, large reorder buffers to eliminate stalls.</dd>
          </dl>
        </section>

        <section>
          <h2>RISC</h2>
          <b>Reduced Instruction Set Computer</b>
          <dl style="text-align: justify;">
            <dt><strong>Designed in the 1980s</strong></dt>
            <dd>which was a time there IPL was the most concern.</dd>
            <dt><strong>The goal</strong></dt>
            <dd>decrease the number of clocks per instruction (CPI), pipeline instructions as much as possible.</dd>
            <dt><strong>Features</strong></dt>
            <dd>Relatively few instructions all the same length. Only load and store instructions access memory.
            Has more registers than CISC processors have. <strike>No microcode</strike></dd>
            <dt><strong>Examples</strong></dt>
            <dd>MIPS, ARM.</dd>
            <dt><strong>Nowadays</strong></dt>
            <dd>Most architectures that comes from RISC are called Load-Store architectures. They use most modern
            hardware enhancements with deeper pipelines, multi-cycle instructions, OoO execution. Modern RISC may have
            micro ops. Whereas they still employ special instructions for memory accesses.</dd>
          </dl>
        </section>

        <section>
          <h2>VLIW</h2>
          Very Long Instruction Word
          <ul>
          <dl style="text-align: justify;">
            <dt><strong>Designed in the 1980s</strong></dt>
            <dd>which was a time there IPL was the most concern.</dd>
            <dt><strong>The goal</strong></dt>
            <dd>is maximize instruction level parallelism (ILP) with pipelining done by software.</dd>
            <dt><strong>Features</strong></dt>
            <dd>Software determines which instructions can be performed in parallel, bundles this information
            and the instructions, and passes the bundle to the hardware.</dd>
            <dt><strong>Examples</strong></dt>
            <dd>Intel­HP Itanium.</dd>
            <dt><strong>Nowadays</strong></dt>
            <dd>Aren't used in generic processors design, but widely for programmable co-processors where shrink in
            the power consumption is crustal.</dd>
          </dl>
        </section>

        <section>
          <h2>Vector Processors</h2>
          <dl>
            <dt><strong>First introduced in 1976.</strong></dt>
            <dd>dominated for HPC in the 1980s because of high instruction throughput.</dd>
            <dt><strong>The goal</strong></dt>
            <dd>perform operations on vectors of data to increase instruction throughput. Vector pipelining which is
            also called chaining.</dd>
            <dt><strong>Examples</strong></dt>
            <dd>Cray</dd>
            <dt><strong>Nowadays</strong></dt>
            <dd>Aren't used in generic processors design, but used as a co-processors for a specific workloads.</dd>
          </dl>
        </section>

        <section>
          <h2>SIMD vs Vector co-processors</h2>
        </section>

        <section>
          <h2>Hardware optimizations</h2>
          <ul>
            <li>Pipelined execution</li><br/>
            <li>Speculative execution</li><br/>
            <li>Out-of-order execution</li><br/>
            <li>Cache hierarchies</li><br/>
            <li>Branch prediction</li><br/>
            <li>Hardware prefetching</li><br/>
            <li>Write stream detection</li>
          </ul>
        </section>

        <section>
          <h2>Pipelined execution</h2>
          › Overlapping the execution of multiple instructions
› Assembly line metaphor
› Simple pipeline stages
Instruction fetch cycle (IF)
Instruction decode/register fetch cycle (ID)
Execution/effective address cycle (EX)
Memory access/branch completion cycle (MEM)
Write­back cycle (WB)
        </section>

        <section>
          <h2>Hazards</h2>
          › Situations that prevent the next instruction in the pipeline
from executing during its designated clock cycle and
thus cause pipeline stalls
› Types of hazards
› Structural hazard – resource conflict when the hardware
cannot support all instructions simultaneously
› Data hazard – when an instruction depends on the results of
a previous instruction
› Control hazard – caused by branches and other instructions
that change the PC
        </section>

        <section>
          <h2>Pipeline with Multicycle Operations</h2>
        </section>

        <section>
          <h2>Speculative execution</h2>
        </section>

        <section>
          <h2>Out-of-order execution</h2>
        </section>

        <section>
          <h2>Cache hierarchies</h2>
          <strong>The goal for caches is to mitigate a performance gap between latency of memory access and processor
          arithmetics.</strong>
          <dl>
            <dt><b>Temporal locality</b></dt>
            <dd>recently accessed items are likely to be accessed in the near future. If we access address
            <code>M(n)</code> at time t, it is likely that <code>M(n)</code> will be used again at time t + &epsilon;,
            where &epsilon; is small.</dd><br/>
            <dt><b>Spatial locality</b></dt>
            <dd>items whose addresses are near each other likely to accessed close together in time. If we access
            <code>M(n)</code> it is likely that <code>M(n + &epsilon;)</code> will be used, where
            <code>&epsilon;</code> is small.</dd>
          </dl>
        </section>

        <section>
          <section>
            <h2>Cache terminology</h2>
            <dl>
              <dt><b>Access time</b></dt>
              <dd>is the time from when a read or write is requested until it arrives at its destination.</dd><br/>
              <dt><b>Cycle time</b></dt>
              <dd>is the minimum time between requests to memory.</dd><br/>
              <dt><b>Cache line size</b></dt>
              <dd>is the smallest unit of memory that can be transferred to and form main memory.</dd><br/>
              <dt><b>Cache hit</b></dt>
              <dd>is the situation when the requested data is in the cache.</dd><br/>
              <dt><b>Cache hit</b></dt>
              <dd>is the situation when the requested data isn't in the cache. Upon a cache miss, a cache line is retrieved
               from main memory (or a higher level of cache) and placed in the cache.</dd>
            </dl>
          </section>

          <section>
            <h2>Cache terminology</h2>
            <dl>
              <dt><b>Cache miss rate</b></dt>
              <dd>is the fraction of cache accesses that result in a miss.</dd><br/>
              <dt><b>Cache miss penalty</b></dt>
              <dd>is the time required for a cache miss (depends on both the latency and bandwidth of the memory).</dd><br/>
              <dt><b>Memory stall cycles</b></dt>
              <dd>is the cycles during which the CPU is stalled waiting for memory access</dd>.
            </dl>
          </section>
        </section>

        <section>
          <h2>Cache properties </h2>
          <dl>
            <dt><b>Number of caches</b></dt>
            <dt><b>Cache line size</b></dt>
            <dd>is the smallest unit of memory that can be transferred to and form main memory.</dd>
            <dt><b>Cache sizes</b></dt>
            <dt><b>Associativity</b></dt>
            <dt><b>Replacement policy</b></dt>
            <dt><b>Write strategy</b></dt>
          </dl>
        </section>

        <section>
          <h2>Cache misses taxonomy</h2>
          <dl>
            <dt><b>Compulsory</b></dt>
            <dd>refers to misses for the very first access to a cache line.</dd>
            <dt><b>Capacity</b></dt>
            <dd>is the situation than the cache cannot contain all the cache lines needed during execution of a program.</dd>
            <dt><b>Conflict</b></dt>
            <dd>occurs when a block must be discarded and later retrieved because too many cache lines mapped to the
            same set. It is only applicable to a (less than fully) set associative or direct mapped cache.</dd>
          </dl>
        </section>

        <section>
          <h2>Nonblocking Caches</h2>
          Pipelined computers that allow out-of-order execution can continue fetching instructions from the instruction
          cache while waiting on a data cache miss. A nonblocking cache design allows the data cache to continue to
          supply cache hits during a miss, called <b>hit under miss</b>, or <b>hit under multiple miss</b> if multiple
          misses can be overlapped.  Hit under miss significantly increases the complexity of the cache controller,
          with the complexity increasing as the number of outstanding misses allowed increases. Out-of-order processors
          with hit under miss are generally capable of hiding the  miss penalty of an L1 data cache miss that hits
          in the L2 cache, but are not capable of hiding a significant portion of the L2 miss penalty.
        </section>

        <section>
› Usually between 32 and 128 bytes
› In an n­way associative cache, any cache line from
memory can map to any of the n locations in a set.
› 1­way set associative cache is called direct mapped
› A fully associative cache is one in which a cache line can be
placed anywhere in cache.



Using multiple levels of cache allows a small fast cache to keep pace with the
CPU, while slower larger caches can be used to reduce the miss penalty since
the next level cache can be used to capture many accesses that would go to
main memory.
The local miss rate is large for higher level caches because the first level
cache benefits the most from data locality.  Thus a global miss rate that
indicates what fraction of the memory access that leave the CPU go all the way
to memory is a more useful measure.  Let us define these terms as follows:
› local miss rate – the number of misses in a cache divided by the total number of
memory access to this cache
› global miss rate – the number of misses in a cache divided by the total number of
memory accesses generated



Possible policies
› Least Recently Used (LRU)
› Random
› Round robin
› LRU performs better than random or round
robin but is more difficult to implement.

Cache Write Strategy
› When a store instruction writes data into a cache­
resident line, one of following policies is usually used:
› Write through: The data are written to both the cache line in
the cache and to main memory.
› Write back: The data are written only to the cache line in the
cache.  The modified cache line is written to memory only
when necessary (e.g., when it is replaced by another cache
line.

At least 3 major approaches:
Make cores as fast/slow as main memory (SiCortex, Tensilica)
Add faster/closer memory pipes (Opteron, Nehalem)
Streaming compute engines (NVIDIA,AMD),
vectorized memory pipelines (Convey).

Memory cannot live anywhere in a cache.
Cache associativity – The number of unique
places in a cache where any given memory item
can reside.
–
–
Location is determined by some bits in the physical
or virtual address..
Direct mapped means only one location.
But very, very fast.
Higher associativity is better, but costly in terms of
gates and complexity (power and performance).

Most caches are inclusive.
–
●
With multiple cores, more than one level can
keep MESI states.
–
●
Data kept in multiple levels at the same time
In Nehalem, L3 keeps state per socket, L1 and L2 per
core
Transitions to E, I, S are often performance hits.
–
NOTUR2009
But they can be identified with the right tools and
insight.

Hardware Prefetching

When you see linear scaling graphs, be
(somewhat) suspicious.
Linear scalability is easy(ier) when per-core
performance is low!
The faster a single core computes, the more
vulnerable it is to other bottlenecks.
–
●
Memory, Sync, Comm, I/O
So producing a linear graph, does not make your
program efficient.


Cache Blocking for Multicore
●
Generally, block for the largest non-shared
cache.
–
●
●
L2 on Nehalem.
Depending on the speed difference and amount
of work per iteration, L1 may be better.
Never block for the shared cache size.

Reduced Instruction Set Architecture
We can:
Design, place and route more elegantly.
Drive a higher clock rate
Have a deeper pipeline.
Expose opportunities for instruction parallelism to the compiler.
Guess what? Your Pentium is a RISC.
●
CISC translated to RISC “micro-ops”.

●
●
Reduced Instruction Set Architecture
If we:
Keep the number of instructions small.
Keep the functionality of the instructions orthogonal.
Keep the instructions isolated to one piece of hardware on chip.

        </section>

        <section>
          <h2>Coherency</h2>
          Cache Contention on SMPs
          › When two or more CPUs alternately and repeatedly update the same cache line
          › memory contention
          •  when two or more CPUs update the same variable
          • correcting it involves an algorithm change
          › false sharing
          • when CPUs update distinct variables that occupy the same cache line
          • correcting it involves modification of data structure layout
        </section>

        <section>
          <h2>Branch prediction</h2>
        </section>

        <section>
          <h2>Prefetch streams and write stream</h2>
        </section>

        <section>
          <h2>Summary</h2>
        </section>

        <section id="end1">
          <h1>THE END</h1>
          <img class="simple" src="images/popt/infinity.png">
          <h4><a href="https://github.com/cuda-geek">Marina Kolpakova</a> / 2015</h4s>
        </section>

      </div>

    </div>

    <script src="plugin/reveal/lib/js/head.min.js"></script>
    <script src="plugin/reveal/js/reveal.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        slideNumber: true,
        history: true,
        center: false,

        width: 960,
        height: 720,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'plugin/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>
