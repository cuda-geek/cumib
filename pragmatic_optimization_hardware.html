<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Pragmatic optimization in modern programming</title>
    <meta name="description" content="Pragmatic optimization in modern programming - Introduction">
    <meta name="author" content="Marina Kolpakova (cuda.geek)">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

    <link rel="stylesheet" href="plugin/reveal/css/reveal.css">
    <link rel="stylesheet" href="plugin/reveal/css/theme/geek.css" id="theme">
    <link rel="stylesheet" href="css/colors-orange.css">

    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="plugin/reveal/lib/css/zenburn.css">

    <style type="text/css">
    .rotate
    {
      transform: rotate(-90.0deg);
      -moz-transform: rotate(-90.0deg);
      -webkit-transform: rotate(-90.0deg);
      -o-transform: rotate(-90.0deg);
      filter: progid: DXImageTransform.Microsoft.BasicImage(rotation=0.083);
      -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0.083)";
    }
    </style>

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'plugin/reveal/css/print/pdf.css' : 'plugin/reveal/css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="plugin/reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Pragmatic optimization</h1>
          <h2>in modern programming</h2>
          <h3>Modern computer architecture concepts</h3>
          <!-- Optimizations in hardware -->
          <br>
          <br>
          <br>
          <small>Created by
            <a href="http://github.com/cuda-geek">Marina Kolpakova</a>
            for
            <a href="http://www.unn.ru/eng/">UNN</a>
          </small>
        </section>

        <section>
          <h2>Themes &amp; Contents</h2>
          <ul>
            <li><strong>Pragmatics</strong>
              <ul>
                <li>Ordering optimization approaches</li>
                <li>Demystifying a compiler</li>
                <li>Mastering compiler optimizations</li>
              </ul>
            </li>
            <li><strong>Computer Architectures</strong>
              <ul>
                <li><b>Modern computer architectures</b></li>
                <li>SIMD extensions</li>
                <li>Specific co-processors</li>
              </ul>
            </li>
          </ul>
        </section>

        <section>
          <h2>Outline</h2>
          <ul>
            <li>Modern architecture taxonomy
              <ul>
                <li><strong>RISC</strong></li>
                <li>CISC</li>
                <li>VLIW</li>
                <li>Vector</li>
              </ul>
            </li>
            <li>Hardware optimizations
              <ul style="width:100%; columns: 2; -webkit-columns: 2; -moz-columns: 2;">
                <li>Cache hierarchies</li>
                <li>Hardware prefetching</li>
                <li>Write stream detection</li>

                <li>Pipelined execution</li>
                <li>Superscalar execution</li>
                <li>Speculative execution</li>
                <li>Out-of-order execution</li>
              </ul>
            </li>
            <li>summary</li>
          </ul>
        </section>

        <section>
          <section>
            <h2>Three Aspects of Computer Architecture</h2>
            <dl style="text-align: justify;">
              <dt><b>Instruction Set Architecture or ISA (interface)</b></dt>
              <dd>is a contract between the Hardware and Software, which specifies <strong>right</strong>,
              <strong>possibilities</strong> and <strong>limitations</strong>.<br/>
                <ul>
                  <li>Class of ISA (e.g. load-store, register-memory)</li>
                  <li>Memory addressing (e.g. alignment rules)</li>
                  <li>Addressing modes (e.g. base-immediate, base-register)</li>
                  <li>Types and sizes of operands (e.g. size of byte, short)</li>
                  <li>Operations (e.g. general arithmetic, control, logical)</li>
                  <li>Control flow instructions (e.g. branches, jumps, calls, and returns)</li>
                  <li>Encoding an ISA (e.g. fixed or variable length)</li>
                </ul>
              </dd>

              <dt><b>Microarchitecture (organization)</b></dt>
              <dd>is a concrete implementation of the ISA, the high-level aspects of a processors design (memory system,
              memory interconnect, design of the internal processor).</dd>

              <dt><b>Hardware (design)</b></dt>
              <dd>is the specifics of a computer, including the logic design and packaging, a concrete implementation of
              the microarchitecture.</dd>
            </dl>
          </section>

          <section>
            <h2>Example: 64-bit ARM</h2>
              <table>
                <colgroup>
                  <col></col>
                  <col></col>
                  <col></col>
                  <col></col>
                </colgroup>
                <thead>
                  <tr>
                    <th>ISA</th>
                    <th>&mu;arch</th>
                    <th>IP</th>
                    <th>Hardware</th>
                  </tr>
                </thead>
                <tbody style="">
                  <tr>
                    <td style="background-color:rgba(100,100,100,0.5); vertical-align: middle; max-width:1em;" rowspan="11">
                      <div class='rotate'>Armv8-a</div>
                    </td>
                    <td>Cortex-A53</td>
                    <td>ARM</td>
                    <td>Octa Exynos 7(7580) 1.6GHz 28nm <abbr title="High-k Metal Gate">HKMG</abbr></td>
                  </tr>
                  <tr>
                    <td>Cortex-A57</td>
                    <td>ARM</td>
                    <td>Octa Exynos 7(7420) <small style="vertical-align:bottom; ">big.LITTLE</small> 2.1/1.5 14nm LPE</td>
                  </tr>
                  <tr>
                    <td>Cortex-A72</td>
                    <td>ARM</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>Cortex-A35</td>
                    <td>ARM</td>
                    <td></td>
                  </tr>
                  <tr>
                    <td>Denver</td>
                    <td>NVIDIA</td>
                    <td>Dual Tegra K1 2.3GHz 28nm <abbr title="High Performance Mobile">HPM</abbr></td>
                  </tr>
                  <tr>
                    <td>Cyclone</td>
                    <td>Apple</td>
                    <td>Dual A7 (APL0698) 1.4GHz 28nm <abbr title="High-k Metal Gate">HKMG</abbr></td>
                  </tr>
                  <tr>
                    <td>Typhoon</td>
                    <td>Apple</td>
                    <td>Dual A8 (APL1012) 1.5GHz 20nm HKMG (TSMC)</td>
                  </tr>
                  <tr>
                    <td rowspan="2">Twister</td>
                    <td rowspan="2">Apple</td>
                    <td>Dual A9 (APL0898) 1.85GHz 16<abbr title="Fin-Shaped Field Effect Transistor">FinFET</abbr> (TSMC)</td>
                  </tr>
                  <tr>
                    <td>Dual A9 (APL1022) 1.85GHz 14<abbr title="Fin-Shaped Field Effect Transistor">FinFET</abbr> (Samsung)</td>
                  </tr>
                  <tr>
                    <td>Kryo</td>
                    <td>Qualcomm</td>
                    <td>Tetra Snapdragon 820 <small style="vertical-align:bottom; ">big.LITTLE</small> 2.2/1.6 14LPP</td>
                  </tr>
                  <tr>
                    <td>Exynos M1</td>
                    <td>Samsung</td>
                    <td>Exynos 8890 ? </td>
                  </tr>
                </tbody>
              </table>
          </section>
        </section>

        <section>
          <section>
            <h2>Key architecture families</h2>
            <ul>
              <li><strong>RISC</strong> - Reduced Instruction Set Computer</li><br/>
              <li><strong>CISC</strong> - Complex Instruction Set Computer</li><br/>
              <li><strong>VLIW</strong> - VEry Long Instruction Word</li><br/>
              <li><strong>Vector</strong> architectures</li>
            </ul>
          </section>

          <section>
            <h2>CISC</h2>
            <b>Complex Instruction Set Computer</b>
            <dl style="text-align: justify;">
              <dt><strong>Designed in the 1970s</strong></dt>
              <dd>which was a time there transistors were expensive and compilers were weak.</dd>
              <dt><strong>The goal</strong></dt>
              <dd>is to define an instruction set that allows high level language constructs be translated into as few
              assembly language instructions as possible, improving code density as well.</dd>
              <dt><strong>Features</strong></dt>
              <dd>As the result <i>many instructions access memory</i>, <i>plenty of addressing modes</i>,
              many instruction families, <i>very reach ISA</i>, and consequently, <i>complicated instruction decoding</i> logic.
              Only <i>few registers are available</i> for programmers.</dd>
              <dt><strong>Examples</strong></dt>
              <dd>VAX, x86, AMD64.</dd>
              <dt><strong>Nowadays</strong></dt>
              <dd>CISC instructions are typically broken down into lower level instructions called microcode which are
              much easy to pipeline, large reorder buffers to eliminate stalls.</dd>
            </dl>
          </section>

          <section>
            <h2>RISC</h2>
            <b>Reduced Instruction Set Computer</b>
            <dl style="text-align: justify;">
              <dt><strong>Designed in the 1980s</strong></dt>
              <dd>which was a time there IPL was the most concern.</dd>
              <dt><strong>The goal</strong></dt>
              <dd>decrease the number of clocks per instruction (CPI), pipeline instructions as much as possible.</dd>
              <dt><strong>Features</strong></dt>
              <dd>Relatively few instructions all the same length. Only load and store instructions access memory.
              Has more registers than CISC processors have. <strike>No microcode</strike></dd>
              <dt><strong>Examples</strong></dt>
              <dd>MIPS, ARM, PowerPC.</dd>
              <dt><strong>Nowadays</strong></dt>
              <dd>Most architectures that comes from RISC are called Load-Store architectures. They use most modern
              hardware enhancements with deeper pipelines, multi-cycle instructions, OoO execution. Modern RISC may have
              micro ops. Whereas they still employ special instructions for memory accesses.</dd>
            </dl>
          </section>

          <section>
            <h2>VLIW</h2>
            Very Long Instruction Word
            <dl style="text-align: justify;">
              <dt><strong>Designed in the 1980s</strong></dt>
              <dd>which was a time there IPL was the most concern.</dd>
              <dt><strong>The goal</strong></dt>
              <dd>is maximize instruction level parallelism (ILP) with pipelining done by software.</dd>
              <dt><strong>Features</strong></dt>
              <dd>Software determines which instructions can be performed in parallel, bundles this information
              and the instructions, and passes the bundle to the hardware.</dd>
              <dt><strong>Examples</strong></dt>
              <dd>Intel­HP Itanium.</dd>
              <dt><strong>Nowadays</strong></dt>
              <dd>Aren't used in generic processors design, but widely for programmable co-processors where shrink in
              the power consumption is crustal.</dd>
            </dl>
          </section>

          <section>
            <h2>Vector Processors</h2>
            <dl style="text-align: justify;">
              <dt><strong>First introduced in 1976.</strong></dt>
              <dd>dominated for HPC in the 1980s because of high instruction throughput.</dd>
              <dt><strong>The goal</strong></dt>
              <dd>perform operations on vectors of data to increase instruction throughput. Vector pipelining which is
              also called chaining.</dd>
              <dt><strong>Examples</strong></dt>
              <dd>Cray</dd>
              <dt><strong>Nowadays</strong></dt>
              <dd>Aren't used in generic processors design, but used as a co-processors for a specific workloads.</dd>
            </dl>
          </section>

          <section>
            <h2>Why everything doing to be <strike>RISC</strike> Load/Store?</h2>
            <p>there is a trend of instruction unification. </p>
            <ol>
              <li>Simple instructions of the same width
                <ul>
                  <li>Simple hardware logic &#8594; power efficient chips</li>
                  <li>Easily compiler-optimizable. Complex instructions are mostly ignored by compilers due to semantic
                  gap</li>
                  <li>Complex addressing modes lead to variable length instructions logic &#8594; inefficient decoding
                  and scheduling as well as alignment issues.</li>
                  <li>Large register set</li>
                  <li>Orthogonal functionality of the instructions orthogonal.</li>
                </ul>
              </li>
              <li>Large register set</li>
              <li>Keep the number of instructions small.</li>
              <li>Keep the instructions isolated to one piece of hardware on chip.</li>
              <li>Decoding logic is power efficient</li>
              <li>Easy for compiler optimizations</li>
              <li>Design, place and route more elegantly</li>
              <li>Drive a higher clock rate</li>
              <li>Have a deeper pipeline</li>
              <li>Expose opportunities for instruction parallelism to the compiler</li>
              <li>Efficient support for procedure calls and returns</li>
            </ol>
          </section>
        </section>

        <section>
          <h2>Hardware optimizations</h2>
          <ul>
            <li>Cache hierarchies</li><br/>

            <li>Pipelined execution</li>
            <li>Superscalar execution</li>
            <li>Speculative execution</li>
            <li>Out-of-order execution</li>
            <li>Forwarding</li><br>

            <li>Hardware prefetching</li>
            <li>Write stream detection</li>
          </ul>
        </section>

        <section>
          <section>
            <h2>The memory-processor gap</h2>
            <div><img src="images/popt/hp-mem.jpg"></div>
            <small>image source Hennessey &amp; Paterson</small>
          </section>

          <section>
            <h2>Cache hierarchies</h2>
            <p><strong>The goal for caches is to mitigate a performance gap between<br/>latencies of memory access
            and processor operations</strong></p>
            <dl style="text-align: justify;">
              <dt><b>Temporal locality</b></dt>
              <dd>relies on the fact that <strong>recently accessed items</strong> are <strong>likely</strong>
              to be accessed in the <strong>near future</strong>. If address <strong><code>M(n)</code></strong>
              is accessed at time <strong><code>t</code></strong>, it is likely that <strong><code>M(n)</code></strong>
              will be used again at time <strong><code>t+</code>&epsilon;</strong> , where <strong>&epsilon;</strong>
              is small.</dd><br/>

              <dt><b>Spatial locality</b></dt>
              <dd>relies on the fact that items whose addresses are <strong>near each other likely to accessed close
              together in time</strong>. If we access <strong><code>M(n)</code></strong> it is likely that
              <strong><code>M(n + &epsilon;)</code></strong> will be used, where <strong><code>&epsilon;</code></strong>
              is small.</dd>
            </dl>
          </section>
        </section>

        <section>
          <section>
            <h2>Cache terminology</h2>
            <dl style="text-align: justify;">
              <dt><b>Access time</b></dt>
              <dd>is the time from when a read or write is requested until it arrives at its destination.</dd><br/>
              <dt><b>Cycle time</b></dt>
              <dd>is the minimum time between requests to memory.</dd><br/>
              <dt><b>Cache hit</b></dt>
              <dd>is the situation when the requested data is in the cache.</dd><br/>
              <dt><b>Cache miss</b></dt>
              <dd>is the situation when the requested data isn't in the cache. Upon a cache miss, a cache line is retrieved
               from main memory (or a higher level of cache) and placed in the cache.</dd>
            </dl>
          </section>

          <section>
            <h2>Cache terminology</h2>
            <dl style="text-align: justify;">
              <dt><b>Cache miss rate</b></dt>
              <dd>is the fraction of cache accesses that result in a miss.</dd>
              <dt><b>Local miss rate</b></dt>
              <dd>is the number of misses in a cache divided by the total number of memory access to this cache</dd>
              <dt><b>Global miss rate</b></dt>
              <dd>is the number of misses in a cache divided by the total number of memory accesses generated</dd>
              <br/>
              <dt><b>Cache miss penalty</b></dt>
              <dd>is the time required for serving a cache miss<br/>(depends on the latency and bandwidth of the memory).</dd><br/>
              <dt><b>Memory stall cycles</b></dt>
              <dd>is the cycles during which a CPU is stalled waiting for a memory access.</dd>
            </dl>
          </section>
        </section>

<!--         Using multiple levels of cache allows a small fast cache to keep pace with the
CPU, while slower larger caches can be used to reduce the miss penalty since
the next level cache can be used to capture many accesses that would go to
main memory.
The local miss rate is large for higher level caches because the first level
cache benefits the most from data locality.  Thus a global miss rate that
indicates what fraction of the memory access that leave the CPU go all the way
to memory is a more useful measure. -->

        <section>
          <h2>Cache properties </h2>
          <ul>
            <li>Modern processors have from 1 to 4 levels of caches
            <ul>
              <li>Inclusive caches duplicates the data from higher level cache in lower levels</li>
              <li>Exclusive caches contains data only in one level of cache</li>
              <li>Most caches are <b>inclusive</b>.</li>
            </ul>
            </li>
            <li>Data transfer unit between registers and L1 cache is usually in between 1-16 bytes</li>
            <li><b>Cache line</b> is the smallest unit of memory that can be transferred to and form main memory
            (typically 32, 64 bytes, sometimes 128 bytes and more)</li>
            <li>Line is also unit of storage allocation in cache</li>
          </ul>
        </section>

        <section>
          <h2>Associativity</h2>
          <ul>
            <li><b>Cache associativity</b> is the number of unique places in a cache where any given memory item can reside.</li>
            <li>Each line has a <b>tag</b> that indicates the address in memory from which the line has been copied.</li>
            <li>Cache hit is detected through an <b>associative search</b> of all the tags.</li>
            <li>Consequently, there is a size/hit latency tread off, large caches can keep mere data,
            but need more time for tag search.</li>
            <li>In an <b>n way associative</b> cache, any cache line from memory can map to any of the
            n locations in a set. 2-, 4-, 8- and 16-way set associative caches are often used in modern processors.</li>
            <li>1 way set associative cache is called <b>direct mapped</b>. Direct mapped means only one location,
            and so high probability of conflict, but tag search is very fast.</li>
            <li>A fully associative cache is one in which a cache line can be placed anywhere in cache.</li>
            <li>Higher associativity is better, but costly in terms of gates and complexity (power and performance).
            Full association is feasible for very small size caches only (TLB's)</li>
          </ul>
          <p>
        </section>

        <section>
          <section>
            <h2>Direct Mapped</h2>
            <img class="simple" width="50%" src="images/popt/dmc.svg"/>
          </section>
          <section>
            <h2>Set-associative</h2>
            <img class="simple" width="50%" src="images/popt/sac.svg"/>
          </section>
          <section>
            <h2>Fully-associative</h2>
            <img class="simple" width="50%" src="images/popt/fac.svg"/>
          </section>
        </section>

        <section>
          <h2>Cache misses taxonomy</h2>
          <dl style="text-align: justify;">
            <dt><b>Compulsory</b></dt>
            <dd>refers to misses for the very first access to a cache line.</dd><br/>
            <dt><b>Capacity</b></dt>
            <dd>is the situation than the cache cannot contain all the cache lines needed during execution of a program.</dd><br/>
            <dt><b>Conflict</b></dt>
            <dd>occurs when a block must be discarded and later retrieved because too many cache lines mapped to the
            same set. It is only applicable to a (less than fully) set associative or direct mapped cache.</dd><br/>
            <dt><b>Coherency</b></dt>
            <dd></dd>
          </dl>
        </section>

        <section>
          <h2>Replacement policy &amp; Write strategy</h2>
          <p>Replacement policy</p>
          <ul>
            <li>Random</li>
            <li>Round robin</li>
            <li>Least Recently Used (LRU): performs better than random or round robin but is more difficult to implement</li>
            <li>Pseudo-LRU</li>
          </ul>
          <p>Write Strategy is strategy to maintain coherence between cache and memory</p>
          <ul>
            <li>Write through: The data are written to both the cache line in the cache and to main memory.</li>
            <li>Write back: The data are written only to the cache line in the cache.  The modified cache line
            is written to memory only when necessary (e.g., when it is replaced by another cache line.</li>
          </ul>
        </section>

        <section>
          <h2>Nonblocking Caches</h2>
          Pipelined computers that allow out-of-order execution can continue fetching instructions from the instruction
          cache while waiting on a data cache miss. A nonblocking cache design allows the data cache to continue to
          supply cache hits during a miss, called <b>hit under miss</b>, or <b>hit under multiple miss</b> if multiple
          misses can be overlapped.  Hit under miss significantly increases the complexity of the cache controller,
          with the complexity increasing as the number of outstanding misses allowed increases. Out-of-order processors
          with hit under miss are generally capable of hiding the  miss penalty of an L1 data cache miss that hits
          in the L2 cache, but are not capable of hiding a significant portion of the L2 miss penalty.
        </section>

        <section>
          <h2>Coherency</h2>
          Cache Contention on SMPs
          › When two or more CPUs alternately and repeatedly update the same cache line
          › memory contention
          •  when two or more CPUs update the same variable
          • correcting it involves an algorithm change
          › false sharing
          • when CPUs update distinct variables that occupy the same cache line
          • correcting it involves modification of data structure layout
        </section>

        <section>
          <h2>Hardware cache optimizations</h2>
          <ol>
            <li>Larger block size to reduce miss rate</li>
            <li>Bigger caches to reduce miss rate</li>
            <li>Higher associativity to reduce miss rate</li>
            <li>Multilevel caches to reduce miss penalty</li>
            <li>Giving priority to read misses over writes to reduce miss penalty</li>
            <li>Avoiding address translation during indexing of the cache to reduce hit time</li>
          </ol>
        </section>

        <section>
          <h2>Optimizer's point of view</h2>
          <ul>
            <li>Ensure data locality</li>
            <li>Generally, tile for the largest non-shared cache. Depending on the speed difference and amount of
            work per iteration, L1 may be better. Never block for the shared cache size.</li>
            <li>Do not access the same locations memory from different processes very often</li>
            <li>Avoid associativity conflicts while fetching from different memory streams</li>
            <li></li>
          </ul>
        </section>

        <!--  -->

        <section>
          <section>
            <h2>Pipelined execution - Classic 5-stage pipeline</h2>
            <p>The idea is to break down an instruction into multiple stages so that instructions
            on different stages can be processed at the same clock cycle. Introduced in 1960s.</p>
            <dl style="text-align: justify;">
              <dt><b>Instruction fetch cycle (IF)</b></dt>
              <dd>An instructions is fetched from IC and may be placed either into an instruction register
              or a queue of pending instructions.</dd>
              <dt><b>Instruction decode/register fetch cycle (ID)</b></dt>
              <dd>An instruction is decoded, register operands are read from the register file.</dd>
              <dt><b>Execution/effective address cycle (EX)</b></dt>
              <dd>Either an ALU operation is performed or an effective address for memory access is computed,
              which depends on an instruction type</dd>
              <dt><b>Memory access/branch completion cycle (MEM)</b></dt>
              <dd>The memory is accessed, if appropriate.</dd>
              <dt><b>Write­back cycle (WB)</b></dt>
              <dd>The result is written into the register file, whether it comes from the memory system (for a load)
              or from the ALU (for an ALU instruction)</li>
            </dl>
          </section>

          <section>
            <h2>Without pipelined execution</h2>
            <img class="simple" width="80%" src="images/popt/nopipeline.svg">
          </section>

          <section>
            <h2>Pipelined execution</h2>
            <img class="simple" width="80%" src="images/popt/pipeline.svg">
          </section>

          <section>
            <h2>Pipeline hazards</h2>
            <p><b>Pipeline hazard</b> is a situation that prevents the next instruction in the pipeline from executing
            during its designated clock cycle and thus cause pipeline stalls.</p>
            <dl style="text-align: justify;">
              <dt><b>Structural hazard</b></dt>
              <dd>arise from resource conflicts when the hardware cannot support all possible combinations of instructions
              simultaneously in overlapped execution.</dd>
              <dt><b>Data hazard</b></dt>
              <dd>arise when an instruction depends on the results of a previous instruction in a way that is exposed
              by the overlapping of instructions in the pipeline.</dd>
              <dt><b>Control hazard</b></dt>
              <dd>arise from the pipelining of branches and other instructions that change the PC.</dd>
            </dl>
            <br/><small>All the terms used are taken from Hennessey & Paterson</small>
          </section>

          <section>
            <h2>multiple-issue superscalars: Cortex-A53</h2>
            Instructions are dispatched 2 per cycle to the appropriate issue queue.<br/>
            Issue Width &mdash; 2 &mu;ops, Pipeline Length &mdash; 8 stages, Pipeline Length &mdash; 8 stages,
            Number of pipes &mdash; 5: I0, I1, FP, B, LD/DT.
            <div><img width="50%" src="images/popt/A53-Diagram.png"></div>
            <small>image source is <a href="http://images.anandtech.com/doci/8718/A53-Diagram.png">AnandTech.com</a></small>
          </section>
        </section>


        <section>
          <h2>Speculative execution</h2>
        </section>

        <section>
          <h2>Out-of-order execution</h2>
        </section>

        <section>
          <h2>Prefetch streams and write stream</h2>
        </section>


        <section>
          <h2>SIMD vs Vector co-processors</h2>
        </section>

        <section>
          <h2>Summary</h2>
        </section>

        <section id="end1">
          <h1>THE END</h1>
          <img class="simple" src="images/popt/infinity.png">
          <h4><a href="https://github.com/cuda-geek">Marina Kolpakova</a> / 2015</h4s>
        </section>

<!--         <section>
At least 3 major approaches:
Make cores as fast/slow as main memory (SiCortex, Tensilica)
Add faster/closer memory pipes (Opteron, Nehalem)
Streaming compute engines (NVIDIA,AMD), vectorized memory pipelines (Convey).

With multiple cores, more than one level can keep MESI states.

Hardware Prefetching
When you see linear scaling graphs, be (somewhat) suspicious.
Linear scalability is easy(ier) when per-core performance is low!
The faster a single core computes, the more vulnerable it is to other bottlenecks.
So producing a linear graph, does not make your program efficient.
        </section> -->

      </div>

    </div>

    <script src="plugin/reveal/lib/js/head.min.js"></script>
    <script src="plugin/reveal/js/reveal.js"></script>

    <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        slideNumber: true,
        history: true,
        center: false,

        width: 960,
        height: 720,

        transition: 'slide', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'plugin/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true }
        ]
      });

    </script>

  </body>
</html>
