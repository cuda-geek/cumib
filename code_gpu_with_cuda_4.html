<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Code GPU with CUDA</title>

    <meta name="description" content="CUDA cource: Code GPU with CUDA">
    <meta name="author" content="cuda.geek">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="3dparty/reveal/css/reveal.css">
    <link rel="stylesheet" href="stylesheets/cuda.geek.css" id="theme">
    <link rel="stylesheet" href="stylesheets/cuda.css">
    <link rel="stylesheet" href="3dparty/reveal/lib/css/zenburn.css">
    <script>
      document.write( '<link rel="stylesheet" href="3dparty/reveal/css/print/' + ( window.location
        .search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>
    <!--[if lt IE 9]>
      <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Code GPU with CUDA</h1>
          <h2>Common optimization techniques</h2>
          <h3>Memory &amp; control flow optimization</h3>
          <br>
          <br>
          <br>
          <small>Created by Marina Kolpakova (
            <a href="http://github.com/cuda-geek">cuda.geek</a>
            ) for
            <a href="http://itseez.com">Itseez</a>
          </small>
          <h4><a href="common_optimization_techniques_device_code.html#/end1">previous</a></h4>
        </section>

        <section>
          <section>
            <h2>Outline</h2>
            <ul>
              <li><a href="#/sec_0">Introduction to CUDA</a></li>
              <li><a href="#/sec_1">Device code optimization</a>
                <ul>
                  <li><a href="#/sec_0_0">Improve parallelism</a></li>
                  <li><a href="#/sec_0_1">Improve memory accesses</a></li>
                  <li><a href="#/sec_0_2">Improve control flow</a></li>
                </ul>
              </li>
              <li><a href="#/sec_2">General principles</a></li>
            </ul>
          </section>
          <section>
            <h2>Out of scope</h2>
            <ul>
              <li>CUDA API</li>
              <li>Dynamic parallelism (sm_35)</li>
              <li>Surfaces and layered textures</li>
              <li>OpenGL interoperability</li>
            </ul>
          </section>
        </section>

        <section id="sec_0_1">
          <h1>Memory<br/>Optimization</h1>
        </section>

        <section>
          <h2>Memory Types</h2>
          <table class="tbl1">
            <colgroup>
              <col></col>
              <col></col>
              <col></col>
              <col></col>
              <col></col>
              <col></col>
            </colgroup>
            <thead>
              <tr>
                <th>Memory</th>
                <th>Scope</th>
                <th>Location</th>
                <th>Cached</th>
                <th>Access</th>
                <th>Lifetime</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <th>Register</th><td>Thread</td><td>On-chip</td><td>N/A</td><td>R/W</td><td>Thread</td>
              </tr>
              <tr>
                <th>Local</th><td>Thread</td><td>Off-chip</td><td>L1/L2</td><td>R/W</td><td>Thread</td>
              </tr>
              <tr>
                <th>Shared</th><td>Block</td><td>On-chip</td><td>N/A</td><td>R/W</td><td>Block</td>
              </tr>
              <tr>
                <th>Global</th><td>Grid + Host</td><td>Off-chip</td><td>L2</td><td>R/W</td><td>App</td>
              </tr>
              <tr>
                <th>Constant</th><td>Grid + Host</td><td>Off-chip</td><td>L1,L2,L3</td><td>R</td><td>App</td>
              </tr>
              <tr>
                <th>Texture</th><td>Grid + Host</td><td>Off-chip</td><td>L1,L2</td><td>R</td><td>App</td>
              </tr>
            </tbody>
          </table>
        </section>

        <section>
          <h2>Memory types (Cont.)</h2>
          <section>
            <img src="images/c1/gk107b.jpg" width="58%">
          </section>
          <section>
            <img src="images/c2/smx_sm.png" width="55%">
          </section>
        </section>

        <section>
          <h2>GPU caches</h2>
            <b>GPU caches are not intended for the same use as CPU's</b>
            <ul>
              <li><b>Not aimed at temporal reuse</b>. Smaller than CPU size (especially per thread, e.g.
              Fermi: 48&thinsp;KB L1, 1536 threads on fly, cache / thread = 1 x 128-byte line).</li>
              <li><b>Aimed at spatial reuse</b>. Intended to smooth some access patterns, help with spilled registers and stack.</li>
              <li><b>Do not tile relying on block size</b>. Lines likely become evicted next few access
                <ul>
                  <li>Use smem for tiling. Same latency, fully programmable</li>
                </ul>
              </li>
              <li><b>L2 aimed to speed up atomics and gmem writes.</b></li>
            </ul>
        </section>

        <section>
          <h2>GMEM</h2>
          <p>Learn your access pattern before thinking about latency hiding and <b>try not to thresh the memory bus</b>.</p>
          <p>Four general categories of inefficient memory access patterns:</p>
          <ul>
            <li><b>Miss-aligned</b> (offset) warp addresses</li>
            <li><b>Strided</b> access between threads within a warp</li>
            <li><b>Thread-affine</b> (each thread in a warp accesses a large contiguous region)</li>
            <li><b>Irregular</b> (scattered) addresses</li>
          </ul>
          <p>Always be aware about bytes you actually need and bytes you transfer through the bus</p>
        </section>

        <section>
          <h2>GMEM: Miss-aligned</h2>
          <img src="images/c1/unaligned.svg" class="simple">
          <ul>
            <li>Add extra padding for data to force alignment</li>
            <li>Use read-only texture L1</li>
            <li>Combination of above</li>
          </ul>
          <!-- <img src="img/p11.png"> -->
        </section>

        <section>
          <h2>GMEM: Strided</h2>
          <img src="images/c2/stridden.svg" class="simple">
          <section>
            <ul>
              <li>If pattern is regular, try to change data layout: <abbr title="Array of structures">AoS</abbr> -> <abbr title="Structure of arrays">SoA</abbr></li>
            </ul>
            <img src="images/c2/soa.svg" class="simple">
          </section>
          <section>
            <ul>
              <li>Use smem to correct access pattern.
                <ol>
                  <li>load gmem -> smem with best coalescing</li>
                  <li>synchronize</li>
                  <li>use</li>
                </ol>
              </li>
            </ul>
          </section>
          <section>
            <ul>
              <li>Use warp shuffle to permute elements for warp
                <ol>
                  <li>coalescingly load elements needed by warp</li>
                  <li>permute</li>
                  <li>use</li>
                </ol>
              </li>
              <img src="images/c2/shuffle.png" style="align:center;">
              </li>
            </ul>
          </section>
          <section>
            <ul>
              <li>Use proper caching strategy</li>
              <ul>
                <li><b>cg</b> &ndash; cache global</li>
                <li><b>ldg</b> &ndash; cache in texture L1</li>
                <li><b>cs</b> &ndash; cache streaming</li>
              </ul>
            </ul>
          </section>
        </section>

        <section>
          <h2>GMEM: Thread-affine</h2>
          <section>
            <p>Each thread accesses relatively long continuous memory region</p>
            <img src="images/c2/th_affine.svg" class="simple">
            <ul>
              <li>Load big structures using AoS</li>
              <li>Thread loads continuous region of data</li>
              <li>All threads load the same data</li>
            </ul>
          </section>
          <section>
            <p>Work distribution</p>
            <pre><code class="cpp"> int tid = blockIdx.x * blockDim.x + threadIdx.x;</code></pre>
            <pre><code class="cpp"> int threadN = N / blockDim.x * gridDim.x;
  for (size_t i = tid * N; i < (tid + 1) * N; ++i )
  {
    sum =+ in[i]
  }</code></pre>
            <pre><code class="cpp"> for (size_t i = tid; i < N; i += blockDim.x * gridDim.x )
  {
    sum =+ in[i]
  }</code></pre>
          </section>
          <section>
            <h3>Uniform load</h3>
            <p>All threads in a block access the same address as read only.</p>
            <p>Memory operation uses 3-level constant cache</p>
            <ul>
              <li>Generated by compiler</li>
              <li>Available as PTX asm insertion</li>
            </ul>
            <pre><code class="cpp"><span class="keyword">__device__ __forceinline__</span> float __ldu(const float* ptr)
{
  float val;
  asm ("ldu.global.f32 %0, [%1];" : "="f(val) : l(ptr));
  return val;
}</code></pre>
          </section>
        </section>

        <section>
          <h2>GMEM: Irregular</h2>
          <p>Random memory access. Threads in a warp access many lines, strides are irregular.</p>
          <!-- <img src="img/p3.png" class="simple"> -->
          <ul>
            <li>Improve data locality</li>
            <li>Try 2D-local arrays (Morton-ordered)</li>
            <li>Use read-only texture L1</li>
            <li>Kernel fission to localize the worst case.</li>
          </ul>
        </section>

        <section>
          <h2>Texture</h2>
          <ul>
              <li>Smaller transactions and different caching (dedicated L1, 48&thinsp;KB, ~104 clock latency)</li>
              <li>Cache is not polluted by other GMEM loads, separate partition for each warp scheduler helps to prevent cache threshing</li>
              <li>Possible hardware interpolation (Note: 9-bit alpha)</li>
              <li>Hardware handling of out-of-bound access</li>
          </ul>
          <p>Kepler improvements:</p>
          <ul>
            <li>sm_30+ Bindless textures. No global static variables. Can be used in threaded code</li>
            <li>sm_32+ GMEM access through texture cache bypassing interpolation units</li>
          </ul>
        </section>

        <section>
          <h2>SMEM: Banking</h2>
          <!-- <section> -->
            <!-- <h4>No bank conflict</h4> -->
            <!-- <img src="img/smem-no-bk.png"> -->
          <!-- </section> -->
          <!-- <section> -->
            <!-- <h4>4- and 8-way bank conflicts</h4> -->
            <!-- <img src="img/smem-bk.png"> -->
          <!-- </section> -->
          <section>
            <h3>Kepler: 32-bit and 64-bit modes</h3>
            <img src="images/c1/banks.svg">
            <p>special case: 2D smem usage (Fermi example)</p>
            <pre><code class="cpp"><span class="keyword"> __shared__</span> float smem_buffer [32][32 + 1] </code></pre>
          </section>
          <!-- <p>note about throughput and volatile</p> -->
        </section>

        <section>
          <h2>SMEM</h2>
          <p><b>The common techniques are:</b></p>
          <ul>
            <li>use smem to improve memory access pattern</li>
            <li>use smem for stencil processing</li>
          </ul>
          <p><b>But the gap between smem and math throughput is increasing</b></p>
          <ul>
            <li>Tesla: 16 (32 bit) banks vs 8 thread processors (2:1)</li>
            <li>GF100: 32 (32 bit) banks vs 32 thread processors (1:1)</li>
            <li>GF104: 32 (32 bit) banks vs 48 thread processors (2:3)</li>
            <li>Kepler: 32 (64 bit) banks vs 192 thread processors (1:3)</li>
          </ul>
          <p>Max size 48&thinsp;KB (49152&thinsp;B), assume max occupancy 64x32,<br/>so <b>24</b> bytes per thread.<br/>
          More intensive memory usage affects occupancy.</p>
        </section>

        <section>
          <h2>SMEM (Cont.)</h2>
          <p>smem + L1 use the same 64K&thinsp;B. Program-configurable split:</p>
          <ul>
            <li>Fermi: 48:16, 16:48</li>
            <li>Kepler: 48:16, 16:48, 32:32</li>
          </ul>
          <p><b>cudaDeviceSetCacheConfig(), cudaFuncSetCacheConfig()</b></p>
          <ul>
            <li>prefer L1 to improve lmem usage</li>
            <li>prefer smem for stencil kernels</li>
          </ul>
          <p>smen often used for:</p>
          <ul>
            <li>data sharing across the block</li>
            <li>inter-block communication</li>
            <li>bock-level buffers (for scan or reduction)</li>
            <li>stencil code</li>
          </ul>
        </section>


        <section>
          <h2>LMEM</h2>
          <p>Local memory is a stack memory analogue: call stack, register spilling.
          Note: Both Local memory reads/writes are cached in L1.</p>
          <ul style="none">
            <li>Registers are for automatic variables
            <pre><code class="cpp"> int a = 42;</code></pre>
            </li>
            <li>Volatile keyword enforces spilling</li>
            <li>Registers do not support indexing: local memory is used for local arrays
              <pre><code class="cpp"> int b[ SIZE ] = {0,};</code></pre>
            </li>
            <li>Register spilling leads to more instructions and memory traffic</li>
          </ul>
        </section>
        <section>
          <h2>spilling control</h2>
            <ol>
              <li>Use <b>__launch_bounds__</b> to help compiler to select maximum amount of registers.
                <pre width="100%"><code class="cpp"><span class="keyword">__global__</span> void <span class="keyword">__launch_bounds__</span>(
maxThreadsPerBlock, minBlocksPerMultiprocessor) kernel(...)
{
  //...
}</code></pre>
              </li>
              <li>Compile with <b>-maxrregcount</b> to enforce compiler optimization for register usage and register spilling if needed</li>
              <li>By default you run less concurrent warps per SM</li>
            </ol>
        </section>

        <section id="sec_1_2">
          <h1>Control flow</h1>
        </section>

        <section>
          <h2>Control flow</h2>
          <section>
            <h3>Problems</h3>
            <ul>
              <li><b>Warp divergence</b>: branching, early loop exit... Inspect SASS to find divergent pieces of code</li>
              <li><b>Workload is data dependent</b>: code-path depends on input (like classification task)</li>
              <li><b>Too many synchronization logic</b>: intensive usage of parallel data structures, lots of atomics,
              __sychthreads(), etc</li>
              <li><b>Resident warps</b>: occupy resources but do nothing</li>
              <li><b>Big blocks</b>: tail effect</li>
            </ul>
          </section>
          <section>
            <h3>Solutions</h3>
            <ul>
              <li>Understand your problem. Select best algorithm keeping in mind GPU architecture. Maximize independent parallelism</li>
              <li>Compiler generates branch predication with -O3 during if/switch optimization but number of instructions
              has to be less or equal than a given threshold. Threshold = 7 if lots of divergent warps, 4 otherwise</li>
              <li>Adjust thread block size</li>
              <li>Try work queues</li>
            </ul>
          </section>
          <section>
            <h3>Kernel Fusion and Fission</h3>
            <ul class="none">
              <li><b>Fusion</b>
                <ul>
                  <li>Replace chain of kernel calls with fused one</li>
                  <li>Helps to save memory reads/writes. Intermediate results can be kept in registers</li>
                  <li>Enables further ILP optimizations</li>
                  <li>Kernels should have almost the same access pattern</li>
                </ul>
              </li>
              <li><b>Fission</b>
                <ul>
                  <li>Replace one kernel call with a chain</li>
                  <li>Helps to localize ineffective memory access patterns</li>
                  <li>Insert small kernels that repack data (e.g. integral image)</li>
                </ul>
              </li>
            </ul>
          </section>
        </section>

        <section id="sec_2">
          <h2>it is always advised</h2>
          <section>
            <ul>
              <li>Basic CUDA Code Optimizations
                <ul>
                  <li>use compiler flags</li>
                  <li>do not trick compiler</li>

                  <li>use structure of arrays</li>
                  <li>improve memory layout</li>
                  <li>load by cache line</li>
                  <li>process by row</li>
                  <li>cache data in registers</li>

                  <li>re-compute values instead of re-loading</li>
                  <li>keep data on GPU</li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <ul>
              <li>Conventional parallelization optimizations
              <ul>
                <li>use light-weight locking,</li>
                <li>... atomics,</li>
                <li>... and lock-free code.</li>
                <li>minimize locking,</li>
                <li>... memory fences,</li>
                <li>... and volatile accesses.</li>
              </ul>
              </li>
            </ul>
          </section>
          <section>
            <ul>
              <li>Conventional architectural optimizations
                <ul>
                  <li>utilize shared memory,</li>
                  <li>... constant memory,</li>
                  <li>... streams,</li>
                  <li>... thread voting,</li>
                  <li>... and rsqrtf;</li>
                  <li>detect compute capability and number of SMs;</li>
                  <li>tune thread count,</li>
                  <li>... blocks per SM,</li>
                  <li>... launch bounds,</li>
                  <li>and L1 cache/shared memory configuration</li>
                </ul>
              </li>
            </ul>
          </section>
        </section>

        <section>
          <h1>THE END</h1>
          <h3>BY <a href="https://github.com/cuda-geek">cuda.geek</a> / 2013 &ndash; 2015</h3>
          <h4><a href="cuda3.html">next</a></h4>
        </section>

      </div>
    </div>
    <script src="3dparty/reveal/lib/js/head.min.js"></script>
    <script src="3dparty/reveal/js/reveal.min.js"></script>
    <script>
            Reveal.initialize({
                controls: true,
                progress: true,
                history: true,
                center: false,
                rollingLinks: false,

                theme: Reveal.getQueryHash().theme,
                transition: Reveal.getQueryHash().transition || 'concave', // default/cube/page/concave/zoom/linear/fade/none
                dependencies: [
                    { src: '3dparty/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: '3dparty/reveal/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: '3dparty/reveal/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: '3dparty/reveal/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: '3dparty/reveal/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: '3dparty/reveal/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
                    // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
                    // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
                ]
            });
    </script>
  </body>
</html>
