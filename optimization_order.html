<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>cumib by cuda-geek</title>
    <meta name="description" content="cuda.geek's personal blog about architectures and optimization them.">
    <meta name="author" content="Marina Kolpakova (cuda.geek)">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="stylesheets/normalize.css">
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="3dparty/reveal/lib/css/zenburn.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.3/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="3dparty/reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>
  <body>
    <div id="header">
      <div id="title">
        <h1>cumib</h1>
        <p>The CUDA microbenchmarks</p>
      </div>
    </div>

    <div class="wrapper">
      <section>
        <h1>Order matters</h1>
        <p>Well, we've decided to be pragmatic. <b>What to begin optimization with?</b> I know, all of us like to dive
        into assembly and do all these geeky things. Nevertheless, for sure there are so many things to do before it.</p>
        <p>I want share the steps I walk through while doing optimization, regardless a problem size.
        No matter if it is a full pipeline or just a concrete computational kernel, you can use proposed approach.
        This steps are quite scalable.</p>

        <h2 style="color: #6F3;">1. Understand the code</h2>
        <p>Before starting optimization you should understand the code. What is an input? What is an output?
        What operation/operations are performed? The most part of the "magic" in programming stars with phrase: "Hm...
        This code works... <b>somehow</b>." Keep calm and understand it. Optimization of the code which is unknown
        is like walking through a minefield. What I mean is, changes in the code look very dangerous, while the results
        of optimizations seem unpredictable. </p>

        <h2 style="color: #6F3;">2. Use appropriate algorithm</h2>
        <p>After you get enough knowledge there is a time to think about the algorithm itself. Is it an optimal
        solution for the problem that your code should solve? Check, whether more optimal algorithms with lower
        computational complexity exist.</p>
        <p>The classical example here is sorting. There are so many ways to sort things like quick sort
        or histogram based approaches such as radix sort. All of them differ in number of operations per data element.
        Depending on the nature of an algorithm and data as well as data distribution, your algorithmic choose will vary.
        </p>
        <p>Choosing algorithm you should consider many trade-offs. It is noteworthy, one of the most important factors
        to think of is optimization opportunities. In particular, if there are more than one core is available you
        should consider parallelization possibility. Furthermore, if SIMD optimization is planed, prefer to use
        algorithms whose critical loops can be explained in scalarizable code.</p>
        <!-- <p>add example here, probably detector</p> -->

        <h2 style="color: #6F3;">3. Optimize memory access patterns</h2>
        <p>It is everything about memory! Interesting enough, almost all algorithms are memory bound. If it comes to
        function which is more complicated than streaming kernel we may found that access pattern is not ideal. Whereas
        other algorithms just need a huge amount of data to process.</p>
        <p>I practice multiple optimization technologies and one of them is CUDA (which is used to optimize general
        purpose codes with GPUs). GPUs have very interesting architecture. They are absolutely unbalanced in terms of
        memory latency relative to arithmetic latency. You know what, I cannot stand when people provide technical
        reports and parers which say phrase similar to this one "almost theoretical throughput is achieved" with
        no single world about how many bytes from this amount of data they really need. Such statement
        about throughput says to me only that application consumes power on byte walking through the bus.</p>
        <p>You should beware your memory patterns. It is crucial to load only data that is really needed for
        computations. Usually it means some data restructuring, loop reorganization and so on.</p>
        <!-- <p>add example from NEON here</p> -->

        <h2 style="color: #6F3;">4. Minimize the number of operations</h2>
        <p>As a matter of fact, less number of operations require less time to process. However, This is not a rule
        of thumb for the most of the modern architectures because typically they are deeply pipelined. Hence multiple
        operations may be executed on different pipes. Moreover, depth varies from pipe to pipe, so different operations
        have different execution latency. As the result, it comes more about critical path optimization rather than just
        code reduction. But still the number of operations is not bad approximation of how well your optimization is.</p>
        <p>It is quite frequent that on this step you need to figure out problems that comes from some compiler limitations.
        For instance, if compiler failed with register allocation you'll get extra operations with stack or register
        movements. It is clear what these operations are the fist candidates for removal since it don't required
        by the algorithm. It usually mean that you need to "rephrase" the ideas explained in your code.</p>
        <p>For instance, there are two types of vector registers in NEON (which SIMD extension to ARM processors):
        64-bit D registers and 128-bit Q registers. Many, but not all, operations can be performed on both types of registers.
        For example, you can add two D registers with instruction <code class="asm"> vadd.u8 d0, d1, d2</code> as well
        as two Q registers <code class="asm"> vadd.u8 q0, q1, q2</code> with the same one. All the instructions are
        wrapped by c/c++ intrinsics so that operations on long and short vectors are distinguished by postfix
        <code>q</code> like <code >add</code> and <code >addq</code>. As for intrinsics, the set is complete. Thus
        there are intrinsics that doesn't corresponds to single assembly instruction, such as <code >vst3q</code>
        instruction for ARMv7 ISA. This instruction performs 3-way register interleaving before storing the result.
        This is extremely useful for decoding into RGB images. Presently I've done NEON optimization for packaged Yuv
        to RGB color space with use of this intrinsic. Here is an assembly snippet generated
        with use of <code>vst3q</code> followed by a snippet generated with use of 2 <code>vst3q</code> intrinsics.</p>
        <p>As you can see in both cases two D-variants of store instructions are generated, but in the first case
        compiler some struggled with additional dependencies between registers.</p>
        <p>Over and above, reduction in the number of operations is one of reasons wherefore loop unrolling benefits on
        modern out-of-order architectures.</p>
        <!-- <p>tell about the principal of SIMD execution </p> -->
        <!-- <p>Add NEON color conversion example here</p> -->

        <h2>5. Shrink critical path</h2>
        <p>This step is connected to precious one. What is important is that...</p>

        <h2 style="color: #6F3;">6. Perform hardware specific optimizations</h2>
        <p>This is not about usage of <code class="cpp" style="background-color:#3F3F3F;padding:0;"> x &gt;&gt; 1</code>
        instead of <code class="cpp" style="background-color:#3F3F3F;padding:0;">x / 1</code>.
        For sure, compiler can figure this out. I am talking about optimizations that require deep understanding of the
        hardware.</p>
        <p>One example here is again from CUDA. GPUs support atomic operations in hardware. They are performed in L2
        cache (that is shared between separate units) as well as normal writes. One atomic operation per cycle is
        scheduled for each unit (I do not go in CUDA terminology to make this example understandable to a wide audience).
        Performance of atomics is data-dependent. I mean, everything is fast while different cache lines
        are accessed. To cut long story short, atomics are more optimal for operations like
        <pre><code class="cpp">
    ptr[index] += value;</code></pre></p>
        <p>The reason is simple. This line of code contains three operations: load, addition and store. Each of them
        depends on previous. Then we issue atomic we need data only in L2 not in registers. So path is shorter.
        This trick added me 15% improvement relative to code in the listing for streaming kernel on Kepler GPU.</p>

        <h2 style="color: #6F3;">6. Dive into assembly</h2>
        <p>Yep, you've done everything you can to accelerate your code. Before enter into a contract with the devil
        let's dive into assembly. As for me, I tend to use disasm during steps from 3 to 5. As for writing, the most
        common scenarios there it is affordable are overcoming compiler bugs or usage of instructions which
        are not present in higher level ISA.</p>
        <p>For example, in NEON there is a 256-bit load instruction that is present only in raw assembly.
        <code class="asm">vld1 {d0, d1, d2, d3}</code>. Usage of this instruction will not accelerate your code twice
        (latency of this instruction higher as well), but it could help. Consecutively, if it helps, I use assembly.
        I newer seen compiler generated this. Even for codes like the following.
        <pre><code class="cpp">
    uint8_t* ptr = /**/;
    uint8x16_t val0 = vld1q(ptr+   0);
    uint8x16_t val1 = vld1q(ptr + 16);</code></pre>It is clear that likelihood that pointers alias is equal to
        zero but compiler do not merge this operations into one.</p>
        <!-- add information about compiler bug overcoming -->
        <p>Writing assembly you should always consider many tread-offs like code portability, readability, and so on. So,
        think twice.</p>
        <h2>Final words</h2>
        <p></p>
        <hr>
        <hr>
        <a class="credits left" href="optimization_rules.html">&lt;&lt; go to the previous page</a>
        <a class="credits right" href="optimization_order.html">go to the next page&gt;&gt;</a>
        <hr>
        <hr>
        <hr>
        <span class="credits left">Project maintained by <a href="https://github.com/cuda-geek">cuda-geek</a></span>
        <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
      </section>

    </div>
  </body>
</html>