<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">

    <title>Code GPU with CUDA</title>

    <meta name="description" content="CUDA cource: Code GPU with CUDA">
    <meta name="author" content="cuda.geek">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="3dparty/reveal/css/reveal.css">
    <link rel="stylesheet" href="stylesheets/cuda.geek.css" id="theme">
    <link rel="stylesheet" href="stylesheets/cuda.css">
    <link rel="stylesheet" href="3dparty/reveal/lib/css/zenburn.css">
    <script>
      document.write( '<link rel="stylesheet" href="3dparty/reveal/css/print/' + ( window.location
        .search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>
    <!--[if lt IE 9]>
      <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Code GPU with CUDA</h1>
          <h2>Identifying performance limiters</h2>
          <h3></h3>
          <br>
          <br>
          <br>
          <small>Created by Marina Kolpakova (
            <a href="http://github.com/cuda-geek">cuda.geek</a>
            ) for
            <a href="http://itseez.com">Itseez</a>
          </small>
          <h4><a href="code_gpu_with_cuda_5.html#/end1">previous</a></h4>
        </section>

        <section>
          <section>
            <h2>Outline</h2>
            <ul>
              <li>How to identify performance limiters?</li>
              <li>What and how to measure?</li>
              <li>Why to profile?</li>
              <li>Profiling case study: transpose</li>
              <li>Code paths analysis</li>
            </ul>
          </section>
          <section>
            <h2>Out of scope</h2>
            <ul>
              <li>Visual profiler opportunities</li>
            </ul>
          </section>
        </section>

        <section>
          <h2>How to identify performance limiters</h2>
          <section>
            <ul class="none">
              <li>
                <b>Time</b>
                <ul>
                  <li>Subsample when measuring performance</li>
                  <li>Determine your code wall time. You'll optimize it</li>
                </ul>
              </li>
              <li>
                <b>Profile</b>
                <ul>
                  <li>Collect metrics and events</li>
                  <li>Determine limiting factors (e.c. memory, divergence)</li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <ul class="none">
              <li>
                <b>Prototype</b>
                <ul>
                  <li>Prototype kernel parts separately and time them</li>
                  <li>Determine memory access or data dependency patterns</li>
                </ul>
              </li>
              <li>
                <b>(Micro)benchmark</b>
                <ul>
                  <li>Determine hardware characteristics</li>
                  <li>Tune for particular architecture, GPU class</li>
                </ul>
              </li>
              <li><b>Look into SASS</b>
                <ul>
                  <li>Check compiler optimizations</li>
                  <li>Look for a further improvements</li>
                </ul>
              </li>
            </ul>
          </section>
        </section>

        <section>
          <h2>Timing: What to measure?</h2>
          <ul>
            <li><b>Wall time</b>: user will see this time</li>
            <li><b>GPU time</b>: specific kernel time</li>
            <li><b>CPU &hArr; GPU memory transfers time</b>:
              <ul>
                <li>not considered for GPU time analysis</li>
                <li>significantly impact wall time</li>
              </ul>
            </li>
            <li>Data dependent cases timing:
              <ul>
                <li>worst case time</li>
                <li>time of single iteration</li>
                <li>consider probability</li>
              </ul>
            </li>
          </ul>
        </section>

        <section>
          <h2>How to measure?</h2>
          <section>
            <h3>system timer (Unix)</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp">#include &lt;time.h&gt;
double runKernel(const <span class="keyword">dim3</span> grid, const <span class="keyword">dim3</span> block)
{
    struct timespec startTime, endTime;
    clock_gettime(CLOCK_MONOTONIC, &startTime);
    kernel<span class="keyword">&lt;&lt;&lt;grid, block&gt;&gt;&gt;</span>();
    <b>cudaDeviceSynchronize();</b>
    clock_gettime(CLOCK_MONOTONIC, &endTime);
    int64 startNs = (int64)startTime.tv_sec * 1000000000 + startTime.tv_nsec;
    int64 endNs   = (int64)endTime.tv_sec   * 1000000000 + endTime.tv_nsec;

    return (endNs - startNs) / 10000000.; // get ms
}</code></pre>
            <p>Preferred for wall time measurement</p>
          </section>

          <section>
            <h3>Timing with CUDA events</h3>
            <pre style="width:100%;"><code style="padding: 10px;" class="cpp">double runKernel(const <span class="keyword">dim3</span> grid, const <span class="keyword">dim3</span> block)
{
    <span class="keyword">cudaEvent_t</span> start, stop;
    cudaEventCreate(&start); cudaEventCreate(&stop);
    cudaEventRecord(start, 0);
    kernel<span class="keyword">&lt;&lt;&lt;grid, block&gt;&gt;&gt;</span>();
    cudaEventRecord(stop, 0);
    <b>cudaEventSynchronize(stop);</b>
    float ms;
    cudaEventElapsedTime(&ms, start, stop);
    cudaEventDestroy(start); cudaEventDestroy(stop);
    return ms;
}</code></pre>
            <ul class="none">
              <li>Preferred for GPU time measurement</li>
              <li>Can be used with CUDA streams without synchronization</li>
            </ul>
          </section>
        </section>

        <section>
          <h2>Why to profile?</h2>
          <p><b>Profiler will not do your work for you</b>,<br/> but profiler helps:</p>
          <ul>
            <li>to verify memory access patterns</li>
            <li>to identify bottlenecks</li>
            <li>to collect statistic in data-dependent workloads</li>
            <li>to check your hypothesis</li>
            <li>to understand how hardware behaves</li>
          </ul>
          <blockquote cite="http://searchservervirtualization.techtarget.com/definition/Our-Favorite-Technology-Quotations" style=" margin-top:1em;">
            <p><b>Think about profiling and benchmarking<br> as about scientific experiments</b></p>
          </blockquote>
        </section>

        <section>
          <h2>Device code profiler</h2>
          <ul>
            <li><b>events</b> are hardware counters, usually reported per SM
              <ul>
                <li>SM id selected by profiler with assumption that all SMs do approximately the same amount of work</li>
                <li>Exceptions: L2 and DRAM counters</li>
              </ul>
            </li>
            <li><b>metrics</b> computed from number of events and hardware specific properties (e.c. number of SM)</li>
            <li>Single run can collect only a few counters
              <ul>
                <li>Profiler repeats kernel launches to collect all counters</li>
              </ul>
            </li>
            <li>Results may vary for repeated runs</li>
          </ul>
        </section>

        <section>
          <h2>Profiling for memory</h2>
          <section>
            <ul>
              <li>Memory metrics
                <ul>
                  <li>which have load or store in name counts from software perspective (in terms of memory requests)
                    <ul><li><code>local_store_transactions</code></li></ul>
                  </li>
                  <li>which have read or write in name counts from hardware perspective (in terms of bytes transfered)
                    <ul><li><code>l2_subp0_read_sector_misses</code></li></ul>
                  </li>
                </ul>
              </li>
              <li>Counters are incremented
                <ul>
                  <li>per warp</li>
                  <li>per cache line/transaction size</li>
                  <li>per request/instruction</li>
                </ul>
              </li>
            </ul>
          </section>
          <section>
            <ul>
              <li><b>Access pattern efficiency</b>
                <ul>
                  <li>check the ratio between bytes requested by the threads / application code and bytes moved by the hardware (L2/DRAM)</li>
                  <li>use <code><b>g{ld,st}_transactions_per_request</b></code> metric</li>
                </ul>
              </li>
              <li>Throughput analysis
                <ul>
                  <li>compare application HW throughput to possible for your GPU (can be found in documentation)</li>
                  <li><code><b>g{ld,st}_requested_throughput</b></code></li>
                </ul>
              </li>
            </ul>
          </section>
        </section>
        <section>
          <h2>instructions/bytes ratio</h2>
          <ul>
            <li>Profiler counters:
              <ul>
                <li><b><code>instructions_issued, instructions_executed</code></b></li>
                <li>incremented by warp, but “issued” includes replays</li>
              </ul>
              <ul>
                <li><b><code>global_store_transaction, uncached_global_load_transaction</code></b></li>
                <li>transaction can be 32,64,128 byte. Requires additional analysis to determine average.</li>
              </ul>
            </li>
            <li>Compute ratio:
              <ul><b>(warpSize X instructions_issued)</b> v.s. <b>(global_store_transaction + l1_global_load_miss) * avgTransactionSize</b></ul>
            </li>
          </ul>
        </section>

          <section>
            <h2>List of events for sm_35</h2>
            <section>
              <table class="tbl1" style="width:100%;">
                <colgroup>
                <col></col>
                <col></col>
              </colgroup>
              <thead>
                <tr>
                  <th>domain</th>
                  <th>event</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th rowspan="3">texture (a)</th><td>tex{0,1,2,3}_cache_sector_{queries,misses}</td>
                </tr>
                <tr><td>rocache_subp{0,1,2,3}_gld_warp_count_{32,64,128}b</td></tr>
                <tr><td>rocache_subp{0,1,2,3}_gld_thread_count_{32,64,128}b</td></tr>
                <tr>
                  <th rowspan="6">L2 (b)</th><td>fb_subp{0,1}_{read,write}_sectors</td>
                </tr>
                <tr><td>l2_subp{0,1,2,3}_total_{read,write}_sector_queries</td></tr>
                <tr><td>l2_subp{0,1,2,3}_{read,write}_{l1,system}_sector_queries</td></tr>
                <tr><td>l2_subp{0,1,2,3}_{read,write}_sector_misses</td></tr>
                <tr><td>l2_subp{0,1,2,3}_read_tex_sector_queries</td></tr>
                <tr><td>l2_subp{0,1,2,3}_read_{l1,tex}_hit_sectors</td></tr>

                <tr>
                  <th rowspan="2">LD/ST (c)</th><td>g{ld,st}_inst_{8,16,32,64,128}bit</td>
                </tr>
                <tr><td>rocache_gld_inst_{8,16,32,64,128}bit</td></tr>
              </tbody>
            </table>
          </section>
          <section>
            <table class="tbl1" style="width:100%;">
              <colgroup>
                <col></col>
                <col></col>
              </colgroup>
              <thead>
                <tr>
                  <th>domain</th>
                  <th>event</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <th rowspan="10">sm (d)</th><td>prof_trigger_0{0-7}</td>
                </tr>
                <tr><td>{shared,local}_{load,store}</td></tr>
                <tr><td>g{ld,st}_request</td></tr>
                <tr><td>{local,l1_shared,__l1_global}_{load,store}_transactions</td></tr>
                <tr><td>l1_local_{load,store}_{hit,miss}</td></tr>
                <tr><td>l1_global_load_{hit,miss}</td></tr>
                <tr><td>uncached_global_load_transaction</td></tr>
                <tr><td>global_store_transaction</td></tr>
                <tr><td>shared_{load,store}_replay</td></tr>
                <tr><td>global_{ld,st}_mem_divergence_replays</td></tr>
              </tbody>
            </table>
          </section>
          <section>
            <table class="tbl1" style="width:100%;">
              <colgroup>
              <col></col>
              <col></col>
            </colgroup>
            <thead>
              <tr>
                <th>domain</th>
                <th>event</th>
              </tr>
            </thead>
            <tbody>
              <tr><th rowspan="5">sm (d)</th><td>{threads,warps,sm_cta}_launched</td></tr>
              <tr><td>inst_issued{1,2}</td></tr>
              <tr><td>[thread_,not_predicated_off_thread_]inst_executed</td></tr>
              <tr><td>{atom,gred}_count</td></tr>
              <tr><td>active_{cycles,warps}</td></tr>
            </tbody>
          </table>
        </section>
      </section>

      <section>
        <h2>List of metrics for sm_35</h2>
        <section>
          <table class="tbl1" style="width:100%;">
            <colgroup>
              <col></col>
            </colgroup>
            <thead>
              <tr>
                <th>metric</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>g{ld,st}_requested_throughput</td></tr>
              <tr><td>tex_cache_{hit_rate,throughput}</td></tr>
              <tr><td>dram_{read,write}_throughput</td></tr>
              <tr><td>nc_gld_requested_throughput</td></tr>
              <tr><td>{local,shared}_{load,store}_throughput</td></tr>
              <tr><td>{l2,system}_{read,write}_throughput</td></tr>
              <tr><td>g{st,ld}_{throughput,efficiency}</td></tr>
              <tr><td>l2_{l1,texture}_read_{hit_rate,throughput}</td></tr>
              <tr><td>l1_cache_{global,local}_hit_rate</td></tr>
            </tbody>
          </table>
        </section>
        <section>
          <table class="tbl1" style="width:100%;">
            <colgroup>
              <col></col>
            </colgroup>
            <thead>
              <tr>
                <th>metric</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>{local,shared}_{load,store}_transactions[_per_request]</td></tr>
              <tr><td>gl{d,st}_transactions[_per_request]</td></tr>
              <tr><td>{sysmem,dram,l2}_{read,write}_transactions</td></tr>
              <tr><td>tex_cache_transactions</td></tr>
              <tr><td>{inst,shared,global,global_cache,local}_replay_overhead</td></tr>
              <tr><td>local_memory_overhead</td></tr>
              <tr><td>shared_efficiency</td></tr>

              <tr><td>achieved_occupancy</td></tr>
              <tr><td>sm_efficiency[_instance]</td></tr>
              <tr><td>ipc[_instance]</td></tr>
              <tr><td>issued_ipc</td></tr>
              <tr><td>inst_per_warp</td></tr>
            </tbody>
          </table>
        </section>
        <section>
          <table class="tbl1" style="width:100%;">
            <colgroup>
              <col></col>
            </colgroup>
            <thead>
              <tr>
                <th>metric</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>flops_{sp,dp}[_add,mul,fma]</td></tr>
              <tr><td>warp_execution_efficiency</td></tr>

              <tr><td>warp_nonpred_execution_efficiency</td></tr>
              <tr><td>flops_sp_special</td></tr>
              <tr><td>stall_{inst_fetch,exec_dependency,data_request,texture,sync,other}</td></tr>

              <tr><td>{l1_shared,l2,tex,dram,system}_utilization</td></tr>
              <tr><td>{cf,ldst}_{issued,executed}</td></tr>
              <tr><td>{ldst,alu,cf,tex}_fu_utilization</td></tr>
              <tr><td>issue_slot_utilization</td></tr>
              <tr><td>inst_{issued,executed}</td></tr>
              <tr><td>issue_slots</td></tr>
            </tbody>
          </table>
        </section>
      </section>

        <section>
          <h2>ROI profiling</h2>
          <pre style="width:100%;"><code style="padding: 10px;" class="cpp">#include &lt;cuda_profiler_api.h&gt;

// algorithm setup code
udaProfilerStart();
perf_test_cuda_accelerated_code();
cudaProfilerStop();
</code></pre>
          <ul>
            <li>Profile only part that you are optimizing right now</li>
            <li>shorter and simpler profiler log</li>
            <li>Do not significantly overhead your code runtime</li>
            <li>Used with <b><code>--profile-from-start off</code></b> nvprof option</li>
          </ul>
        </section>

        <section>
          <h2>Case study: Matrix transpose</h2>
          <section>
            <pre><code class="bash">& nvprof --devices 2 ./bin/demo_bench</code></pre>
            <img src="images/c3/cs1.png" class="simple" style="padding:10px; background-color:#000;">
          </section>
          <section>
            <pre><code class="bash">& nvprof --devices 2 \
--metrics gld_transactions_per_request,gst_transactions_per_request \
./bin/demo_bench</code></pre>
            <img src="images/c3/cs2.png" class="simple" style="padding:10px; background-color:#000;">
          </section>
          <section>
            <pre><code class="bash">& nvprof --devices 2 --metrics shared_replay_overhead ./bin/demo_bench</code></pre>
            <img src="images/c3/cs3.png" class="simple" style="padding:10px; background-color:#000;">
            <img src="images/c3/cs4.png" class="simple" style="padding:10px; background-color:#000;">
          </section>
        </section>

        <section>
          <h2>Code paths analysis</h2>
          <ul>
            <li>The main idea: determine performance limiters through measuring different parts independently</li>
            <li>Simple case: time memory-only and math-only versions of the kernel</li>
            <li>Shows how well memory operations are overlapped with arithmetic: compare the sum of mem-only and math-only times to full-kernel time</li>
        </ul>
        <pre style="width:100%;"><code style="padding: 10px;" class="cpp">template&lt;typename T&gt;
<span class="keyword">__global__</span> void
benchmark_contiguous_direct_load(T* s, typename T::value_type* r, bool doStore)
{
   int global_index = threadIdx.x + blockDim.x * blockIdx.x;
   T data = s[global_index];
   asm (""::: "memory");
   if (s && doStore)
       r[global_index] = sum(data);
}</code></pre>
        </section>
        <section>
          <h2>device side timing</h2>
          <ul>
            <li>Device timer located on ROP/SM depending on hardware revision</li>
            <li>It's relatively easy to compute per thread values but hard to analyze kernel performance
            due to grid serialization</li>
            <li>sometimes is suitable for benchmarking</li>
          </ul>
          <pre style="width:100%;"><code style="padding: 10px;" class="cpp">template&lt;typename T, typename D, typename L&gt;<span class="keyword">__global__</span>
void latency_kernel(T** a, int len, int stride, int inner_its, D* latency, L func)
{
    D start_time, end_time;
    volatile D sum_time = 0;
    for (int k = 0; k < inner_its; ++k)
    {
        T *j = ((T*) a) + threadIdx.y * len + threadIdx.x;
        start_time = clock64();
        for (int curr = 0; curr < len / stride; ++curr) j = func(j);
        end_time = clock64(); sum_time += (end_time - start_time);
    }
    if (!threadIdx.x) atomicAdd(latency, sum_time);
}</code></pre>
        </section>

          <section>
          <h2>Final words</h2>
          <ul>
            <li>Time</li>
            <li>Profile</li>
            <li>(Micro)benchmark</li>
            <li>Prototype</li>
            <li>Look into SASS</li>
          </ul>
        </section>

        <section id="end1">
          <h1>THE END</h1>
          <h6><a href="index.html">list of presentations</a></h6>
          <br>
          <br>
          <br>
          <h3>BY <a href="https://github.com/cuda-geek">cuda.geek</a> / 2013&ndash;2015</h3>
        </section>
      </div>
    </div>
    <script src="3dparty/reveal/lib/js/head.min.js"></script>
    <script src="3dparty/reveal/js/reveal.min.js"></script>
    <script>
            Reveal.initialize({
                controls: true,
                progress: true,
                history: true,
                center: false,
                rollingLinks: false,

                theme: Reveal.getQueryHash().theme,
                transition: Reveal.getQueryHash().transition || 'concave', // default/cube/page/concave/zoom/linear/fade/none
                dependencies: [
                    { src: '3dparty/reveal/lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: '3dparty/reveal/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: '3dparty/reveal/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: '3dparty/reveal/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: '3dparty/reveal/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: '3dparty/reveal/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
                    // { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
                    // { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
                ]
            });
    </script>
  </body>
</html>
